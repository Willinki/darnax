{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"darnax","text":"<p>Deep Asymmetric Recurrent Networks in JAX.</p> <p>darnax is a research library for building and experimenting with asymmetric recurrent neural networks and their learning dynamics. It is inspired by recent work by Badalotti, Baldassi, M\u00e9zard, Scardecchia and Zecchina, showing that simple, distributed plasticity rules can give rise to powerful learning behaviors without relying on backpropagation.</p> <p>The library provides:</p> <ul> <li>Composable modules based on Equinox, with clean support for sparse and structured connectivity. They can be easily extended to accomodate for complex network structures (convolutional, transformers-like, etc...). They also naturally support layers of varying shapes and connectivity.</li> <li>Orchestrators for running recurrent dynamics, either sequentially or in parallel.</li> <li>Local update rules implementing gradient-free plasticity mechanisms.</li> <li>Jax speed and transparency. Everything is a pytree, whether you are building a simple 1 hidden-layer model or a complex interconnected structure, the training logic remains the same.</li> <li>Natural integration with optax. Despite not relying on explicit gradients, the models can be naturally optimized with Optax.</li> </ul> <p>darnax is not a framework chasing SOTA benchmarks. It is a sandbox for exploring recurrent dynamics as a computational primitive \u2014 bridging machine learning, theoretical neuroscience, and statistical physics. This is also a work-in-progress, and contributions are more than welcome!</p> <p>\ud83d\udc49 Check the Tutorials to get started, or browse the API Reference for details.</p>"},{"location":"reference/","title":"Reference","text":"<p>Auto-generated API lives here.</p> <ul> <li>\ud83d\udc49 API Index</li> </ul>"},{"location":"reference/api/","title":"API Index","text":"<p>Auto-generated. Top-level packages mirror the directory layout.</p>"},{"location":"reference/api/#packages","title":"Packages","text":"<ul> <li>darnax.datasets</li> <li>darnax.layer_maps</li> <li>darnax.modules</li> <li>darnax.orchestrators</li> <li>darnax.states</li> <li>darnax.trainers</li> <li>darnax.utils</li> </ul>"},{"location":"reference/api/darnax/","title":"darnax","text":""},{"location":"reference/api/darnax/#subpackages","title":"Subpackages","text":"<ul> <li>darnax.datasets</li> <li>darnax.layer_maps</li> <li>darnax.modules</li> <li>darnax.orchestrators</li> <li>darnax.states</li> <li>darnax.trainers</li> <li>darnax.utils</li> </ul>"},{"location":"reference/api/darnax/datasets/","title":"darnax.datasets","text":""},{"location":"reference/api/darnax/datasets/#subpackages","title":"Subpackages","text":"<ul> <li>darnax.datasets.classification</li> </ul>"},{"location":"reference/api/darnax/layer_maps/","title":"darnax.layer_maps","text":""},{"location":"reference/api/darnax/layer_maps/#modules","title":"Modules","text":"<ul> <li>darnax.layer_maps.sparse</li> </ul>"},{"location":"reference/api/darnax/modules/","title":"darnax.modules","text":""},{"location":"reference/api/darnax/modules/#modules","title":"Modules","text":"<ul> <li>darnax.modules.debug</li> <li>darnax.modules.ferromagnetic</li> <li>darnax.modules.fully_connected</li> <li>darnax.modules.input_output</li> <li>darnax.modules.interfaces</li> <li>darnax.modules.recurrent</li> <li>darnax.modules.recurrent_tanh</li> </ul>"},{"location":"reference/api/darnax/orchestrators/","title":"darnax.orchestrators","text":""},{"location":"reference/api/darnax/orchestrators/#modules","title":"Modules","text":"<ul> <li>darnax.orchestrators.interface</li> <li>darnax.orchestrators.sequential</li> </ul>"},{"location":"reference/api/darnax/states/","title":"darnax.states","text":""},{"location":"reference/api/darnax/states/#modules","title":"Modules","text":"<ul> <li>darnax.states.interface</li> <li>darnax.states.sequential</li> </ul>"},{"location":"reference/api/darnax/trainers/","title":"darnax.trainers","text":""},{"location":"reference/api/darnax/trainers/#modules","title":"Modules","text":"<ul> <li>darnax.trainers.alternate</li> <li>darnax.trainers.dynamical</li> <li>darnax.trainers.hebbian_contrastive</li> <li>darnax.trainers.interface</li> <li>darnax.trainers.utils</li> </ul>"},{"location":"reference/api/darnax/utils/","title":"darnax.utils","text":""},{"location":"reference/api/darnax/utils/#modules","title":"Modules","text":"<ul> <li>darnax.utils.cont_perceptron_rule</li> <li>darnax.utils.default_list</li> <li>darnax.utils.layermap_utils</li> <li>darnax.utils.perceptron_rule</li> <li>darnax.utils.typing</li> </ul>"},{"location":"reference/api/darnax/datasets/classification/","title":"darnax.datasets.classification","text":""},{"location":"reference/api/darnax/datasets/classification/#modules","title":"Modules","text":"<ul> <li>darnax.datasets.classification.cifar10</li> <li>darnax.datasets.classification.cifar10_features</li> <li>darnax.datasets.classification.fashion_mnist</li> <li>darnax.datasets.classification.interface</li> <li>darnax.datasets.classification.mnist</li> </ul>"},{"location":"reference/api/darnax/datasets/classification/cifar10/","title":"darnax.datasets.classification.cifar10","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10/#darnax.datasets.classification.cifar10.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10/#darnax.datasets.classification.cifar10.Cifar10","title":"<code>Cifar10(batch_size=64, linear_projection=100, num_images_per_class=None, label_mode='c-rescale', x_transform='sign', validation_fraction=0.0)</code>","text":"<p>               Bases: <code>ClassificationDataset</code></p> <p>CIFAR-10 dataset with configurable preprocessing.</p> Parameters. <p>batch_size : int, default=64     Batch size for iterators. linear_projection : int or None, default=100     Output dimension for random projection. If None, uses full 3072 dimensions. num_images_per_class : int or None, default=None     Maximum training images per class. If None, uses full training set. label_mode : {\"pm1\", \"ooe\", \"c-rescale\"}, default=\"c-rescale\"     Label encoding: \"pm1\" (\u00b11), \"ooe\" (one-hot), \"c-rescale\" (scaled). x_transform : {\"sign\", \"tanh\", \"identity\"}, default=\"sign\"     Input transform: \"sign\" (\u00b11), \"tanh\", \"identity\" (no transform). validation_fraction : float, default=0.0     Fraction of training data for validation (0.0 to 1.0).</p> References <pre><code>- https://www.cs.toronto.edu/~kriz/cifar.html\n- https://huggingface.co/datasets/cifar10\n</code></pre> <p>Initialize CIFAR-10 dataset configuration.</p>"},{"location":"reference/api/darnax/datasets/classification/cifar10/#darnax.datasets.classification.cifar10.Cifar10.NUM_CLASSES","title":"<code>NUM_CLASSES = 10</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10/#darnax.datasets.classification.cifar10.Cifar10.FLAT_DIM","title":"<code>FLAT_DIM = 32 * 32 * 3</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10/#darnax.datasets.classification.cifar10.Cifar10.CACHE_SUBDIR","title":"<code>CACHE_SUBDIR = 'darnax/cifar10'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10/#darnax.datasets.classification.cifar10.Cifar10.batch_size","title":"<code>batch_size = int(batch_size)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10/#darnax.datasets.classification.cifar10.Cifar10.linear_projection","title":"<code>linear_projection = linear_projection</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10/#darnax.datasets.classification.cifar10.Cifar10.num_images_per_class","title":"<code>num_images_per_class = num_images_per_class</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10/#darnax.datasets.classification.cifar10.Cifar10.label_mode","title":"<code>label_mode = label_mode</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10/#darnax.datasets.classification.cifar10.Cifar10.x_transform","title":"<code>x_transform = x_transform</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10/#darnax.datasets.classification.cifar10.Cifar10.validation_fraction","title":"<code>validation_fraction = validation_fraction</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10/#darnax.datasets.classification.cifar10.Cifar10.input_dim","title":"<code>input_dim = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10/#darnax.datasets.classification.cifar10.Cifar10.num_classes","title":"<code>num_classes = self.NUM_CLASSES</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10/#darnax.datasets.classification.cifar10.Cifar10.x_train","title":"<code>x_train = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10/#darnax.datasets.classification.cifar10.Cifar10.y_train","title":"<code>y_train = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10/#darnax.datasets.classification.cifar10.Cifar10.x_valid","title":"<code>x_valid = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10/#darnax.datasets.classification.cifar10.Cifar10.y_valid","title":"<code>y_valid = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10/#darnax.datasets.classification.cifar10.Cifar10.x_test","title":"<code>x_test = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10/#darnax.datasets.classification.cifar10.Cifar10.y_test","title":"<code>y_test = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10/#darnax.datasets.classification.cifar10.Cifar10.build","title":"<code>build(key)</code>","text":"<p>Load, preprocess, and prepare CIFAR-10 splits.</p>"},{"location":"reference/api/darnax/datasets/classification/cifar10/#darnax.datasets.classification.cifar10.Cifar10.iter_test","title":"<code>iter_test()</code>","text":"<p>Iterate over test batches.</p>"},{"location":"reference/api/darnax/datasets/classification/cifar10/#darnax.datasets.classification.cifar10.Cifar10.iter_valid","title":"<code>iter_valid()</code>","text":"<p>Iterate over validation batches.</p>"},{"location":"reference/api/darnax/datasets/classification/cifar10/#darnax.datasets.classification.cifar10.Cifar10.spec","title":"<code>spec()</code>","text":"<p>Return dataset specification.</p>"},{"location":"reference/api/darnax/datasets/classification/cifar10_features/","title":"darnax.datasets.classification.cifar10_features","text":"<p>CIFAR-10 (precomputed features, small) dataset for darnax.</p> <p>Loads standardized 512-D features from Hugging Face:     repo = \"willinki/cifar10-features-s\"</p> <p>Split semantics (HF naming quirk):     - \"train\"       \u2192 training (optionally split into train/valid via <code>validation_fraction</code>)     - \"validation\"  \u2192 actually the TEST set for this repo</p> <p>Columns:     - \"x\": features, float32 tensor of shape [N, 512]     - \"y\": labels,   int32 tensor of shape [N]</p> <p>Options:     - Optional uniform-per-class subsampling on the training set.     - Optional linear projection W \u2208 \u211d^{D_out\u00d7D_in} with entries ~ N(0,1)/\u221aD_in       (variance-preserving when input var\u22481).     - Optional x_transform:         * \"identity\": no change (default)         * \"sign\": binarize to \u00b11 (0 \u2192 \u22121)</p> <p>Post-build shapes:     x_train: [N_tr, D_out], y_train: [N_tr, C]     x_valid: [N_va, D_out], y_valid: [N_va, C]   (if validation_fraction &gt; 0)     x_test:  [N_te, D_out], y_test:  [N_te, C]</p>"},{"location":"reference/api/darnax/datasets/classification/cifar10_features/#darnax.datasets.classification.cifar10_features.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10_features/#darnax.datasets.classification.cifar10_features.Cifar10FeaturesSmall","title":"<code>Cifar10FeaturesSmall(batch_size=64, linear_projection=None, num_images_per_class=None, label_mode='c-rescale', x_transform='identity', validation_fraction=0.0)</code>","text":"<p>               Bases: <code>ClassificationDataset</code></p> <p>CIFAR-10 features dataset (512-D, standardized).</p> <p>Initilize Cifar10Features data.</p> <p>Takes care of downloading the tensors in jax format, sampling with equal frequency, sample a validation set and transforming x or y when requested.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for the iterators</p> <code>64</code> <code>linear_projection</code> <code>int</code> <p>If integer, data is linearly projected to the size. If None, the step is skipped.</p> <code>None</code> <code>num_images_per_class</code> <code>int</code> <p>If integer, we simple a fixed amount of images per class. If a class does not contain enough images, we sample them all.</p> <code>None</code> <code>label_mode</code> <code>Literal['pm1', 'ooe', 'c-rescale']</code> <p>if pm1 the positive class is assigned +1, the others -1. if ooe, regular one-hot-encoding. if c-rescale, the positive class is rescaled to C/2, while the negative are rescaled to -0.5.</p> <code>'c-rescale'</code> <code>validation_fraction</code> <code>float</code> <p>If not zero, we sample a random holdout set from training.</p> <code>0.0</code> <code>x_transform</code> <code>Literal['sign', 'identity']</code> <p>if sign, we binarize features after linear transform. if identity this step is skipped.</p> <code>'identity'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if batch_size &lt;= 1, linear_projection is not int &gt;= 1. if num_images_per_class is negative or not None. if validation_fraction is outside [0.0, 1.0).</p>"},{"location":"reference/api/darnax/datasets/classification/cifar10_features/#darnax.datasets.classification.cifar10_features.Cifar10FeaturesSmall.NUM_CLASSES","title":"<code>NUM_CLASSES = 10</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10_features/#darnax.datasets.classification.cifar10_features.Cifar10FeaturesSmall.FEAT_DIM","title":"<code>FEAT_DIM = 512</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10_features/#darnax.datasets.classification.cifar10_features.Cifar10FeaturesSmall.SHAPE_DIM","title":"<code>SHAPE_DIM = 2</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10_features/#darnax.datasets.classification.cifar10_features.Cifar10FeaturesSmall.HF_REPO","title":"<code>HF_REPO = 'willinki/cifar10-features-s'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10_features/#darnax.datasets.classification.cifar10_features.Cifar10FeaturesSmall.CACHE_SUBDIR","title":"<code>CACHE_SUBDIR = 'darnax/cifar10_features_small'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10_features/#darnax.datasets.classification.cifar10_features.Cifar10FeaturesSmall.batch_size","title":"<code>batch_size = int(batch_size)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10_features/#darnax.datasets.classification.cifar10_features.Cifar10FeaturesSmall.linear_projection","title":"<code>linear_projection = linear_projection</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10_features/#darnax.datasets.classification.cifar10_features.Cifar10FeaturesSmall.num_images_per_class","title":"<code>num_images_per_class = num_images_per_class</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10_features/#darnax.datasets.classification.cifar10_features.Cifar10FeaturesSmall.label_mode","title":"<code>label_mode = label_mode</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10_features/#darnax.datasets.classification.cifar10_features.Cifar10FeaturesSmall.validation_fraction","title":"<code>validation_fraction = validation_fraction</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10_features/#darnax.datasets.classification.cifar10_features.Cifar10FeaturesSmall.x_transform","title":"<code>x_transform = x_transform</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10_features/#darnax.datasets.classification.cifar10_features.Cifar10FeaturesSmall.input_dim","title":"<code>input_dim = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10_features/#darnax.datasets.classification.cifar10_features.Cifar10FeaturesSmall.num_classes","title":"<code>num_classes = self.NUM_CLASSES</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10_features/#darnax.datasets.classification.cifar10_features.Cifar10FeaturesSmall.x_train","title":"<code>x_train = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10_features/#darnax.datasets.classification.cifar10_features.Cifar10FeaturesSmall.y_train","title":"<code>y_train = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10_features/#darnax.datasets.classification.cifar10_features.Cifar10FeaturesSmall.x_valid","title":"<code>x_valid = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10_features/#darnax.datasets.classification.cifar10_features.Cifar10FeaturesSmall.y_valid","title":"<code>y_valid = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10_features/#darnax.datasets.classification.cifar10_features.Cifar10FeaturesSmall.x_test","title":"<code>x_test = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10_features/#darnax.datasets.classification.cifar10_features.Cifar10FeaturesSmall.y_test","title":"<code>y_test = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10_features/#darnax.datasets.classification.cifar10_features.Cifar10FeaturesSmall.build","title":"<code>build(key)</code>","text":"<p>Load, optionally subsample, optionally project, transform, encode labels, split.</p>"},{"location":"reference/api/darnax/datasets/classification/cifar10_features/#darnax.datasets.classification.cifar10_features.Cifar10FeaturesSmall.iter_test","title":"<code>iter_test()</code>","text":"<p>Iterate over test batches (HF 'validation' split).</p>"},{"location":"reference/api/darnax/datasets/classification/cifar10_features/#darnax.datasets.classification.cifar10_features.Cifar10FeaturesSmall.iter_valid","title":"<code>iter_valid()</code>","text":"<p>Iterate over validation batches (holdout from training).</p>"},{"location":"reference/api/darnax/datasets/classification/cifar10_features/#darnax.datasets.classification.cifar10_features.Cifar10FeaturesSmall.spec","title":"<code>spec()</code>","text":"<p>Return dataset specification for model wiring.</p>"},{"location":"reference/api/darnax/datasets/classification/cifar10_features/#darnax.datasets.classification.cifar10_features.Cifar10FeaturesLarge","title":"<code>Cifar10FeaturesLarge(batch_size=64, linear_projection=None, num_images_per_class=None, label_mode='c-rescale', x_transform='identity', validation_fraction=0.0)</code>","text":"<p>               Bases: <code>Cifar10FeaturesSmall</code></p> <p>Same contract as Small, but 4096-D features from a different HF repo.</p>"},{"location":"reference/api/darnax/datasets/classification/cifar10_features/#darnax.datasets.classification.cifar10_features.Cifar10FeaturesLarge.FEAT_DIM","title":"<code>FEAT_DIM = 4096</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10_features/#darnax.datasets.classification.cifar10_features.Cifar10FeaturesLarge.HF_REPO","title":"<code>HF_REPO = 'willinki/cifar10-features-l'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10_features/#darnax.datasets.classification.cifar10_features.Cifar10FeaturesLarge.CACHE_SUBDIR","title":"<code>CACHE_SUBDIR = 'darnax/cifar10_features_large'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10_features/#darnax.datasets.classification.cifar10_features.Cifar10FeaturesVit","title":"<code>Cifar10FeaturesVit(batch_size=64, linear_projection=None, num_images_per_class=None, label_mode='c-rescale', x_transform='identity', validation_fraction=0.0)</code>","text":"<p>               Bases: <code>Cifar10FeaturesSmall</code></p> <p>Instead of extracting from VGG11, we extract from a vision transformer.</p>"},{"location":"reference/api/darnax/datasets/classification/cifar10_features/#darnax.datasets.classification.cifar10_features.Cifar10FeaturesVit.FEAT_DIM","title":"<code>FEAT_DIM = 192</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10_features/#darnax.datasets.classification.cifar10_features.Cifar10FeaturesVit.HF_REPO","title":"<code>HF_REPO = 'willinki/cifar10-features-vit'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/cifar10_features/#darnax.datasets.classification.cifar10_features.Cifar10FeaturesVit.CACHE_SUBDIR","title":"<code>CACHE_SUBDIR = 'darnax/cifar10_features_vit'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/fashion_mnist/","title":"darnax.datasets.classification.fashion_mnist","text":"<p>Fashion-MNIST dataset implementation for darnax.</p>"},{"location":"reference/api/darnax/datasets/classification/fashion_mnist/#darnax.datasets.classification.fashion_mnist.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/fashion_mnist/#darnax.datasets.classification.fashion_mnist.FashionMnist","title":"<code>FashionMnist(batch_size=64, linear_projection=None, num_images_per_class=None, label_mode='c-rescale', x_transform='sign', validation_fraction=0.0, flatten=True)</code>","text":"<p>               Bases: <code>ClassificationDataset</code></p> <p>Fashion-MNIST dataset with configurable preprocessing.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for iterators.</p> <code>64</code> <code>linear_projection</code> <code>int or None</code> <p>Output dimension for random projection. If None, uses full 784 dimensions.</p> <code>100</code> <code>num_images_per_class</code> <code>int or None</code> <p>Maximum training images per class. If None, uses full training set.</p> <code>None</code> <code>label_mode</code> <code>('pm1', 'ooe', 'c-rescale')</code> <p>Label encoding: \"pm1\" (\u00b11), \"ooe\" (one-hot), \"c-rescale\" (scaled).</p> <code>\"pm1\"</code> <code>x_transform</code> <code>('sign', 'tanh', 'identity')</code> <p>Input transform: \"sign\" (\u00b11), \"tanh\", \"identity\" (no transform).</p> <code>\"sign\"</code> <code>validation_fraction</code> <code>float</code> <p>Fraction of training data for validation (0.0 to 1.0).</p> <code>0.0</code> <code>flatten</code> <code>bool</code> <p>If True, flatten inputs to (B, 784) and (optionally) apply random projection. If False, keep inputs as (B, 28, 28) and disable random projection.</p> <code>= True</code> <p>Initialize Fashion-MNIST dataset configuration.</p>"},{"location":"reference/api/darnax/datasets/classification/fashion_mnist/#darnax.datasets.classification.fashion_mnist.FashionMnist.NUM_CLASSES","title":"<code>NUM_CLASSES = 10</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/fashion_mnist/#darnax.datasets.classification.fashion_mnist.FashionMnist.FLAT_DIM","title":"<code>FLAT_DIM = 28 * 28</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/fashion_mnist/#darnax.datasets.classification.fashion_mnist.FashionMnist.CACHE_SUBDIR","title":"<code>CACHE_SUBDIR = 'darnax/fashion_mnist'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/fashion_mnist/#darnax.datasets.classification.fashion_mnist.FashionMnist.batch_size","title":"<code>batch_size = int(batch_size)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/fashion_mnist/#darnax.datasets.classification.fashion_mnist.FashionMnist.linear_projection","title":"<code>linear_projection = linear_projection</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/fashion_mnist/#darnax.datasets.classification.fashion_mnist.FashionMnist.num_images_per_class","title":"<code>num_images_per_class = num_images_per_class</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/fashion_mnist/#darnax.datasets.classification.fashion_mnist.FashionMnist.label_mode","title":"<code>label_mode = label_mode</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/fashion_mnist/#darnax.datasets.classification.fashion_mnist.FashionMnist.x_transform","title":"<code>x_transform = x_transform</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/fashion_mnist/#darnax.datasets.classification.fashion_mnist.FashionMnist.validation_fraction","title":"<code>validation_fraction = validation_fraction</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/fashion_mnist/#darnax.datasets.classification.fashion_mnist.FashionMnist.flatten","title":"<code>flatten = bool(flatten)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/fashion_mnist/#darnax.datasets.classification.fashion_mnist.FashionMnist.input_dim","title":"<code>input_dim = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/fashion_mnist/#darnax.datasets.classification.fashion_mnist.FashionMnist.num_classes","title":"<code>num_classes = self.NUM_CLASSES</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/fashion_mnist/#darnax.datasets.classification.fashion_mnist.FashionMnist.x_train","title":"<code>x_train = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/fashion_mnist/#darnax.datasets.classification.fashion_mnist.FashionMnist.y_train","title":"<code>y_train = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/fashion_mnist/#darnax.datasets.classification.fashion_mnist.FashionMnist.x_valid","title":"<code>x_valid = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/fashion_mnist/#darnax.datasets.classification.fashion_mnist.FashionMnist.y_valid","title":"<code>y_valid = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/fashion_mnist/#darnax.datasets.classification.fashion_mnist.FashionMnist.x_test","title":"<code>x_test = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/fashion_mnist/#darnax.datasets.classification.fashion_mnist.FashionMnist.y_test","title":"<code>y_test = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/fashion_mnist/#darnax.datasets.classification.fashion_mnist.FashionMnist.build","title":"<code>build(key)</code>","text":"<p>Load, preprocess, and prepare Fashion-MNIST splits.</p>"},{"location":"reference/api/darnax/datasets/classification/fashion_mnist/#darnax.datasets.classification.fashion_mnist.FashionMnist.iter_test","title":"<code>iter_test()</code>","text":"<p>Iterate over test batches.</p>"},{"location":"reference/api/darnax/datasets/classification/fashion_mnist/#darnax.datasets.classification.fashion_mnist.FashionMnist.iter_valid","title":"<code>iter_valid()</code>","text":"<p>Iterate over validation batches.</p>"},{"location":"reference/api/darnax/datasets/classification/fashion_mnist/#darnax.datasets.classification.fashion_mnist.FashionMnist.spec","title":"<code>spec()</code>","text":"<p>Return dataset specification.</p>"},{"location":"reference/api/darnax/datasets/classification/interface/","title":"darnax.datasets.classification.interface","text":""},{"location":"reference/api/darnax/datasets/classification/interface/#darnax.datasets.classification.interface.ClassificationDataset","title":"<code>ClassificationDataset</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for datasets compatible with darnax trainers.</p> <p>Datasets must implement train/test iteration and provide metadata via <code>spec()</code>. Validation split is optional.</p> Required Methods <ul> <li>build(key) : Load and preprocess data</li> <li>iter() : Training batch iterator</li> <li>iter_test() : Test batch iterator</li> <li>len() : Number of training batches</li> <li>spec() : Dataset metadata and structure</li> </ul> Optional Methods <ul> <li>iter_valid() : Validation batch iterator (default raises NotImplementedError)</li> </ul>"},{"location":"reference/api/darnax/datasets/classification/interface/#darnax.datasets.classification.interface.ClassificationDataset.build","title":"<code>build(key)</code>  <code>abstractmethod</code>","text":"<p>Load, preprocess, and prepare dataset splits.</p>"},{"location":"reference/api/darnax/datasets/classification/interface/#darnax.datasets.classification.interface.ClassificationDataset.iter_test","title":"<code>iter_test()</code>  <code>abstractmethod</code>","text":"<p>Iterate over test batches.</p>"},{"location":"reference/api/darnax/datasets/classification/interface/#darnax.datasets.classification.interface.ClassificationDataset.iter_valid","title":"<code>iter_valid()</code>","text":"<p>Iterate over validation batches (optional).</p>"},{"location":"reference/api/darnax/datasets/classification/interface/#darnax.datasets.classification.interface.ClassificationDataset.spec","title":"<code>spec()</code>  <code>abstractmethod</code>","text":"<p>Return dataset specification with metadata.</p>"},{"location":"reference/api/darnax/datasets/classification/mnist/","title":"darnax.datasets.classification.mnist","text":"<p>MNIST dataset implementation for darnax.</p>"},{"location":"reference/api/darnax/datasets/classification/mnist/#darnax.datasets.classification.mnist.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/mnist/#darnax.datasets.classification.mnist.Mnist","title":"<code>Mnist(batch_size=64, linear_projection=None, num_images_per_class=None, label_mode='c-rescale', x_transform='sign', validation_fraction=0.0, flatten=True)</code>","text":"<p>               Bases: <code>ClassificationDataset</code></p> <p>MNIST dataset with configurable preprocessing.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for iterators.</p> <code>64</code> <code>linear_projection</code> <code>int or None</code> <p>Output dimension for random projection. If None, uses full 784 dimensions.</p> <code>100</code> <code>num_images_per_class</code> <code>int or None</code> <p>Maximum training images per class. If None, uses full training set.</p> <code>None</code> <code>label_mode</code> <code>('pm1', 'ooe', 'c-rescale')</code> <p>Label encoding: \"pm1\" (\u00b11), \"ooe\" (one-hot), \"c-rescale\" (scaled).</p> <code>\"pm1\"</code> <code>x_transform</code> <code>('sign', 'tanh', 'identity')</code> <p>Input transform: \"sign\" (\u00b11), \"tanh\", \"identity\" (no transform).</p> <code>\"sign\"</code> <code>validation_fraction</code> <code>float</code> <p>Fraction of training data for validation (0.0 to 1.0).</p> <code>0.0</code> <code>flatten</code> <code>bool</code> <p>If True, flatten inputs to (B, 784) and (optionally) apply random projection. If False, keep inputs as (B, 28, 28) and disable random projection.</p> <code>= True</code> <p>Initialize MNIST dataset configuration.</p>"},{"location":"reference/api/darnax/datasets/classification/mnist/#darnax.datasets.classification.mnist.Mnist.NUM_CLASSES","title":"<code>NUM_CLASSES = 10</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/mnist/#darnax.datasets.classification.mnist.Mnist.FLAT_DIM","title":"<code>FLAT_DIM = 28 * 28</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/mnist/#darnax.datasets.classification.mnist.Mnist.CACHE_SUBDIR","title":"<code>CACHE_SUBDIR = 'darnax/mnist'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/mnist/#darnax.datasets.classification.mnist.Mnist.batch_size","title":"<code>batch_size = int(batch_size)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/mnist/#darnax.datasets.classification.mnist.Mnist.linear_projection","title":"<code>linear_projection = linear_projection</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/mnist/#darnax.datasets.classification.mnist.Mnist.num_images_per_class","title":"<code>num_images_per_class = num_images_per_class</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/mnist/#darnax.datasets.classification.mnist.Mnist.label_mode","title":"<code>label_mode = label_mode</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/mnist/#darnax.datasets.classification.mnist.Mnist.x_transform","title":"<code>x_transform = x_transform</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/mnist/#darnax.datasets.classification.mnist.Mnist.validation_fraction","title":"<code>validation_fraction = validation_fraction</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/mnist/#darnax.datasets.classification.mnist.Mnist.flatten","title":"<code>flatten = bool(flatten)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/mnist/#darnax.datasets.classification.mnist.Mnist.input_dim","title":"<code>input_dim = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/mnist/#darnax.datasets.classification.mnist.Mnist.num_classes","title":"<code>num_classes = self.NUM_CLASSES</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/mnist/#darnax.datasets.classification.mnist.Mnist.x_train","title":"<code>x_train = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/mnist/#darnax.datasets.classification.mnist.Mnist.y_train","title":"<code>y_train = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/mnist/#darnax.datasets.classification.mnist.Mnist.x_valid","title":"<code>x_valid = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/mnist/#darnax.datasets.classification.mnist.Mnist.y_valid","title":"<code>y_valid = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/mnist/#darnax.datasets.classification.mnist.Mnist.x_test","title":"<code>x_test = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/mnist/#darnax.datasets.classification.mnist.Mnist.y_test","title":"<code>y_test = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/datasets/classification/mnist/#darnax.datasets.classification.mnist.Mnist.build","title":"<code>build(key)</code>","text":"<p>Load, preprocess, and prepare MNIST splits.</p>"},{"location":"reference/api/darnax/datasets/classification/mnist/#darnax.datasets.classification.mnist.Mnist.iter_test","title":"<code>iter_test()</code>","text":"<p>Iterate over test batches.</p>"},{"location":"reference/api/darnax/datasets/classification/mnist/#darnax.datasets.classification.mnist.Mnist.iter_valid","title":"<code>iter_valid()</code>","text":"<p>Iterate over validation batches.</p>"},{"location":"reference/api/darnax/datasets/classification/mnist/#darnax.datasets.classification.mnist.Mnist.spec","title":"<code>spec()</code>","text":"<p>Return dataset specification.</p>"},{"location":"reference/api/darnax/layer_maps/sparse/","title":"darnax.layer_maps.sparse","text":"<p>LayerMap: a static, PyTree-friendly adjacency of modules.</p> <p><code>LayerMap</code> wraps a nested dict-of-dicts that maps receiver rows <code>i</code> to neighbors (columns) <code>j</code> \u2192 module, i.e. it represents edges <code>(i, j)</code>. The structure (row/column keys and their order) is static for JIT stability, while the values (modules and their parameters) are dynamic PyTree leaves.</p> Design goals <ul> <li>Keep a clear nested dict API while making the structure part of the treedef.</li> <li>Flatten through Equinox/JAX modules so inner arrays are visible to JAX/Optax.</li> <li>Forbid structural mutation after construction (frozen dataclass, read-only views).</li> </ul> Conventions <ul> <li>Rows and, within each row, columns are sorted once at construction.</li> <li>Keys are integers (layer indices). Edge <code>(i, j)</code> connects sender/neighbor   <code>j</code> into receiver <code>i</code> (lower-triangular including the diagonal is common).</li> <li>The diagonal policy can be enforced: every row must have its <code>(i, i)</code> self-edge.</li> </ul> PyTree behavior <p><code>tree_flatten</code> returns all modules in deterministic row-major order as children, plus static aux data describing the key layout. JAX/Equinox then flattens module parameters further, so optimizers and transforms \"see\" the arrays inside.</p>"},{"location":"reference/api/darnax/layer_maps/sparse/#darnax.layer_maps.sparse.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/layer_maps/sparse/#darnax.layer_maps.sparse.LayerMap","title":"<code>LayerMap(_data, _rows, _ndim=2)</code>  <code>dataclass</code>","text":"<p>PyTree wrapper around a dict-of-dicts with static keys and non-static values.</p> <p>Parameters:</p> Name Type Description Default <code>_data</code> <code>dict[int, dict[int, AbstractModule]]</code> <p>Internal mapping from row <code>i</code> \u2192 (col <code>j</code> \u2192 module). Keys are sorted.</p> required <code>_rows</code> <code>tuple[int, ...]</code> <p>All row keys in sorted order (becomes part of the treedef).</p> required <code>_ndim</code> <code>int</code> <p>Tuple key arity (<code>(i, j)</code>). Exposed mainly for reconstruction.</p> <code>2</code> Notes <ul> <li>The dataclass is frozen to prevent structural mutation after creation.   Use :meth:<code>to_dict</code> to obtain a mutable deep copy if you truly need one.</li> <li>Read-only accessors (e.g., :meth:<code>neighbors</code>) return <code>MappingProxyType</code>.</li> <li>The keys (rows/cols) are included in the PyTree aux data, so the layout   is static under JIT; values (modules) are the dynamic leaves.</li> </ul> Public type <p>Values are typed as :class:<code>~darnax.modules.interfaces.AbstractModule</code> so any concrete layer/adapter subtype can be stored.</p>"},{"location":"reference/api/darnax/layer_maps/sparse/#darnax.layer_maps.sparse.LayerMap.from_dict","title":"<code>from_dict(mapping, *, require_diagonal=True)</code>  <code>staticmethod</code>","text":"<p>Construct a LayerMap from a nested mapping.</p> <p>Parameters:</p> Name Type Description Default <code>mapping</code> <code>Mapping[int, Mapping[int, AbstractModule]]</code> <p>Nested mapping from row <code>i</code> \u2192 (col <code>j</code> \u2192 module).</p> required <code>require_diagonal</code> <code>bool</code> <p>If <code>True</code>, enforce that each present row <code>i</code> has an explicit self-edge <code>(i, i)</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>LayerMap</code> <p>A new instance with rows and per-row columns sorted deterministically.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If <code>require_diagonal=True</code> and some <code>(i, i)</code> is missing.</p>"},{"location":"reference/api/darnax/layer_maps/sparse/#darnax.layer_maps.sparse.LayerMap.tree_flatten","title":"<code>tree_flatten()</code>","text":"<p>Deconstruct into children and aux (PyTree protocol).</p> <p>Returns:</p> Name Type Description <code>children</code> <code>tuple[AbstractModule, ...]</code> <p>Modules in deterministic row-major order; JAX/Equinox will flatten their parameter fields further.</p> <code>aux</code> <code>tuple[Any, ...]</code> <p>Static metadata: <code>(rows, cols_per_row, ndim)</code> to reconstruct the treedef.</p> Notes <p>We intentionally do not include keys as children; keys are part of the static aux data so JIT sees a stable structure even when values change.</p>"},{"location":"reference/api/darnax/layer_maps/sparse/#darnax.layer_maps.sparse.LayerMap.tree_unflatten","title":"<code>tree_unflatten(aux, children)</code>  <code>classmethod</code>","text":"<p>Reconstruct from aux and children (PyTree protocol).</p> <p>Parameters:</p> Name Type Description Default <code>aux</code> <code>tuple[Any, ...]</code> <p>The static metadata returned by :meth:<code>tree_flatten</code>: <code>(rows, cols_per_row, ndim)</code>.</p> required <code>children</code> <code>Iterable[AbstractModule]</code> <p>Modules in the exact row-major order produced by :meth:<code>tree_flatten</code>.</p> required <p>Returns:</p> Type Description <code>LayerMap</code> <p>A new instance with the same static key layout and provided values.</p>"},{"location":"reference/api/darnax/layer_maps/sparse/#darnax.layer_maps.sparse.LayerMap.rows","title":"<code>rows()</code>","text":"<p>All row indices in sorted order (static).</p>"},{"location":"reference/api/darnax/layer_maps/sparse/#darnax.layer_maps.sparse.LayerMap.cols_of","title":"<code>cols_of(i)</code>","text":"<p>All column indices of row <code>i</code> in sorted order (static for a given map).</p>"},{"location":"reference/api/darnax/layer_maps/sparse/#darnax.layer_maps.sparse.LayerMap.neighbors","title":"<code>neighbors(i)</code>","text":"<p>Read-only mapping of neighbors (<code>col \u2192 module</code>) for row <code>i</code>.</p>"},{"location":"reference/api/darnax/layer_maps/sparse/#darnax.layer_maps.sparse.LayerMap.row_items","title":"<code>row_items(skip_last=False, subset='all')</code>","text":"<p>Iterate over rows with deterministic ordering and read-only views.</p> <p>Parameters:</p> Name Type Description Default <code>skip_last</code> <code>bool</code> <p>If <code>True</code>, omit the last receiver row (useful when the output row is sink-only).</p> <code>False</code> <code>subset</code> <code>['backward', 'forward', 'all']</code> <p>If <code>forward</code>, keep only edges <code>(i, j)</code> with <code>j &lt;= i</code> (i.e., lower-triangular including the diagonal), which is a common \u201cfeed-forward\u201d scheduling constraint. If <code>backward</code>, keep only edges <code>(i, j)</code> with <code>j &gt;= i</code> (i.e., upper-triangular including the diagonal).</p> <code>\"all</code> <p>Yields:</p> Type Description <code>(row, neighbors) : tuple[int, Mapping[int, AbstractModule]]</code> <p>The row index and a read-only mapping of its neighbors.</p>"},{"location":"reference/api/darnax/layer_maps/sparse/#darnax.layer_maps.sparse.LayerMap.edge_items","title":"<code>edge_items()</code>","text":"<p>Iterate over edges in deterministic row-major order.</p> <p>Yields:</p> Type Description <code>((i, j), module) : tuple[tuple[int, int], AbstractModule]</code> <p>Edge key and its module.</p>"},{"location":"reference/api/darnax/layer_maps/sparse/#darnax.layer_maps.sparse.LayerMap.to_dict","title":"<code>to_dict()</code>","text":"<p>Deep copy as a plain, mutable dict-of-dicts.</p> <p>Returns:</p> Type Description <code>dict[int, dict[int, AbstractModule]]</code> <p>A new mapping with the same keys and module values. Mutations on this result do not affect the original <code>LayerMap</code>.</p>"},{"location":"reference/api/darnax/modules/debug/","title":"darnax.modules.debug","text":"<p>Debug/test modules for Darnax.</p> <p>This module provides tiny implementations used to exercise the orchestration and documentation pipeline:</p> <ul> <li>:class:<code>DebugLayer</code>: a stateful layer with a single scalar parameter.</li> <li>:class:<code>DebugAdapter</code>: a stateless adapter with a single scalar parameter.</li> </ul> <p>They are intentionally simple and not meant for learning quality; they exist to validate wiring, typing, and update flows.</p>"},{"location":"reference/api/darnax/modules/debug/#darnax.modules.debug.DebugLayer","title":"<code>DebugLayer()</code>","text":"<p>               Bases: <code>Layer</code></p> <p>A minimal trainable layer for testing and debugging.</p> <p>Multiplies inputs elementwise by a learnable scalar <code>w</code>. The activation (:meth:<code>activation</code>) is the sign function, but the default forward pass (:meth:<code>__call__</code>) deliberately returns <code>w * x</code> to keep behavior simple.</p> <p>Attributes:</p> Name Type Description <code>w</code> <code>Array</code> <p>Learnable weight, shape <code>(1,)</code>, dtype <code>float32</code>.</p> <code>a</code> <code>bool</code> <p>Static flag (non-PyTree) to mark the object as a layer. Kept for debugging/demo purposes only.</p> Notes <ul> <li>This class is stateful (as all :class:<code>~darnax.modules.interfaces.Layer</code>).</li> <li>The local update (:meth:<code>backward</code>) is a no-op that returns <code>self</code>; it   exists to exercise the training loop, not to learn.</li> </ul> <p>Initialize parameters.</p> Notes <p>Sets <code>w = 1.0</code> (<code>float32</code>) and <code>a = True</code>.</p>"},{"location":"reference/api/darnax/modules/debug/#darnax.modules.debug.DebugLayer.w","title":"<code>w = jnp.ones((1,), dtype=(jnp.float32))</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/debug/#darnax.modules.debug.DebugLayer.a","title":"<code>a = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/debug/#darnax.modules.debug.DebugLayer.activation","title":"<code>activation(x)</code>","text":"<p>Apply the layer\u2019s activation function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Pre-activation tensor.</p> required <p>Returns:</p> Type Description <code>Array</code> <p><code>sign(x)</code>.</p>"},{"location":"reference/api/darnax/modules/debug/#darnax.modules.debug.DebugLayer.reduce","title":"<code>reduce(h)</code>","text":"<p>Aggregate incoming messages into a single tensor.</p> <p>Parameters:</p> Name Type Description Default <code>h</code> <code>PyTree</code> <p>PyTree of arrays to be summed.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Sum over all leaves in <code>h</code> via an associative reduction.</p> Notes <p>Uses :func:<code>jax.tree.reduce_associative</code> with <code>operator.add</code>.</p>"},{"location":"reference/api/darnax/modules/debug/#darnax.modules.debug.DebugLayer.backward","title":"<code>backward(x, y, y_hat, gate=None)</code>","text":"<p>Return a no-op parameter update.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Forward input (unused).</p> required <code>y</code> <code>Array</code> <p>Target/supervision (unused).</p> required <code>y_hat</code> <code>Array</code> <p>Prediction (unused).</p> required <code>gate</code> <code>Array | None</code> <p>Multiplicative gate (unused).</p> <code>None</code> <p>Returns:</p> Type Description <code>Self</code> <p><code>self</code> \u2014 identity update (no change).</p>"},{"location":"reference/api/darnax/modules/debug/#darnax.modules.debug.DebugAdapter","title":"<code>DebugAdapter()</code>","text":"<p>               Bases: <code>Adapter</code></p> <p>A minimal stateless mapping for testing and debugging.</p> <p>Multiplies inputs elementwise by a scalar <code>w</code>. Carries no persistent state and returns a no-op update in :meth:<code>backward</code>.</p> <p>Attributes:</p> Name Type Description <code>w</code> <code>Array</code> <p>Weight, shape <code>(1,)</code>, dtype <code>float32</code>.</p> <code>a</code> <code>bool</code> <p>Static flag (non-PyTree) to mark the object as an adapter.</p> <p>Initialize parameters.</p> Notes <p>Sets <code>w = 1.0</code> (<code>float32</code>) and <code>a = False</code>.</p>"},{"location":"reference/api/darnax/modules/debug/#darnax.modules.debug.DebugAdapter.w","title":"<code>w = jnp.ones((1,), dtype=(jnp.float32))</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/debug/#darnax.modules.debug.DebugAdapter.a","title":"<code>a = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/debug/#darnax.modules.debug.DebugAdapter.backward","title":"<code>backward(x, y, y_hat, gate=None)</code>","text":"<p>Return a no-op parameter update.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Forward input (unused).</p> required <code>y</code> <code>Array</code> <p>Target/supervision (unused).</p> required <code>y_hat</code> <code>Array</code> <p>Prediction (unused).</p> required <code>gate</code> <code>Array | None</code> <p>Multiplicative gate (unused).</p> <code>None</code> <p>Returns:</p> Type Description <code>Self</code> <p><code>self</code> \u2014 identity update (no change).</p>"},{"location":"reference/api/darnax/modules/ferromagnetic/","title":"darnax.modules.ferromagnetic","text":"<p>Adapters: fixed linear couplings.</p> <p>This module defines :class:<code>Ferromagnetic</code>, a stateless adapter that scales signals elementwise by a fixed coupling <code>strength</code> (scalar or per-feature).</p> <p>Adapters in Darnax are Equinox <code>Module</code>s and thus PyTrees; they should not hold persistent state and typically return zero updates in :meth:<code>backward</code>.</p>"},{"location":"reference/api/darnax/modules/ferromagnetic/#darnax.modules.ferromagnetic.KeyArray","title":"<code>KeyArray = Array</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/modules/ferromagnetic/#darnax.modules.ferromagnetic.Ferromagnetic","title":"<code>Ferromagnetic(features, strength, dtype=jnp.float32)</code>","text":"<p>               Bases: <code>Adapter</code></p> <p>Elementwise scaling adapter with fixed coupling strength.</p> <p>Multiplies inputs by a constant coupling vector <code>strength</code>. This module is stateless and non-trainable: its :meth:<code>backward</code> returns a zero-shaped update PyTree.</p> <p>Attributes:</p> Name Type Description <code>strength</code> <code>Array</code> <p>Coupling strengths, shape <code>(features,)</code>; may originate from a scalar broadcast to that shape at construction time.</p> <p>Construct the adapter with scalar or per-feature strength.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>int</code> <p>Number of features; determines the length of <code>strength</code>.</p> required <code>strength</code> <code>ArrayLike</code> <p>Either a scalar (broadcast to <code>(features,)</code>) or a 1D array of length <code>features</code>.</p> required <code>dtype</code> <code>DTypeLike</code> <p>Dtype for the internal <code>strength</code> array. Default is <code>float32</code>.</p> <code>float32</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>strength</code> is neither a scalar nor a 1D array of length <code>features</code>.</p>"},{"location":"reference/api/darnax/modules/ferromagnetic/#darnax.modules.ferromagnetic.Ferromagnetic.strength","title":"<code>strength = self._set_shape(strength, features, dtype)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/ferromagnetic/#darnax.modules.ferromagnetic.Ferromagnetic.backward","title":"<code>backward(x, y, y_hat, gate=None)</code>","text":"<p>Return a zero update to indicate non-trainability.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Forward input (unused).</p> required <code>y</code> <code>Array</code> <p>Target/supervision (unused).</p> required <code>y_hat</code> <code>Array</code> <p>Prediction (unused).</p> required <code>gate</code> <code>Array</code> <p>Multiplicative gate (unused).</p> <code>None</code> <p>Returns:</p> Type Description <code>Self</code> <p>A PyTree matching <code>self</code> where all leaves are zeros.</p>"},{"location":"reference/api/darnax/modules/fully_connected/","title":"darnax.modules.fully_connected","text":"<p>Fully connected adapters.</p> <p>This module provides two Equinox-based adapters:</p> <ul> <li>:class:<code>FullyConnected</code>: trainable affine map with per-output scaling and   a local perceptron-style update (only <code>W</code> is updated).</li> <li>:class:<code>FrozenFullyConnected</code>: same forward as <code>FullyConnected</code> but returns   a zero update (useful for inference or ablation).</li> </ul> <p>Both classes are stateless in the runtime sense (no persistent state across steps) but parameterized (PyTrees with trainable weights).</p>"},{"location":"reference/api/darnax/modules/fully_connected/#darnax.modules.fully_connected.KeyArray","title":"<code>KeyArray = Array</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/modules/fully_connected/#darnax.modules.fully_connected.FullyConnected","title":"<code>FullyConnected(in_features, out_features, strength, threshold, key, dtype=jnp.float32)</code>","text":"<p>               Bases: <code>Adapter</code></p> <p>Fully connected trainable adapter <code>y = (x @ W) * strength</code>.</p> <p>A dense linear projection followed by an elementwise per-output scaling. Learning uses a local perceptron-style rule parameterized by a per-output <code>threshold</code>; only <code>W</code> receives updates, while <code>strength</code> and <code>threshold</code> act as (learnable-if-you-want) hyperparameters that are not updated by :meth:<code>backward</code>.</p> <p>Attributes:</p> Name Type Description <code>W</code> <code>Array</code> <p>Weight matrix with shape <code>(in_features, out_features)</code>.</p> <code>strength</code> <code>Array</code> <p>Per-output scale, shape <code>(out_features,)</code>; broadcast across the last dimension of the forward output.</p> <code>threshold</code> <code>Array</code> <p>Per-output margin used by the local update rule, shape <code>(out_features,)</code>.</p> Notes <ul> <li>Adapters are stateless per the Darnax interface, but they may carry   trainable parameters. This class advertises trainability through <code>W</code>.</li> <li>The local rule is supplied by   :func:<code>darnax.utils.perceptron_rule.perceptron_rule_backward</code> and is not   required to be a gradient.</li> </ul> <p>Initialize weights and per-output scale/threshold.</p> <p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>int</code> <p>Input dimensionality.</p> required <code>out_features</code> <code>int</code> <p>Output dimensionality.</p> required <code>strength</code> <code>float or ArrayLike</code> <p>Scalar (broadcast to <code>(out_features,)</code>) or a vector of length <code>out_features</code> providing the per-output scaling.</p> required <code>threshold</code> <code>float or ArrayLike</code> <p>Scalar or vector of length <code>out_features</code> with the per-output margins used by the local update rule.</p> required <code>key</code> <code>Array</code> <p>JAX PRNG key to initialize <code>W</code> with Gaussian entries scaled by <code>1/sqrt(in_features)</code>.</p> required <code>dtype</code> <code>DTypeLike</code> <p>Dtype for parameters (default: <code>jnp.float32</code>).</p> <code>float32</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>strength</code> or <code>threshold</code> is neither a scalar nor a 1D array of the expected length.</p>"},{"location":"reference/api/darnax/modules/fully_connected/#darnax.modules.fully_connected.FullyConnected.strength","title":"<code>strength = self._set_shape(strength, out_features, dtype)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/fully_connected/#darnax.modules.fully_connected.FullyConnected.threshold","title":"<code>threshold = self._set_shape(threshold, out_features, dtype)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/fully_connected/#darnax.modules.fully_connected.FullyConnected.W","title":"<code>W = jax.random.normal(key, (in_features, out_features), dtype=dtype) * self.strength / jnp.sqrt(in_features)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/fully_connected/#darnax.modules.fully_connected.FullyConnected.backward","title":"<code>backward(x, y, y_hat, gate=None)</code>","text":"<p>Return a module-shaped local update where only <code>\u0394W</code> is set.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Forward input(s), shape <code>(..., in_features)</code>.</p> required <code>y</code> <code>Array</code> <p>Supervision signal/targets, broadcast-compatible with <code>y_hat</code>.</p> required <code>y_hat</code> <code>Array</code> <p>Current prediction/logits, broadcast-compatible with <code>y</code>.</p> required <code>gate</code> <code>Array</code> <p>Multiplicative gate applied to the update, default is <code>1.0</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Self</code> <p>A PyTree with the same structure as <code>self</code> where: - <code>W</code> holds the update <code>\u0394W</code> from the local rule, - <code>strength</code> and <code>threshold</code> leaves are zeros.</p> Notes <p>Calls :func:<code>darnax.utils.perceptron_rule.perceptron_rule_backward</code> with the stored per-output <code>threshold</code>.</p>"},{"location":"reference/api/darnax/modules/fully_connected/#darnax.modules.fully_connected.FrozenFullyConnected","title":"<code>FrozenFullyConnected(in_features, out_features, strength, threshold, key, dtype=jnp.float32)</code>","text":"<p>               Bases: <code>FullyConnected</code></p> <p>Fully connected adapter with frozen parameters.</p> <p>Same forward behavior as :class:<code>FullyConnected</code>, but :meth:<code>backward</code> returns zeros for all leaves. Useful for inference-only deployments or to ablate learning of a particular edge type in a graph.</p>"},{"location":"reference/api/darnax/modules/fully_connected/#darnax.modules.fully_connected.FrozenFullyConnected.backward","title":"<code>backward(x, y, y_hat, gate=None)</code>","text":"<p>Return zero update for all parameters.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Forward input (unused).</p> required <code>y</code> <code>Array</code> <p>Target/supervision (unused).</p> required <code>y_hat</code> <code>Array</code> <p>Prediction/logits (unused).</p> required <code>gate</code> <code>Array</code> <p>Multiplicative gate (unused).</p> <code>None</code> <p>Returns:</p> Type Description <code>Self</code> <p>PyTree of zeros with the same structure as <code>self</code>.</p>"},{"location":"reference/api/darnax/modules/fully_connected/#darnax.modules.fully_connected.SparseFullyConnected","title":"<code>SparseFullyConnected(in_features, out_features, strength, threshold, sparsity, key, dtype=jnp.float32)</code>","text":"<p>               Bases: <code>FullyConnected</code></p> <p>Fully connected adapter with a fixed binary sparsity mask.</p> <p>This variant samples a Bernoulli mask at initialization time and applies it multiplicatively to both the weights and their updates. Forward and local learning behave like :class:<code>FullyConnected</code>, but gradients/updates are constrained to the active connections.</p> <p>Attributes:</p> Name Type Description <code>W</code> <code>Array</code> <p>Weight matrix with shape <code>(in_features, out_features)</code>, masked by <code>_mask</code>.</p> <code>strength</code> <code>Array</code> <p>Per-output scale, shape <code>(out_features,)</code>; used in the same way as in :class:<code>FullyConnected</code> for initialization.</p> <code>threshold</code> <code>Array</code> <p>Per-output margin used by the local update rule, shape <code>(out_features,)</code>.</p> <code>_mask</code> <code>Array</code> <p>Binary mask with shape <code>(in_features, out_features)</code> indicating which connections are active (ones) or pruned (zeros).</p> Notes <p>The mask is sampled once at initialization and kept fixed. Updates produced by :meth:<code>backward</code> are re-masked so that pruned connections remain zero.</p> <p>Initialize sparse mask and masked weight matrix.</p> <p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>int</code> <p>Input dimensionality.</p> required <code>out_features</code> <code>int</code> <p>Output dimensionality.</p> required <code>strength</code> <code>float or ArrayLike</code> <p>Scalar (broadcast to <code>(out_features,)</code>) or a vector of length <code>out_features</code> providing the per-output scaling used in weight initialization.</p> required <code>threshold</code> <code>float or ArrayLike</code> <p>Scalar or vector of length <code>out_features</code> with the per-output margins used by the local update rule.</p> required <code>sparsity</code> <code>float</code> <p>Fraction of entries in <code>W</code> that are set to zero on average, in the interval <code>[0.0, 1.0)</code>. The mask keeps a fraction <code>1 - sparsity</code> of connections.</p> required <code>key</code> <code>Array</code> <p>JAX PRNG key used to sample both the weight matrix and the mask.</p> required <code>dtype</code> <code>DTypeLike</code> <p>Dtype for parameters (default: <code>jnp.float32</code>).</p> <code>float32</code> Notes <p>The weight matrix is initialized with Gaussian entries scaled by <code>strength / sqrt(in_features * (1 - sparsity))</code> and then multiplied by the sampled Bernoulli mask.</p>"},{"location":"reference/api/darnax/modules/fully_connected/#darnax.modules.fully_connected.SparseFullyConnected.strength","title":"<code>strength = self._set_shape(strength, out_features, dtype)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/fully_connected/#darnax.modules.fully_connected.SparseFullyConnected.threshold","title":"<code>threshold = self._set_shape(threshold, out_features, dtype)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/fully_connected/#darnax.modules.fully_connected.SparseFullyConnected.W","title":"<code>W = W * mask</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/fully_connected/#darnax.modules.fully_connected.SparseFullyConnected.backward","title":"<code>backward(x, y, y_hat, gate=None)</code>","text":"<p>Return a masked local update where only <code>\u0394W</code> is nonzero.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Forward input(s), shape <code>(..., in_features)</code>.</p> required <code>y</code> <code>Array</code> <p>Supervision signal/targets, broadcast-compatible with <code>y_hat</code>.</p> required <code>y_hat</code> <code>Array</code> <p>Current prediction/logits, broadcast-compatible with <code>y</code>.</p> required <code>gate</code> <code>Array or None</code> <p>Optional multiplicative gate applied elementwise to the update. If <code>None</code>, the gate is passed through unchanged to :func:<code>perceptron_rule_backward</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Self</code> <p>A PyTree with the same structure as <code>self</code> where: - <code>W</code> holds the masked update <code>\u0394W * _mask</code>, - <code>strength</code>, <code>threshold</code>, and <code>_mask</code> leaves are zeros.</p> Notes <p>The unmasked update is first computed via :func:<code>darnax.utils.perceptron_rule.perceptron_rule_backward</code> and then multiplied by the fixed binary mask so that pruned connections are never updated.</p>"},{"location":"reference/api/darnax/modules/fully_connected/#darnax.modules.fully_connected.FrozenRescaledFullyConnected","title":"<code>FrozenRescaledFullyConnected(in_features, out_features, strength, threshold, key, dtype=jnp.float32)</code>","text":"<p>               Bases: <code>FullyConnected</code></p> <p>Fully connected adapter with frozen parameters.</p> <p>Same forward behavior as :class:<code>FullyConnected</code>, but :meth:<code>backward</code> returns zeros for all leaves. Usually used to propagate information back from the label to internal layers. Before projecting, it applies the following rescaling <code>+1 -&gt; sqrt(C-1); -1 -&gt; 1/sqrt(C-1)</code>, where C is the input dimension (usually number of classes).</p>"},{"location":"reference/api/darnax/modules/fully_connected/#darnax.modules.fully_connected.FrozenRescaledFullyConnected.backward","title":"<code>backward(x, y, y_hat, gate=None)</code>","text":"<p>Return zero update for all parameters.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Forward input (unused).</p> required <code>y</code> <code>Array</code> <p>Target/supervision (unused).</p> required <code>y_hat</code> <code>Array</code> <p>Prediction/logits (unused).</p> required <code>gate</code> <code>Array</code> <p>Multiplicative gate (unused).</p> <code>None</code> <p>Returns:</p> Type Description <code>Self</code> <p>PyTree of zeros with the same structure as <code>self</code>.</p>"},{"location":"reference/api/darnax/modules/fully_connected/#darnax.modules.fully_connected.CEFullyConnected","title":"<code>CEFullyConnected(in_features, out_features, strength, threshold, key, dtype=jnp.float32)</code>","text":"<p>               Bases: <code>FullyConnected</code></p> <p>Fully connected adapter with cross-entropy-style local updates.</p> <p>This variant shares the same forward map as :class:<code>FullyConnected</code> but overrides :meth:<code>backward</code> to implement the local gradient of a softmax + cross-entropy loss with respect to <code>W</code>. Targets are assumed to be in <code>{-1, +1}</code> and are internally mapped to one-hot vectors in <code>{0, 1}</code>.</p> Notes <ul> <li>Only <code>W</code> is updated; <code>strength</code> and <code>threshold</code> are left at zero in   the returned update PyTree.</li> <li>The update is normalized by both the batch size and the input dimension,   to match the scale convention used by the perceptron-style rule.</li> </ul> <p>Initialize weights and hyperparameters for CE-based updates.</p> <p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>int</code> <p>Input dimensionality.</p> required <code>out_features</code> <code>int</code> <p>Output dimensionality (number of classes).</p> required <code>strength</code> <code>float or ArrayLike</code> <p>Scalar or vector of length <code>out_features</code> providing the per-output scaling used in weight initialization.</p> required <code>threshold</code> <code>float or ArrayLike</code> <p>Unused by the CE update but kept for API compatibility with :class:<code>FullyConnected</code>.</p> required <code>key</code> <code>Array</code> <p>JAX PRNG key used to initialize <code>W</code>.</p> required <code>dtype</code> <code>DTypeLike</code> <p>Dtype for parameters (default: <code>jnp.float32</code>).</p> <code>float32</code> Notes <p>Initialization is delegated to :class:<code>FullyConnected</code> via <code>super()</code>.</p>"},{"location":"reference/api/darnax/modules/fully_connected/#darnax.modules.fully_connected.CEFullyConnected.backward","title":"<code>backward(x, y, y_hat, gate=None)</code>","text":"<p>Return a CE/softmax local gradient update for <code>W</code>.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Forward inputs, shape <code>(B, in_features)</code> where <code>B</code> is the batch size.</p> required <code>y</code> <code>Array</code> <p>Targets in <code>{-1, +1}</code>, shape <code>(B, out_features)</code>. Values are internally mapped to one-hot class indicators via <code>(y + 1) / 2</code>.</p> required <code>y_hat</code> <code>Array</code> <p>Logits or pre-softmax activations, shape <code>(B, out_features)</code>.</p> required <code>gate</code> <code>Any or None</code> <p>Currently ignored; present for signature compatibility with the :class:<code>Adapter</code> interface.</p> <code>None</code> <p>Returns:</p> Type Description <code>Self</code> <p>A PyTree with the same structure as <code>self</code> where: - <code>W</code> holds the CE-based update <code>\u0394W</code>, - <code>strength</code> and <code>threshold</code> leaves are zeros.</p> Notes <p>The update implements</p> <p>.. math::</p> <pre><code>p &amp;= \\\\operatorname{softmax}(y_\\\\text{hat}) \\\\\\\\\n\\\\Delta W &amp;\\\\propto x^\\\\top (p - y_\\\\text{onehot})\n</code></pre> <p>scaled by the batch size and by <code>1 / sqrt(in_features)</code> so that its magnitude is comparable to the perceptron-style rule used in :class:<code>FullyConnected</code>.</p>"},{"location":"reference/api/darnax/modules/input_output/","title":"darnax.modules.input_output","text":"<p>Output layer utilities.</p> <p>This module defines :class:<code>OutputLayer</code>, a lightweight aggregation layer meant to sit at the boundary of the network. It provides:</p> <ul> <li>identity activation (no nonlinearity),</li> <li>identity forward pass (conceptually; see Notes),</li> <li>a reduce method that elementwise-sums all array leaves in a PyTree of   predictions,</li> <li>a no-op local update (:meth:<code>backward</code>) since there are no trainable   parameters.</li> </ul> Notes <p>Despite being conceptually stateless, :class:<code>OutputLayer</code> subclasses :class:<code>darnax.modules.interfaces.Layer</code> to reuse the orchestration contract, including the <code>reduce</code> interface.</p>"},{"location":"reference/api/darnax/modules/input_output/#darnax.modules.input_output.KeyArray","title":"<code>KeyArray = Array</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/modules/input_output/#darnax.modules.input_output.OutputLayer","title":"<code>OutputLayer</code>","text":"<p>               Bases: <code>Layer</code></p> <p>Simple output layer that aggregates predictions via summation.</p> <p>The layer leaves activations unchanged and defines a <code>reduce</code> that elementwise-sums a PyTree of predictions. Its backward pass is a no-op because it has no trainable parameters.</p> Notes <p>The current implementation of :meth:<code>__call__</code> returns <code>zeros_like(x)</code>.</p>"},{"location":"reference/api/darnax/modules/input_output/#darnax.modules.input_output.OutputLayer.reduce","title":"<code>reduce(h)</code>","text":"<p>Elementwise-sum all array leaves in a PyTree of predictions.</p> <p>Parameters:</p> Name Type Description Default <code>h</code> <code>PyTree</code> <p>PyTree whose leaves are arrays with identical shapes and dtypes.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>The elementwise sum across all leaves.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>h</code> has no leaves (as per <code>tree_reduce</code> semantics).</p> Notes <p>Uses :func:<code>jax.tree_util.tree_reduce</code> with :data:<code>operator.add</code>.</p>"},{"location":"reference/api/darnax/modules/input_output/#darnax.modules.input_output.OutputLayer.activation","title":"<code>activation(x)</code>","text":"<p>Identity activation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Array</code> <p><code>x</code> unchanged.</p>"},{"location":"reference/api/darnax/modules/input_output/#darnax.modules.input_output.OutputLayer.backward","title":"<code>backward(x, y, y_hat, gate=None)</code>","text":"<p>No-op local update.</p> <p>This layer has no trainable parameters, so it returns itself unchanged.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Forward input (unused).</p> required <code>y</code> <code>Array</code> <p>Target/supervision (unused).</p> required <code>y_hat</code> <code>Array</code> <p>Prediction (unused).</p> required <code>gate</code> <code>Array</code> <p>Multiplicative gate (unused).</p> <code>None</code> <p>Returns:</p> Type Description <code>Self</code> <p><code>self</code> (no parameter updates).</p>"},{"location":"reference/api/darnax/modules/interfaces/","title":"darnax.modules.interfaces","text":"<p>Interfaces for Darnax modules.</p> <p>This module defines the abstract contracts implemented by all computational components in Darnax:</p> <ul> <li><code>AbstractModule</code> is the common base for anything callable during the   recurrent dynamics (layers and adapters).</li> <li><code>Layer</code> is a stateful module that aggregates incoming messages and   applies an activation.</li> <li><code>Adapter</code> is a stateless mapping between layers (e.g., projections,   reshapes, or wiring).</li> </ul> <p>All classes are Equinox <code>Module</code>s (i.e., PyTrees) and are compatible with JAX transformations (<code>jit</code>, <code>vmap</code>, <code>grad</code>/custom rules, etc.).</p>"},{"location":"reference/api/darnax/modules/interfaces/#darnax.modules.interfaces.KeyArray","title":"<code>KeyArray = Array</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/modules/interfaces/#darnax.modules.interfaces.AbstractModule","title":"<code>AbstractModule</code>","text":"<p>               Bases: <code>Module</code>, <code>ABC</code></p> <p>Base class for layers and adapters.</p> <p>Subclasses must implement a pure functional forward pass and a local learning rule via :meth:<code>backward</code>. The object itself is a PyTree: parameter fields are leaves, and nested modules are subtrees. This makes instances compatible with Equinox/Optax update flows.</p> Notes <ul> <li>The forward pass must not mutate parameters or hidden state.   Any persistent state updates are orchestrated outside the call via the   training loop (see tutorials).</li> <li><code>rng</code> is optional; when provided, it should be a JAX PRNG key   (<code>KeyArray</code>). If <code>None</code>, the module must behave deterministically.</li> </ul>"},{"location":"reference/api/darnax/modules/interfaces/#darnax.modules.interfaces.AbstractModule.has_state","title":"<code>has_state</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Whether the module carries persistent state.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the module owns persistent state (e.g., activations, running stats, internal buffers) that is managed by the orchestrator/training loop; <code>False</code> otherwise.</p>"},{"location":"reference/api/darnax/modules/interfaces/#darnax.modules.interfaces.AbstractModule.backward","title":"<code>backward(x, y, y_hat, gate)</code>  <code>abstractmethod</code>","text":"<p>Compute a local parameter update (same PyTree structure).</p> <p>This method implements a local plasticity rule that produces a PyTree of updates aligned with the module's parameter structure. The returned object is typically consumed by an optimizer (e.g., Optax) or combined with other updates by the orchestrator.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Inputs seen at the forward call (may be cached by the caller).</p> required <code>y</code> <code>Array</code> <p>Supervision signal or target associated with the current step. Shape must be compatible with the module's output space.</p> required <code>y_hat</code> <code>Array</code> <p>The module's (or readout's) current prediction.</p> required <code>gate</code> <code>Array(optional)</code> <p>A multiplicative gate applied to the update. Shape must be broadcastable to x shapes.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>A PyTree with the same structure as this module, containing per-parameter updates (e.g., <code>dW</code>, <code>db</code>). Implementations may return zeros for non-trainable fields.</p> Notes <p>The update is not required to be a gradient. It can be any local rule (e.g., perceptron-style, Hebbian/anti-Hebbian) as long as the shape and PyTree layout match the module parameters.</p>"},{"location":"reference/api/darnax/modules/interfaces/#darnax.modules.interfaces.Layer","title":"<code>Layer</code>","text":"<p>               Bases: <code>AbstractModule</code>, <code>ABC</code></p> <p>A stateful, trainable layer that reduces messages then applies an activation.</p> <p>Layers are the primary compute units in Darnax. They typically: (1) aggregate incoming messages from upstream modules with :meth:<code>reduce</code>, and (2) transform the aggregate via :meth:<code>activation</code>.</p> Notes <ul> <li>Layers are stateful by design (e.g., carry hidden activations or   buffers across steps); the orchestrator is responsible for when/how state   is read/written. The State object carries the state.</li> <li>The forward pass should be purely functional with respect to parameters   and external state; do not mutate in-place.</li> </ul>"},{"location":"reference/api/darnax/modules/interfaces/#darnax.modules.interfaces.Layer.has_state","title":"<code>has_state</code>  <code>property</code>","text":"<p>Whether the layer carries persistent state.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Always <code>True</code> for layers.</p>"},{"location":"reference/api/darnax/modules/interfaces/#darnax.modules.interfaces.Layer.activation","title":"<code>activation(x)</code>  <code>abstractmethod</code>","text":"<p>Apply the layer\u2019s activation function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Pre-activation tensor (e.g., the result of :meth:<code>reduce</code>).</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Post-activation tensor. Typically the same shape as <code>x</code>.</p>"},{"location":"reference/api/darnax/modules/interfaces/#darnax.modules.interfaces.Layer.reduce","title":"<code>reduce(h)</code>  <code>abstractmethod</code>","text":"<p>Aggregate incoming messages into a single tensor.</p> <p>Parameters:</p> Name Type Description Default <code>h</code> <code>PyTree</code> <p>Collection of incoming messages from neighbors/upstream modules. Implementations define the exact structure; common reducers include sum, mean, or structured/sparse contractions.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Aggregated input to be passed to :meth:<code>activation</code>.</p> <p>Examples:</p> <p>A sum reducer over message leaves::</p> <pre><code>leaves = jax.tree_util.tree_leaves(h)\nx = jnp.sum(jnp.stack(leaves, axis=0), axis=0)\nreturn x\n</code></pre>"},{"location":"reference/api/darnax/modules/interfaces/#darnax.modules.interfaces.Adapter","title":"<code>Adapter</code>","text":"<p>               Bases: <code>AbstractModule</code>, <code>ABC</code></p> <p>A stateless mapping between layers.</p> <p>Adapters connect layers (e.g., linear projections, reshapes, sparsifying maps). They must not carry persistent state and should behave as pure functions of inputs and parameters (plus optional RNG).</p> Notes <ul> <li>Use adapters to express wiring and shape changes between layers.</li> <li>Because adapters are stateless, orchestration can freely reorder or   parallelize them as long as data dependencies are respected.</li> </ul>"},{"location":"reference/api/darnax/modules/interfaces/#darnax.modules.interfaces.Adapter.has_state","title":"<code>has_state</code>  <code>property</code>","text":"<p>Whether the adapter carries persistent state.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Always <code>False</code> for adapters.</p>"},{"location":"reference/api/darnax/modules/recurrent/","title":"darnax.modules.recurrent","text":"<p>Binary (\u00b11) recurrent layer with dense couplings.</p> <p>This module defines :class:<code>RecurrentDiscrete</code>, a fully connected recurrent layer whose states live in <code>{-1, +1}</code>. The coupling matrix <code>J</code> has a controllable diagonal <code>J_D</code> (self-couplings), and learning uses a local, perceptron-style rule with per-unit margins (<code>threshold</code>).</p> Design <ul> <li>State space: discrete, <code>s_i \u2208 {-1, +1}</code>.</li> <li>Couplings: dense matrix <code>J \u2208 \u211d^{d\u00d7d}</code>, diagonal forced to <code>J_D</code>.</li> <li>Forward: pre-activation <code>h = x @ J</code> (no in-place mutation).</li> <li>Activation: hard sign with ties to <code>+1</code>.</li> <li>Learning: :func:<code>darnax.utils.perceptron_rule.perceptron_rule_backward</code>   produces <code>\u0394J</code>; the diagonal update is masked to zero so self-couplings   remain fixed at <code>J_D</code>.</li> </ul> Notes <p>This class is an Equinox <code>Module</code> (a PyTree). Parameters are leaves and can be updated via Optax or custom update code. The orchestrator controls when to call <code>activation</code> vs forwarding pre-activations.</p> See Also <p>darnax.modules.interfaces.Layer darnax.utils.perceptron_rule.perceptron_rule_backward</p>"},{"location":"reference/api/darnax/modules/recurrent/#darnax.modules.recurrent.KeyArray","title":"<code>KeyArray = jax.Array</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/modules/recurrent/#darnax.modules.recurrent.RecurrentDiscrete","title":"<code>RecurrentDiscrete(features, j_d, threshold, key, strength=1.0, dtype=jnp.float32)</code>","text":"<p>               Bases: <code>Layer</code></p> <p>Binary (\u00b11) recurrent layer with dense couplings.</p> <p>The layer keeps a dense coupling matrix <code>J</code> (with fixed diagonal <code>J_D</code>), a per-unit margin <code>threshold</code> for local updates, and an internal diagonal mask used to zero out self-updates during learning.</p> <p>Attributes:</p> Name Type Description <code>J</code> <code>Array</code> <p>Coupling matrix with shape <code>(features, features)</code>.</p> <code>J_D</code> <code>Array</code> <p>Diagonal self-couplings, shape <code>(features,)</code>. Mirrors <code>jnp.diag(J)</code> and is kept fixed by masking during updates.</p> <code>threshold</code> <code>Array</code> <p>Per-unit margin used by the local perceptron-style rule, shape <code>(features,)</code>.</p> <code>strength</code> <code>float, default=1.0</code> <p>Scalar that multiplies all couplings at initialization to increase layer influence in the dynamics. Similar to strengths in fully connected and ferromagnetic adapters.</p> <code>_mask</code> <code>Array</code> <p>Binary matrix (<code>1 - I</code>) that zeroes the diagonal of <code>\u0394J</code> before applying updates. Same shape and dtype as <code>J</code>.</p> <p>Construct the layer parameters.</p> <p>Initializes a dense coupling matrix <code>J</code> with i.i.d. Gaussian entries scaled by <code>1/sqrt(features)</code> and sets its diagonal to <code>j_d</code>. Stores per-unit margins in <code>threshold</code> and a diagonal masking matrix to keep self-couplings fixed during learning.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>int</code> <p>Number of units (dimension <code>d</code>). Shapes are derived from this.</p> required <code>j_d</code> <code>ArrayLike</code> <p>Self-couplings (diagonal of <code>J</code>). Either a scalar (broadcast to <code>(features,)</code>) or a vector of length <code>features</code>.</p> required <code>threshold</code> <code>ArrayLike</code> <p>Per-unit margins for the local update rule. Scalar or vector of length <code>features</code>.</p> required <code>key</code> <code>KeyArray</code> <p>JAX PRNG key used to initialize the off-diagonal entries of <code>J</code>.</p> required <code>strength</code> <code>float</code> <p>Scalar that multiplies all couplings at initialization to increase layer influence in the dynamics. Similar to strengths in fully connected and ferromagnetic adapters.</p> <code>1.0</code> <code>dtype</code> <code>DTypeLike</code> <p>Parameter dtype, by default <code>jnp.float32</code>.</p> <code>float32</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>j_d</code> or <code>threshold</code> is not scalar or a 1D vector with length <code>features</code>.</p>"},{"location":"reference/api/darnax/modules/recurrent/#darnax.modules.recurrent.RecurrentDiscrete.J","title":"<code>J = J</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/recurrent/#darnax.modules.recurrent.RecurrentDiscrete.J_D","title":"<code>J_D = j_d_vec</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/recurrent/#darnax.modules.recurrent.RecurrentDiscrete.threshold","title":"<code>threshold = thresh_vec</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/recurrent/#darnax.modules.recurrent.RecurrentDiscrete.strength","title":"<code>strength = strength_vec</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/recurrent/#darnax.modules.recurrent.RecurrentDiscrete.activation","title":"<code>activation(x)</code>","text":"<p>Hard-sign activation mapping ties to <code>+1</code>.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Pre-activation tensor.</p> required <p>Returns:</p> Type Description <code>Array</code> <p><code>(+1)</code> where <code>x &gt;= 0</code> and <code>(-1)</code> otherwise, cast to <code>x.dtype</code>.</p> Notes <p>This function is separate from :meth:<code>__call__</code> so orchestrators can decide when to discretize (e.g., training vs inference dynamics).</p>"},{"location":"reference/api/darnax/modules/recurrent/#darnax.modules.recurrent.RecurrentDiscrete.reduce","title":"<code>reduce(h)</code>","text":"<p>Aggregate incoming messages by summation.</p> <p>Parameters:</p> Name Type Description Default <code>h</code> <code>PyTree</code> <p>PyTree of arrays (e.g., messages from neighbors) to be summed.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Elementwise sum over all leaves in <code>h</code>.</p> Notes <p>Uses :func:<code>jax.tree_util.tree_reduce</code> with :data:<code>operator.add</code>.</p>"},{"location":"reference/api/darnax/modules/recurrent/#darnax.modules.recurrent.RecurrentDiscrete.backward","title":"<code>backward(x, y, y_hat, gate=None)</code>","text":"<p>Compute a module-shaped local update.</p> <p>Produces a PyTree of updates where only <code>J</code> receives a nonzero <code>\u0394J</code>; all other fields are zero. The diagonal of <code>\u0394J</code> is masked to zero so self-couplings stay fixed at <code>J_D</code>.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Inputs used to produce the current prediction. Shape <code>(features,)</code> or <code>(batch, features)</code>.</p> required <code>y</code> <code>Array</code> <p>Supervision signal/targets, broadcast-compatible with <code>y_hat</code>.</p> required <code>y_hat</code> <code>Array</code> <p>Current prediction/logits, broadcast-compatible with <code>y</code>.</p> required <code>gate</code> <code>Array</code> <p>A multiplicative gate applied to the update. Shape must be broadcastable to x shapes.</p> <code>None</code> <p>Returns:</p> Type Description <code>Self</code> <p>A PyTree with the same structure as <code>self</code> where: - <code>J</code> contains <code>\u0394J</code> (diagonal zeroed), - all other leaves are zeros.</p> Notes <p>Calls :func:<code>darnax.utils.perceptron_rule.perceptron_rule_backward</code> with the stored per-unit <code>threshold</code>. The rule is local and need not be a true gradient.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; upd = layer.backward(x, y, y_hat)\n&gt;&gt;&gt; new_params = eqx.tree_at(lambda m: m.J, layer, layer.J + lr * upd.J)\n</code></pre>"},{"location":"reference/api/darnax/modules/recurrent/#darnax.modules.recurrent.SparseRecurrentDiscrete","title":"<code>SparseRecurrentDiscrete(features, j_d, sparsity, threshold, key, strength=1.0, dtype=jnp.float32)</code>","text":"<p>               Bases: <code>Layer</code></p> <p>Binary (\u00b11) recurrent layer with SPARSE couplings.</p> <p>The layer keeps a dense coupling matrix <code>J</code> (with fixed diagonal <code>J_D</code>), a per-unit margin <code>threshold</code> for local updates, and an internal diagonal mask used to zero out self-updates during learning.</p> <p>Attributes:</p> Name Type Description <code>J</code> <code>Array</code> <p>Coupling matrix with shape <code>(features, features)</code>.</p> <code>J_D</code> <code>Array</code> <p>Diagonal self-couplings, shape <code>(features,)</code>. Mirrors <code>jnp.diag(J)</code> and is kept fixed by masking during updates.</p> <code>sparsity</code> <code>float</code> <p>Fraction of zero entries in J.</p> <code>threshold</code> <code>Array</code> <p>Per-unit margin used by the local perceptron-style rule, shape <code>(features,)</code>.</p> <code>strength</code> <code>float, default=1.0</code> <p>Scalar that multiplies all couplings at initialization to increase layer influence in the dynamics. Similar to strengths in fully connected and ferromagnetic adapters.</p> <code>_mask</code> <code>Array</code> <p>Binary matrix (<code>1 - I</code>) that zeroes the diagonal of <code>\u0394J</code> before applying updates. Same shape and dtype as <code>J</code>.</p> <p>Construct the layer parameters.</p> <p>Initializes a dense coupling matrix <code>J</code> with i.i.d. Gaussian entries scaled by <code>1/sqrt(features)</code> and sets its diagonal to <code>j_d</code>. Stores per-unit margins in <code>threshold</code> and a diagonal masking matrix to keep self-couplings fixed during learning.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>int</code> <p>Number of units (dimension <code>d</code>). Shapes are derived from this.</p> required <code>j_d</code> <code>ArrayLike</code> <p>Self-couplings (diagonal of <code>J</code>). Either a scalar (broadcast to <code>(features,)</code>) or a vector of length <code>features</code>.</p> required <code>sparsity</code> <code>float</code> <p>in (0.0, 1.0]. Defines the percentage of coupling set to zero.</p> required <code>threshold</code> <code>ArrayLike</code> <p>Per-unit margins for the local update rule. Scalar or vector of length <code>features</code>.</p> required <code>key</code> <code>KeyArray</code> <p>JAX PRNG key used to initialize the off-diagonal entries of <code>J</code>.</p> required <code>strength</code> <code>float</code> <p>Scalar that multiplies all couplings at initialization to increase layer influence in the dynamics. Similar to strengths in fully connected and ferromagnetic adapters.</p> <code>1.0</code> <code>dtype</code> <code>DTypeLike</code> <p>Parameter dtype, by default <code>jnp.float32</code>.</p> <code>float32</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>j_d</code> or <code>threshold</code> is not scalar or a 1D vector with length <code>features</code>.</p>"},{"location":"reference/api/darnax/modules/recurrent/#darnax.modules.recurrent.SparseRecurrentDiscrete.J","title":"<code>J = J</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/recurrent/#darnax.modules.recurrent.SparseRecurrentDiscrete.J_D","title":"<code>J_D = j_d_vec</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/recurrent/#darnax.modules.recurrent.SparseRecurrentDiscrete.threshold","title":"<code>threshold = thresh_vec</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/recurrent/#darnax.modules.recurrent.SparseRecurrentDiscrete.strength","title":"<code>strength = strength_vec</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/recurrent/#darnax.modules.recurrent.SparseRecurrentDiscrete.activation","title":"<code>activation(x)</code>","text":"<p>Hard-sign activation mapping ties to <code>+1</code>.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Pre-activation tensor.</p> required <p>Returns:</p> Type Description <code>Array</code> <p><code>(+1)</code> where <code>x &gt;= 0</code> and <code>(-1)</code> otherwise, cast to <code>x.dtype</code>.</p> Notes <p>This function is separate from :meth:<code>__call__</code> so orchestrators can decide when to discretize (e.g., training vs inference dynamics).</p>"},{"location":"reference/api/darnax/modules/recurrent/#darnax.modules.recurrent.SparseRecurrentDiscrete.reduce","title":"<code>reduce(h)</code>","text":"<p>Aggregate incoming messages by summation.</p> <p>Parameters:</p> Name Type Description Default <code>h</code> <code>PyTree</code> <p>PyTree of arrays (e.g., messages from neighbors) to be summed.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Elementwise sum over all leaves in <code>h</code>.</p> Notes <p>Uses :func:<code>jax.tree_util.tree_reduce</code> with :data:<code>operator.add</code>.</p>"},{"location":"reference/api/darnax/modules/recurrent/#darnax.modules.recurrent.SparseRecurrentDiscrete.backward","title":"<code>backward(x, y, y_hat, gate=None)</code>","text":"<p>Compute a module-shaped local update.</p> <p>Produces a PyTree of updates where only <code>J</code> receives a nonzero <code>\u0394J</code>; all other fields are zero. The diagonal of <code>\u0394J</code> is masked to zero so self-couplings stay fixed at <code>J_D</code>.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Inputs used to produce the current prediction. Shape <code>(features,)</code> or <code>(batch, features)</code>.</p> required <code>y</code> <code>Array</code> <p>Supervision signal/targets, broadcast-compatible with <code>y_hat</code>.</p> required <code>y_hat</code> <code>Array</code> <p>Current prediction/logits, broadcast-compatible with <code>y</code>.</p> required <code>gate</code> <code>Array</code> <p>Multiplicative gate (unused).</p> <code>None</code> <p>Returns:</p> Type Description <code>Self</code> <p>A PyTree with the same structure as <code>self</code> where: - <code>J</code> contains <code>\u0394J</code> (diagonal zeroed), - all other leaves are zeros.</p> Notes <p>Calls :func:<code>darnax.utils.perceptron_rule.perceptron_rule_backward</code> with the stored per-unit <code>threshold</code>. The rule is local and need not be a true gradient.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; upd = layer.backward(x, y, y_hat)\n&gt;&gt;&gt; new_params = eqx.tree_at(lambda m: m.J, layer, layer.J + lr * upd.J)\n</code></pre>"},{"location":"reference/api/darnax/modules/recurrent_tanh/","title":"darnax.modules.recurrent_tanh","text":"<p>Continouos [-1, 1] recurrent layer with dense couplings.</p> <p>This module defines :class:<code>RecurrentTanh</code>, a fully connected recurrent layer whose states live in <code>[-1, +1]</code> (continuous). The coupling matrix <code>J</code> has a controllable diagonal <code>J_D</code> (self-couplings), and learning uses a local, perceptron-style rule with per-unit tolerance (<code>tolerance</code>). The delta rule is applied.</p> Design <ul> <li>State space: continuous, <code>s_i \u2208 [-1, +1]</code>.</li> <li>Couplings: dense matrix <code>J \u2208 \u211d^{d\u00d7d}</code>, diagonal forced to <code>J_D</code>.</li> <li>Forward: pre-activation <code>h = x @ J</code> (no in-place mutation).</li> <li>Activation: tanh().</li> <li>Learning: :func:<code>darnax.utils.cont_perceptron_rule.tanh_perceptron_rule_backward</code>   produces <code>\u0394J</code>; the diagonal update is masked to zero so self-couplings   remain fixed at <code>J_D</code>.</li> </ul> Notes <p>This class is an Equinox <code>Module</code> (a PyTree). Parameters are leaves and can be updated via Optax or custom update code. The orchestrator controls when to call <code>activation</code> vs forwarding pre-activations.</p> See Also <p>darnax.modules.interfaces.Layer darnax.utils.perceptron_rule.perceptron_rule_backward</p>"},{"location":"reference/api/darnax/modules/recurrent_tanh/#darnax.modules.recurrent_tanh.KeyArray","title":"<code>KeyArray = jax.Array</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/modules/recurrent_tanh/#darnax.modules.recurrent_tanh.RecurrentTanh","title":"<code>RecurrentTanh(features, j_d, tolerance, key, strength=1.0, dtype=jnp.float32)</code>","text":"<p>               Bases: <code>Layer</code></p> <p>Continuous [-1, +1] recurrent layer with dense couplings.</p> <p>The layer keeps a dense coupling matrix <code>J</code> (with fixed diagonal <code>J_D</code>), a per-unit tolerance <code>tolerance</code> for local updates, and an internal diagonal mask used to zero out self-updates during learning.</p> <p>Attributes:</p> Name Type Description <code>J</code> <code>Array</code> <p>Coupling matrix with shape <code>(features, features)</code>.</p> <code>J_D</code> <code>Array</code> <p>Diagonal self-couplings, shape <code>(features,)</code>. Mirrors <code>jnp.diag(J)</code> and is kept fixed by masking during updates.</p> <code>tolerance</code> <code>Array</code> <p>Per-unit tolerance used by the local perceptron-style rule, shape <code>(features,)</code>.</p> <code>strength</code> <code>float, default=1.0</code> <p>Scalar that multiplies all couplings at initialization to increase layer influence in the dynamics. Similar to strengths in fully connected and ferromagnetic adapters.</p> <code>_mask</code> <code>Array</code> <p>Binary matrix (<code>1 - I</code>) that zeroes the diagonal of <code>\u0394J</code> before applying updates. Same shape and dtype as <code>J</code>.</p> <p>Construct the layer parameters.</p> <p>Initializes a dense coupling matrix <code>J</code> with i.i.d. Gaussian entries scaled by <code>1/sqrt(features)</code> and sets its diagonal to <code>j_d</code>. Stores per-unit margins in <code>threshold</code> and a diagonal masking matrix to keep self-couplings fixed during learning.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>int</code> <p>Number of units (dimension <code>d</code>). Shapes are derived from this.</p> required <code>j_d</code> <code>ArrayLike</code> <p>Self-couplings (diagonal of <code>J</code>). Either a scalar (broadcast to <code>(features,)</code>) or a vector of length <code>features</code>.</p> required <code>tolerance</code> <code>ArrayLike</code> <p>Per-unit tolerances for the local update rule. Scalar or vector of length <code>features</code>.</p> required <code>key</code> <code>KeyArray</code> <p>JAX PRNG key used to initialize the off-diagonal entries of <code>J</code>.</p> required <code>strength</code> <code>float</code> <p>Scalar that multiplies all couplings at initialization to increase layer influence in the dynamics. Similar to strengths in fully connected and ferromagnetic adapters.</p> <code>1.0</code> <code>dtype</code> <code>DTypeLike</code> <p>Parameter dtype, by default <code>jnp.float32</code>.</p> <code>float32</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>j_d</code> or <code>tolerance</code> is not scalar or a 1D vector with length <code>features</code>.</p>"},{"location":"reference/api/darnax/modules/recurrent_tanh/#darnax.modules.recurrent_tanh.RecurrentTanh.J","title":"<code>J = J</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/recurrent_tanh/#darnax.modules.recurrent_tanh.RecurrentTanh.J_D","title":"<code>J_D = j_d_vec</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/recurrent_tanh/#darnax.modules.recurrent_tanh.RecurrentTanh.tolerance","title":"<code>tolerance = tol_vec</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/recurrent_tanh/#darnax.modules.recurrent_tanh.RecurrentTanh.strength","title":"<code>strength = strength_vec</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/recurrent_tanh/#darnax.modules.recurrent_tanh.RecurrentTanh.activation","title":"<code>activation(x)</code>","text":"<p>Tanh activation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Pre-activation tensor.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>tanh(x) elementwise, cast to <code>x.dtype</code>.</p> Notes <p>This function is separate from :meth:<code>__call__</code> so orchestrators can decide when to discretize (e.g., training vs inference dynamics).</p>"},{"location":"reference/api/darnax/modules/recurrent_tanh/#darnax.modules.recurrent_tanh.RecurrentTanh.reduce","title":"<code>reduce(h)</code>","text":"<p>Aggregate incoming messages by summation.</p> <p>Parameters:</p> Name Type Description Default <code>h</code> <code>PyTree</code> <p>PyTree of arrays (e.g., messages from neighbors) to be summed.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Elementwise sum over all leaves in <code>h</code>.</p> Notes <p>Uses :func:<code>jax.tree_util.tree_reduce</code> with :data:<code>operator.add</code>.</p>"},{"location":"reference/api/darnax/modules/recurrent_tanh/#darnax.modules.recurrent_tanh.RecurrentTanh.backward","title":"<code>backward(x, y, y_hat, gate=None)</code>","text":"<p>Compute a module-shaped local update.</p> <p>Produces a PyTree of updates where only <code>J</code> receives a nonzero <code>\u0394J</code>; all other fields are zero. The diagonal of <code>\u0394J</code> is masked to zero so self-couplings stay fixed at <code>J_D</code>.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Inputs used to produce the current prediction. Shape <code>(features,)</code> or <code>(batch, features)</code>.</p> required <code>y</code> <code>Array</code> <p>Supervision signal/targets, broadcast-compatible with <code>y_hat</code>.</p> required <code>y_hat</code> <code>Array</code> <p>Current prediction/logits, broadcast-compatible with <code>y</code>.</p> required <code>gate</code> <code>Array | None</code> <p>Multiplicative gate applied to the update, default is <code>1.0</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Self</code> <p>A PyTree with the same structure as <code>self</code> where: - <code>J</code> contains <code>\u0394J</code> (diagonal zeroed), - all other leaves are zeros.</p> Notes <p>Calls :func:<code>darnax.utils.cont_perceptron_rule.tanh_perceptron_rule_backward</code> with the stored per-unit <code>threshold</code>. The rule is local and need not be a true gradient.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; upd = layer.backward(x, y, y_hat)\n&gt;&gt;&gt; new_params = eqx.tree_at(lambda m: m.J, layer, layer.J + lr * upd.J)\n</code></pre>"},{"location":"reference/api/darnax/modules/recurrent_tanh/#darnax.modules.recurrent_tanh.RecurrentTanhTruncated","title":"<code>RecurrentTanhTruncated(features, j_d, tolerance, key, threshold, strength=1.0, dtype=jnp.float32)</code>","text":"<p>               Bases: <code>RecurrentTanh</code></p> <p>Continuous [-1, +1] recurrent layer with dense couplings.</p> <p>The layer keeps a dense coupling matrix <code>J</code> (with fixed diagonal <code>J_D</code>), a per-unit tolerance <code>tolerance</code> for local updates, and an internal diagonal mask used to zero out self-updates during learning.</p> <p>It is essentially the same as RecurrentTanh, except for the update rule. In this, during update, we treat the neurons as discrete, we apply the local discrete perceptron rule in <code>utils.perceptron_rule.perceptron_rule_backward</code>, but we consider only the units <code>s_i</code> where <code>1 - |s_i| &lt; tolerance</code></p> <p>Attributes:</p> Name Type Description <code>J</code> <code>Array</code> <p>Coupling matrix with shape <code>(features, features)</code>.</p> <code>J_D</code> <code>Array</code> <p>Diagonal self-couplings, shape <code>(features,)</code>. Mirrors <code>jnp.diag(J)</code> and is kept fixed by masking during updates.</p> <code>tolerance</code> <code>Array</code> <p>Per-unit tolerance used by the local perceptron-style rule, shape <code>(features,)</code>.</p> <code>threshold</code> <code>Array</code> <p>Per-unit threshold used by the local perceptron-style rule, shape <code>(features,)</code>.</p> <code>strength</code> <code>float, default=1.0</code> <p>Scalar that multiplies all couplings at initialization to increase layer influence in the dynamics. Similar to strengths in fully connected and ferromagnetic adapters.</p> <code>_mask</code> <code>Array</code> <p>Binary matrix (<code>1 - I</code>) that zeroes the diagonal of <code>\u0394J</code> before applying updates. Same shape and dtype as <code>J</code>.</p> <p>Construct the layer parameters.</p> <p>Initializes a dense coupling matrix <code>J</code> with i.i.d. Gaussian entries scaled by <code>1/sqrt(features)</code> and sets its diagonal to <code>j_d</code>. Stores per-unit margins in <code>threshold</code> and a diagonal masking matrix to keep self-couplings fixed during learning.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>int</code> <p>Number of units (dimension <code>d</code>). Shapes are derived from this.</p> required <code>j_d</code> <code>ArrayLike</code> <p>Self-couplings (diagonal of <code>J</code>). Either a scalar (broadcast to <code>(features,)</code>) or a vector of length <code>features</code>.</p> required <code>tolerance</code> <code>ArrayLike</code> <p>Per-unit tolerances for the local update rule. Scalar or vector of length <code>features</code>.</p> required <code>threshold</code> <code>ArrayLike</code> <p>Per-unit threshold used by the local perceptron-style rule, shape <code>(features,)</code>.</p> required <code>key</code> <code>KeyArray</code> <p>JAX PRNG key used to initialize the off-diagonal entries of <code>J</code>.</p> required <code>strength</code> <code>float</code> <p>Scalar that multiplies all couplings at initialization to increase layer influence in the dynamics. Similar to strengths in fully connected and ferromagnetic adapters.</p> <code>1.0</code> <code>dtype</code> <code>DTypeLike</code> <p>Parameter dtype, by default <code>jnp.float32</code>.</p> <code>float32</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>j_d</code> or <code>tolerance</code> is not scalar or a 1D vector with length <code>features</code>.</p>"},{"location":"reference/api/darnax/modules/recurrent_tanh/#darnax.modules.recurrent_tanh.RecurrentTanhTruncated.J","title":"<code>J</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/recurrent_tanh/#darnax.modules.recurrent_tanh.RecurrentTanhTruncated.J_D","title":"<code>J_D</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/recurrent_tanh/#darnax.modules.recurrent_tanh.RecurrentTanhTruncated.tolerance","title":"<code>tolerance</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/recurrent_tanh/#darnax.modules.recurrent_tanh.RecurrentTanhTruncated.strength","title":"<code>strength</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/recurrent_tanh/#darnax.modules.recurrent_tanh.RecurrentTanhTruncated.threshold","title":"<code>threshold = threshold_vec</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/recurrent_tanh/#darnax.modules.recurrent_tanh.RecurrentTanhTruncated.backward","title":"<code>backward(x, y, y_hat, gate=None)</code>","text":"<p>Compute a module-shaped local update.</p> <p>Produces a PyTree of updates where only <code>J</code> receives a nonzero <code>\u0394J</code>; all other fields are zero. The diagonal of <code>\u0394J</code> is masked to zero so self-couplings stay fixed at <code>J_D</code>.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Inputs used to produce the current prediction. Shape <code>(features,)</code> or <code>(batch, features)</code>.</p> required <code>y</code> <code>Array</code> <p>Supervision signal/targets, broadcast-compatible with <code>y_hat</code>.</p> required <code>y_hat</code> <code>Array</code> <p>Current prediction/logits, broadcast-compatible with <code>y</code>.</p> required <code>gate</code> <code>Array | None</code> <p>Multiplicative gate applied to the update, default is <code>1.0</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Self</code> <p>A PyTree with the same structure as <code>self</code> where: - <code>J</code> contains <code>\u0394J</code> (diagonal zeroed), - all other leaves are zeros.</p> Notes <p>Calls :func:<code>darnax.utils.perceptron_rule.perceptron_rule_backward</code> with the stored per-unit <code>tolerance</code>, applies the rule only for units <code>s_i</code> such that <code>1-|s_i| &lt; tol</code> The rule is local and need not be a true gradient.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; upd = layer.backward(x, y, y_hat)\n&gt;&gt;&gt; new_params = eqx.tree_at(lambda m: m.J, layer, layer.J + lr * upd.J)\n</code></pre>"},{"location":"reference/api/darnax/orchestrators/interface/","title":"darnax.orchestrators.interface","text":"<p>Abstract orchestrator interface.</p> <p>An orchestrator schedules message passing, state updates, and local learning over a :class:<code>~darnax.layer_maps.sparse.LayerMap</code>. It owns no parameters itself; it routes tensors through modules, updates a global :class:<code>~darnax.states.interface.State</code>, and returns parameter updates shaped like the layermap.</p> Responsibilities <ul> <li>step: run one forward/update pass that refreshes non-output buffers   (keeps the output buffer unchanged). Typically used inside a recurrent loop.</li> <li>step_inference: like <code>step</code> but restricted to a one-sided schedule   (e.g., skip \u201cright-going\u201d messages) for cheaper partial updates; output   remains unchanged.</li> <li>predict: compute the output buffer from the current internal buffers   (no learning).</li> <li>backward: compute a PyTree of parameter updates with the same structure   as the layermap (suitable for Optax <code>update/apply_updates</code>).</li> </ul> RNG handling <p>Each method accepts an RNG key and returns a new RNG key. Implementations should split the key to ensure reproducible, side-effect-free randomness.</p> See Also <p>tutorials/05_orchestrators.md tutorials/06_simple_net_on_artificial_data.md</p>"},{"location":"reference/api/darnax/orchestrators/interface/#darnax.orchestrators.interface.ModuleT","title":"<code>ModuleT = TypeVar('ModuleT', bound='AbstractModule')</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/orchestrators/interface/#darnax.orchestrators.interface.StateT","title":"<code>StateT = TypeVar('StateT', bound='State')</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/orchestrators/interface/#darnax.orchestrators.interface.KeyArray","title":"<code>KeyArray = jax.Array</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/orchestrators/interface/#darnax.orchestrators.interface.AbstractOrchestrator","title":"<code>AbstractOrchestrator</code>","text":"<p>               Bases: <code>Module</code>, <code>Generic[StateT]</code></p> <p>Handle communication and scheduling over a :class:<code>LayerMap</code>.</p> <p>The orchestrator defines how modules exchange messages (graph traversal, fan-in reduction, activation timing) and when learning rules are applied. Concrete subclasses encode the schedule (e.g., left\u2192right passes, recurrent sweeps, or sparsity-aware traversals).</p> <p>Attributes:</p> Name Type Description <code>lmap</code> <code>LayerMap</code> <p>The static adjacency (rows/cols and order). Module parameters inside the layermap are PyTree leaves visible to JAX/Optax; the key layout is static for JIT stability.</p> Notes <ul> <li>Methods are expected to be pure with respect to inputs: given a state   and RNG, they return a new state and a new RNG (no in-place mutation).</li> <li>Use :func:<code>jax.random.split</code> to manage randomness. Determinism under JIT   requires not reusing RNG keys.</li> <li><code>backward</code> returns updates (same PyTree shape as <code>lmap</code>), not new   parameters. Applying those updates is the optimizer\u2019s job.</li> </ul>"},{"location":"reference/api/darnax/orchestrators/interface/#darnax.orchestrators.interface.AbstractOrchestrator.lmap","title":"<code>lmap</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/orchestrators/interface/#darnax.orchestrators.interface.AbstractOrchestrator.step","title":"<code>step(state, rng, *, filter_messages='all', skip_output_state=True)</code>  <code>abstractmethod</code>","text":"<p>Run one forward/update step without touching the output buffer.</p> <p>Typical use is inside a recurrent loop to evolve hidden buffers while deferring the explicit output computation to :meth:<code>predict</code>.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>StateT</code> <p>Current global state (input at index <code>0</code>, output at <code>-1</code>).</p> required <code>rng</code> <code>Array</code> <p>PRNG key for any stochastic modules. Implementations should split the key internally.</p> required <code>filter_messages</code> <code>Literal[\"all\", \"forward\", \"backward\"]. Default: \"all\"</code> <p>Only a subset of the messages are sent during the step. If forward, only forward messages (lower-triangle and diagonal) are computed. Same for backward. \"All\" computes all the messagesV</p> <code>'all'</code> <code>skip_output_state</code> <code>bool. Deafault: true.</code> <p>If true, we only update internal states (we exclude output state). The idea is that somehow the output is clamped and in some learning phases updating it is useless.</p> <code>True</code> <p>Returns:</p> Type Description <code>(new_state, new_rng) : tuple[StateT, jax.Array]</code> <p>The updated state (same type) and a fresh RNG key.</p> Notes <ul> <li>Scheduling is defined by the subclass (e.g., process edges with   <code>j &lt;= i</code> only, or a full sweep excluding output).</li> </ul>"},{"location":"reference/api/darnax/orchestrators/interface/#darnax.orchestrators.interface.AbstractOrchestrator.step_inference","title":"<code>step_inference(state, rng)</code>  <code>abstractmethod</code>","text":"<p>Run a cheaper, inference-oriented step (output unchanged).</p> <p>This variant avoids computing messages that travel \u201cto the right\u201d (exact meaning is schedule-specific; commonly skip edges with <code>j &gt; i</code>). Useful for partial refreshes of upstream buffers.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>StateT</code> <p>Current global state.</p> required <code>rng</code> <code>Array</code> <p>PRNG key to split.</p> required <p>Returns:</p> Type Description <code>(new_state, new_rng) : tuple[StateT, jax.Array]</code> <p>The updated state and a fresh RNG key.</p> Notes <p>Must not modify the output buffer <code>state[-1]</code>.</p>"},{"location":"reference/api/darnax/orchestrators/interface/#darnax.orchestrators.interface.AbstractOrchestrator.predict","title":"<code>predict(state, *, rng)</code>  <code>abstractmethod</code>","text":"<p>Compute/refresh the output buffer from current internal buffers.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>StateT</code> <p>Current global state (uses existing hidden buffers).</p> required <code>rng</code> <code>Array</code> <p>PRNG key to split, if output computation is stochastic.</p> required <p>Returns:</p> Type Description <code>(new_state, new_rng) : tuple[StateT, jax.Array]</code> <p>A state where <code>state[-1]</code> has been updated, and a fresh RNG key.</p> Notes <p>This method performs no parameter updates; it\u2019s a pure readout pass.</p>"},{"location":"reference/api/darnax/orchestrators/interface/#darnax.orchestrators.interface.AbstractOrchestrator.backward","title":"<code>backward(state, rng, *, filter_messages='forward', target_state=None, gate=None)</code>  <code>abstractmethod</code>","text":"<p>Compute parameter updates aligned with the layermap structure.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>StateT</code> <p>Current global state providing the activations/messages required by local learning rules (e.g., perceptron updates).</p> required <code>rng</code> <code>Array</code> <p>PRNG key to split for stochastic update rules.</p> required <code>filter_messages</code> <code>Literal['all', 'forward', 'backward']</code> <p>Optionally, it is possible to decide to compute the local fields (the messages) based on a different subset of the messages. Forward corresponds to the default behaviour, where only forward messages contribute to the messages.</p> <code>'forward'</code> <code>target_state</code> <code>StateT | None</code> <p>Optionally, it is possible to mix states in the backward update rule. By default, other_state is equal to state unless explicitely requested.</p> <code>None</code> <code>gate</code> <code>Array</code> <p>A multiplicative gate applied to all local updates.</p> <code>None</code> <p>Returns:</p> Type Description <code>Self</code> <p>A PyTree with the same structure as <code>self</code> where the <code>lmap</code> subtree mirrors the original layermap but each module is replaced by its parameter update (zeros for non-trainable fields).</p> Notes <ul> <li>The returned object is intended for use with Optax:   <code>updates = orchestrator.backward(state, rng)</code>,   then <code>new_params = optax.apply_updates(params, updates)</code>.</li> <li>Implementations should avoid side effects and return only updates,   not updated parameters.</li> </ul>"},{"location":"reference/api/darnax/orchestrators/sequential/","title":"darnax.orchestrators.sequential","text":"<p>Sequential orchestrator for layered networks.</p> <p>This module provides :class:<code>SequentialOrchestrator</code>, a concrete implementation of the orchestrator contract that traverses a :class:<code>~darnax.layer_maps.sparse.LayerMap</code> row-by-row (deterministic order), computes messages along edges, reduces them at each receiver, and updates a :class:<code>~darnax.states.sequential.SequentialState</code>.</p> Overview <ul> <li>:meth:<code>step</code> updates all receivers except the output row (keeps <code>state[-1]</code> unchanged).</li> <li>:meth:<code>step_inference</code> is like :meth:<code>step</code> but skips right-going edges   (uses a forward-only schedule; cheaper partial refresh).</li> <li>:meth:<code>predict</code> computes the output buffer from current internal buffers.</li> <li>:meth:<code>backward</code> builds a LayerMap-shaped PyTree of parameter updates by   calling each module\u2019s local rule. The method returns a new orchestrator whose   <code>lmap</code> contains updates (not new parameters), so it can be fed directly   to Optax\u2019s <code>apply_updates</code>.</li> </ul> RNG handling <p>All public methods accept an RNG key and return a new key. Internally, keys are split per receiver and per sender to keep randomness reproducible and side-effect free.</p> See Also <p>tutorials/05_orchestrators.md tutorials/06_simple_net_on_artificial_data.md</p>"},{"location":"reference/api/darnax/orchestrators/sequential/#darnax.orchestrators.sequential.KeyArray","title":"<code>KeyArray = Array</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/orchestrators/sequential/#darnax.orchestrators.sequential.SequentialOrchestrator","title":"<code>SequentialOrchestrator(layers)</code>","text":"<p>               Bases: <code>AbstractOrchestrator[SequentialState]</code></p> <p>Sequential message-passing orchestrator.</p> Assumptions <ul> <li><code>lmap</code> has static structure (sorted integer keys); values are Equinox   modules that are PyTrees (parameters visible to JAX/Optax).</li> <li>For each receiver <code>i</code>, the diagonal module <code>lmap[i, i]</code> implements   <code>reduce(pytree_of_messages)</code> and <code>activation(Array) -&gt; Array</code>.</li> <li>Each edge module <code>lmap[i, j]</code> is callable as <code>module(x, rng=...) -&gt; Array</code>   and exposes <code>backward(x, y, y_hat) -&gt; AbstractModule</code> (same PyTree type).</li> </ul> Notes <p>This orchestrator is pure: given a state and RNG, it returns a new state and a new RNG key. All scheduling choices (row order, forward-only mode) are explicit and deterministic.</p> <p>Initialize the orchestrator from a layermap.</p> <p>Parameters:</p> Name Type Description Default <code>layers</code> <code>LayerMap</code> <p>Static adjacency (rows/cols) with Equinox modules as values.</p> required"},{"location":"reference/api/darnax/orchestrators/sequential/#darnax.orchestrators.sequential.SequentialOrchestrator.lmap","title":"<code>lmap = layers</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/orchestrators/sequential/#darnax.orchestrators.sequential.SequentialOrchestrator.step","title":"<code>step(state, rng, *, filter_messages='all', skip_output_state=True)</code>","text":"<p>Run one forward/update sweep for all receivers except output.</p> <p>For each receiver row <code>i</code> (excluding the last), the orchestrator: (1) computes messages from all neighbors <code>j</code> in <code>lmap[i]</code>, (2) reduces them via <code>lmap[i, i].reduce(messages)</code>, (3) applies <code>lmap[i, i].activation(...)</code>, (4) writes the result into <code>state[i]</code>.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>SequentialState</code> <p>Current global state.</p> required <code>rng</code> <code>KeyArray</code> <p>PRNG key (split per receiver/sender).</p> required <code>filter_messages</code> <code>Literal[\"all\", \"forward\", \"backward\"]. Default: \"all\"</code> <p>Only a subset of the messages are sent during the step. If forward, only forward messages (lower-triangle and diagonal) are computed. Same for backward. \"All\" computes all the messages.</p> <code>'all'</code> <code>skip_output_state</code> <code>bool. Default: true.</code> <p>If true, we only update internal states (we exclude output state (-1)). The idea is that somehow the output is clamped and in some learning phases updating it is useless. By setting skip_readout=True we avoid the computation.</p> <code>True</code> <p>Returns:</p> Type Description <code>(new_state, new_rng) : tuple[SequentialState, KeyArray]</code> <p>Updated state (output unchanged) and an advanced RNG key.</p>"},{"location":"reference/api/darnax/orchestrators/sequential/#darnax.orchestrators.sequential.SequentialOrchestrator.step_inference","title":"<code>step_inference(state, rng)</code>","text":"<p>Run a forward-only sweep (skip output and right-going edges).</p> <p>Like :meth:<code>step</code>, but calls <code>row_items(skip_last=True, forward_only=True)</code>, which filters neighbor edges to a forward schedule (commonly <code>j &lt;= i</code>).</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>SequentialState</code> <p>Current global state.</p> required <code>rng</code> <code>KeyArray</code> <p>PRNG key (split per receiver/sender).</p> required <p>Returns:</p> Type Description <code>(new_state, new_rng) : tuple[SequentialState, KeyArray]</code> <p>Updated state (output unchanged) and an advanced RNG key.</p>"},{"location":"reference/api/darnax/orchestrators/sequential/#darnax.orchestrators.sequential.SequentialOrchestrator.predict","title":"<code>predict(state, rng)</code>","text":"<p>Compute/refresh the output buffer <code>state[-1]</code> from current buffers.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>SequentialState</code> <p>Current global state (uses existing upstream buffers).</p> required <code>rng</code> <code>KeyArray</code> <p>PRNG key (split internally).</p> required <p>Returns:</p> Type Description <code>(new_state, new_rng) : tuple[SequentialState, KeyArray]</code> <p>A state where the output buffer has been updated, and a fresh RNG key.</p>"},{"location":"reference/api/darnax/orchestrators/sequential/#darnax.orchestrators.sequential.SequentialOrchestrator.backward","title":"<code>backward(state, rng, *, filter_messages='forward', target_state=None, gate=None)</code>","text":"<p>Compute per-edge parameter updates and return them as an orchestrator.</p> <p>Two-phase algorithm:</p> <p>1) Activation pass. For each receiver <code>i</code> (including the output),    compute messages <code>msg[j] = lmap[i, j](state[j], rng=...)</code> from    all neighbors, then compute the receiver\u2019s aggregate using forward    messages only (e.g., <code>j &lt;= i</code>) and store it under <code>msg[i]</code>. 2) Local rules. For every edge <code>(i, j)</code>, call    <code>lmap[i, j].backward(x=state[j], y=state[i], y_hat=msg[j])</code> and    assemble the results into a new LayerMap with the same key layout.</p> <p>The method returns <code>type(self)(layers=updates)</code> where <code>updates</code> is the LayerMap of per-edge updates. This allows <code>optax.apply_updates</code> to be used directly with the returned PyTree.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>SequentialState</code> <p>Current global state (provides inputs and targets for local rules).</p> required <code>rng</code> <code>KeyArray</code> <p>PRNG key (split per receiver/sender).</p> required <code>filter_messages</code> <code>Literal['all', 'forward', 'backward']</code> <p>Optionally, it is possible to decide to compute the local fields (the messages) based on a different subset of the messages. Forward corresponds to the default behaviour, where only forward messages contribute to the messages.</p> <code>'forward'</code> <code>target_state</code> <code>SequentialState | None</code> <p>Optionally, it is possible to mix states in the backward update rule. By default, other_state is equal to state unless explicitely requested.</p> <code>None</code> <code>gate</code> <code>Array</code> <p>A multiplicative gate applied to all local updates. Shape must be broadcastable to x shapes.</p> <code>None</code> <p>Returns:</p> Type Description <code>Self</code> <p>An orchestrator whose <code>lmap</code> contains updates (not new params), mirroring the original layermap structure.</p> Notes <p>The receiver aggregate used for learning is computed with forward-only messages (no right-going edges), matching the schedule used in inference.</p>"},{"location":"reference/api/darnax/states/interface/","title":"darnax.states.interface","text":"<p>State interfaces.</p> <p>This module defines the abstract :class:<code>State</code> contract used by orchestrators and layer maps. A concrete state holds per-layer tensors plus two special slots:</p> <ul> <li>index <code>0</code>: input (provided by the caller),</li> <li>last index: output (produced by the network).</li> </ul> <p>Implementations must be Equinox <code>Module</code>s (i.e., PyTrees) and support functional updates (returning new state instances rather than mutating in place).</p>"},{"location":"reference/api/darnax/states/interface/#darnax.states.interface.State","title":"<code>State</code>","text":"<p>               Bases: <code>Module</code></p> <p>Global state for a network (LayerMap).</p> <p>A <code>State</code> stores the tensors associated with each layer plus the designated input and output slots. The total number of elements must be at least <code>#layers + 1</code> (input + one per layer + output).</p> <p>Concretely, orchestrators assume: - <code>state[0]</code> holds the input, - <code>state[-1]</code> holds the output, - the remaining indices map to intermediate layer states according to the   LayerMap\u2019s ordering.</p> Notes <ul> <li><code>State</code> is an Equinox <code>Module</code> and thus a PyTree; it should be safe   to pass through JAX transformations (<code>jit</code>, <code>vmap</code>) as long as   implementations avoid in-place mutation.</li> <li>All mutating operations must be exposed as functional methods that   return a new instance (see :meth:<code>replace</code> and :meth:<code>replace_val</code>).</li> </ul> See Also <p>darnax.states.sequential.SequentialState     A concrete reference implementation using array-backed storage.</p>"},{"location":"reference/api/darnax/states/interface/#darnax.states.interface.State.init","title":"<code>init(x, y=None)</code>  <code>abstractmethod</code>","text":"<p>Initialize input/output slots and return a fresh state.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input tensor to place at index <code>0</code>.</p> required <code>y</code> <code>Array or None</code> <p>Optional target/output tensor to place at the last index. If <code>None</code>, the output slot should be initialized according to the implementation\u2019s policy (e.g., zeros or a placeholder).</p> <code>None</code> <p>Returns:</p> Type Description <code>Self</code> <p>A new state instance with input/output slots set.</p> Notes <p>This method must not mutate the receiver; it returns a new state object with the appropriate slots populated.</p>"},{"location":"reference/api/darnax/states/interface/#darnax.states.interface.State.readout","title":"<code>readout()</code>","text":"<p>Return the readout state.</p>"},{"location":"reference/api/darnax/states/interface/#darnax.states.interface.State.replace","title":"<code>replace(value)</code>  <code>abstractmethod</code>","text":"<p>Return a new state with all underlying values replaced.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>PyTree</code> <p>A PyTree matching the internal storage structure.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>A new instance containing <code>value</code> as its storage.</p> Notes <p>This is the coarse-grained replacement primitive used by orchestrators when recomputing the entire state in one shot.</p>"},{"location":"reference/api/darnax/states/interface/#darnax.states.interface.State.replace_val","title":"<code>replace_val(idx, value)</code>  <code>abstractmethod</code>","text":"<p>Return a new state with a single slot replaced.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>Any</code> <p>Index/key indicating which slot to replace (commonly an <code>int</code>).</p> required <code>value</code> <code>Array</code> <p>New tensor to store at the selected position.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>A new instance where only the selected slot differs from <code>self</code>.</p> Notes <p>This is the fine-grained primitive used during step-wise updates (e.g., updating a single layer\u2019s activation while keeping the rest).</p>"},{"location":"reference/api/darnax/states/sequential/","title":"darnax.states.sequential","text":"<p>Sequential, array-backed global state.</p> <p><code>SequentialState</code> stores per-layer activations for layered networks as a list of JAX arrays and follows a strict indexing convention:</p> <ul> <li>index <code>0</code>: input buffer</li> <li>index <code>-1</code>: output buffer</li> <li>indices <code>1..-2</code>: intermediate layer buffers (left \u2192 right)</li> </ul> <p>Construction allocates placeholder buffers with batch size 1. Call :meth:<code>SequentialState.init</code> to resize all buffers to the true batch size and populate the endpoints. The object is an Equinox <code>Module</code> (a PyTree), so it plays nicely with JAX transforms; all \u201cupdates\u201d are functional (return a new instance) via :meth:<code>replace</code> and :meth:<code>replace_val</code>.</p>"},{"location":"reference/api/darnax/states/sequential/#darnax.states.sequential.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/states/sequential/#darnax.states.sequential.SequentialState","title":"<code>SequentialState(sizes, dtype=jnp.float32)</code>","text":"<p>               Bases: <code>State</code></p> <p>Sequential activations buffer for layered networks.</p> <p>Layers are indexed from left to right: <code>state[0]</code> is input and <code>state[-1]</code> is output. The internal storage is a list of arrays with shapes <code>(B, *size_l)</code> where <code>B</code> is the batch size and <code>*size_l</code> is the per-layer trailing shape.</p> <p>On construction, buffers are initialized as zeros with shape <code>(1, *size_l)</code> (a placeholder batch). Call :meth:<code>init</code> to resize all buffers to a real batch size and optionally set the output.</p> <p>Attributes:</p> Name Type Description <code>states</code> <code>list[Array]</code> <p>Per-layer buffers. Shapes are <code>(B, *size_l)</code> after :meth:<code>init</code>, or <code>(1, *size_l)</code> immediately after construction.</p> <code>dtype</code> <code>dtype</code> <p>Dtype for all buffers (marked static in the PyTree).</p> <code>data_min_ndim</code> <code>int</code> <p>Minimal rank for data buffers (default <code>2</code> \u2192 enforces a batch dim).</p> Notes <ul> <li>This class is functional: methods like :meth:<code>replace</code> and   :meth:<code>replace_val</code> return a new instance; the original is unchanged.</li> <li>Shape assertions in :meth:<code>init</code> are executed at trace time, so they are   checked once per JIT compilation.</li> <li>Nothing prevents you from storing non-floating dtypes, as long as they   are consistent with <code>dtype</code> at construction.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; st = SequentialState([4, 8, 2])        # input(4), hidden(8), output(2)\n&gt;&gt;&gt; st = st.init(jnp.zeros((32, 4)))       # B=32; output left as zeros\n&gt;&gt;&gt; x = st[0]                               # (32, 4)\n&gt;&gt;&gt; st2 = st.replace_val(1, jnp.ones((32, 8)))  # update hidden layer\n</code></pre> <p>Create a sequential state with one buffer per layer.</p> <p>Parameters:</p> Name Type Description Default <code>sizes</code> <code>Iterable[tuple[int, ...] | int]</code> <p>Iterable of positive sizes for each layer, including input and output. Each entry can be a positive <code>int</code> (for 1D layers) or a tuple of positive <code>int</code> (for multi-axis layers). Examples: <code>[D, N, C]</code> or <code>[(H, W, C_in), N, (C_out,)]</code>.</p> required <code>dtype</code> <code>dtype</code> <p>Dtype for all buffers. Default is <code>jnp.float32</code>.</p> <code>float32</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>sizes</code> is empty, or if any entry is not a positive integer or a tuple of positive integers.</p>"},{"location":"reference/api/darnax/states/sequential/#darnax.states.sequential.SequentialState.data_min_ndim","title":"<code>data_min_ndim = eqx.field(default=2, static=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/states/sequential/#darnax.states.sequential.SequentialState.dtype","title":"<code>dtype = dtype</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/states/sequential/#darnax.states.sequential.SequentialState.states","title":"<code>states = [(jnp.zeros((1, *size), dtype=dtype)) for size in shape_tuples]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/states/sequential/#darnax.states.sequential.SequentialState.readout","title":"<code>readout</code>  <code>property</code>","text":"<p>Return the readout state.</p>"},{"location":"reference/api/darnax/states/sequential/#darnax.states.sequential.SequentialState.replace","title":"<code>replace(value)</code>","text":"<p>Return a new instance with <code>states</code> replaced by <code>value</code>.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Sequence[Array] | PyTree</code> <p>A sequence (or PyTree) that will replace the internal list of buffers. In typical use, a list of arrays of length <code>len(self)</code>, each shaped <code>(B, *size_l)</code>.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>A new <code>SequentialState</code> carrying <code>value</code> as its storage.</p> Notes <p>This is a functional update; the original object is not mutated.</p>"},{"location":"reference/api/darnax/states/sequential/#darnax.states.sequential.SequentialState.replace_val","title":"<code>replace_val(idx, value)</code>","text":"<p>Return a new instance with layer <code>idx</code> replaced by <code>value</code>.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Layer index to modify.</p> required <code>value</code> <code>Array</code> <p>New buffer for that layer, typically with shape <code>(B, *size_idx)</code>.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>A new instance where only the selected layer differs from <code>self</code>.</p>"},{"location":"reference/api/darnax/states/sequential/#darnax.states.sequential.SequentialState.init","title":"<code>init(x, y=None)</code>","text":"<p>Resize buffers to batch <code>B</code> and set input (and optionally output).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input batch with shape <code>(B, *D)</code>; the trailing shape <code>*D</code> must equal the configured input layer shape.</p> required <code>y</code> <code>Array or None</code> <p>Optional output batch with shape <code>(B, *C)</code>; the trailing shape <code>*C</code> must equal the configured output layer shape. If omitted, the output buffer is zero-initialized.</p> <code>None</code> <p>Returns:</p> Type Description <code>Self</code> <p>A new instance whose buffers all have shape <code>(B, *size_l)</code>, with layer <code>0</code> set to <code>x</code> and (if provided) layer <code>-1</code> set to <code>y</code>.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If shapes are inconsistent (checked at trace time under JIT).</p> Notes <p>This method does not mutate the receiver; it constructs new buffers with batch size <code>B</code> and returns a fresh state.</p>"},{"location":"reference/api/darnax/trainers/alternate/","title":"darnax.trainers.alternate","text":""},{"location":"reference/api/darnax/trainers/alternate/#darnax.trainers.alternate.StateT","title":"<code>StateT = TypeVar('StateT', bound=State)</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/trainers/alternate/#darnax.trainers.alternate.OrchestratorT","title":"<code>OrchestratorT = TypeVar('OrchestratorT', bound=(AbstractOrchestrator[Any]))</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/trainers/alternate/#darnax.trainers.alternate.Ctx","title":"<code>Ctx = dict[str, Any]</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/trainers/alternate/#darnax.trainers.alternate.AltTrainer","title":"<code>AltTrainer(orchestrator, state, optimizer, optimizer_state, train_clamped_n_iter=7, train_free_n_iter=7, eval_n_iter=14)</code>","text":"<p>               Bases: <code>Trainer[OrchestratorT, StateT]</code>, <code>Generic[OrchestratorT, StateT]</code></p> <p>Minimal trainer that implements an alternated dynamical rule.</p> <p>This trainer wires together an orchestrator and a per-batch <code>State</code>, performing free/clamped dynamics. Only free for evaluation. It assumes a local learning rule implemented by the orchestrator and an Optax optimizer for parameter updates.</p> <p>More specifically: - Warmup: Initial dynamical phase (1 or 2 iterations) with only left fields - Clamped: Dynamical phase with both left and right fields - Free: Final dynamical phase where only the left fields are used</p> <p>During evaluation only free is performed.</p> <p>Parameters:</p> Name Type Description Default <code>orchestrator</code> <code>AbstractOrchestrator[StateT]</code> <p>The model/orchestrator. Treated as read-only during eval by convention.</p> required <code>state</code> <code>StateT</code> <p>Mutable per-batch state pytree (buffers, EMA, etc.).</p> required <code>optimizer</code> <code>GradientTransformation</code> <p>Optax transform (e.g., <code>optax.adam(...)</code>).</p> required <code>optimizer_state</code> <code>PyTree</code> <p>Optax optimizer state (from <code>optimizer.init(params)</code>).</p> required <code>warmup_n_iter</code> <code>int</code> <p>Number of warmup iterations per batch.</p> <code>1</code> <code>train_clamped_n_iter</code> <code>int</code> <p>Number of clamped iterations per batch (train only).</p> <code>7</code> <code>train_free_n_iter</code> <code>int</code> <p>Number of free iterations per batch (train only).</p> <code>7</code> <code>eval_n_iter</code> <code>int</code> <p>Number of free iterations per batch (eval).</p> <code>14</code> <p>Attributes:</p> Name Type Description <code>orchestrator</code> <code>OrchestratorT</code> <p>The current orchestrator/model.</p> <code>state</code> <code>StateT</code> <p>The current mutable state.</p> <code>ctx</code> <code>dict[str, Any]</code> <p>A context dictionary with optimizer, optimizer state, and iteration counts. Keys are: - \"optimizer\" : GradientTransformation - \"optimizer_state\" : PyTree - \"clamped_iter\" : int - \"free_iter\" : int - \"eval_iter\" : int</p> <p>Initialize dynamical trainer.</p>"},{"location":"reference/api/darnax/trainers/alternate/#darnax.trainers.alternate.AltTrainer.orchestrator","title":"<code>orchestrator = orchestrator</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/trainers/alternate/#darnax.trainers.alternate.AltTrainer.state","title":"<code>state = state</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/trainers/alternate/#darnax.trainers.alternate.AltTrainer.ctx","title":"<code>ctx = self.validate_ctx(self.ctx)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/trainers/alternate/#darnax.trainers.alternate.AltTrainer.validate_ctx","title":"<code>validate_ctx(ctx)</code>  <code>staticmethod</code>","text":"<p>Validate and normalize the context dictionary.</p> <p>Ensures required keys are present and returns a normalized copy.</p> <p>Parameters:</p> Name Type Description Default <code>ctx</code> <code>dict[str, Any]</code> <p>Context dictionary supplied at construction.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Normalized context dictionary.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any required key is missing.</p>"},{"location":"reference/api/darnax/trainers/dynamical/","title":"darnax.trainers.dynamical","text":""},{"location":"reference/api/darnax/trainers/dynamical/#darnax.trainers.dynamical.StateT","title":"<code>StateT = TypeVar('StateT', bound=State)</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/trainers/dynamical/#darnax.trainers.dynamical.OrchestratorT","title":"<code>OrchestratorT = TypeVar('OrchestratorT', bound=(AbstractOrchestrator[Any]))</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/trainers/dynamical/#darnax.trainers.dynamical.Ctx","title":"<code>Ctx = dict[str, Any]</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/trainers/dynamical/#darnax.trainers.dynamical.DynamicalTrainer","title":"<code>DynamicalTrainer(orchestrator, state, optimizer, optimizer_state, warmup_n_iter=1, train_clamped_n_iter=7, train_free_n_iter=7, eval_n_iter=14)</code>","text":"<p>               Bases: <code>Trainer[OrchestratorT, StateT]</code>, <code>Generic[OrchestratorT, StateT]</code></p> <p>Minimal trainer that implements the learning rule described in the main paper.</p> <p>This trainer wires together an orchestrator and a per-batch <code>State</code>, performing warmup/clamped/free dynamics for training. Warmup/free for evaluation. It assumes a local learning rule implemented by the orchestrator and an Optax optimizer for parameter updates.</p> <p>More specifically: - Warmup: Initial dynamical phase (1 or 2 iterations) with only left fields - Clamped: Dynamical phase with both left and right fields - Free: Final dynamical phase where only the left fields are used</p> <p>During evaluation only warmup+free is performed.</p> <p>Parameters:</p> Name Type Description Default <code>orchestrator</code> <code>AbstractOrchestrator[StateT]</code> <p>The model/orchestrator. Treated as read-only during eval by convention.</p> required <code>state</code> <code>StateT</code> <p>Mutable per-batch state pytree (buffers, EMA, etc.).</p> required <code>optimizer</code> <code>GradientTransformation</code> <p>Optax transform (e.g., <code>optax.adam(...)</code>).</p> required <code>optimizer_state</code> <code>PyTree</code> <p>Optax optimizer state (from <code>optimizer.init(params)</code>).</p> required <code>warmup_n_iter</code> <code>int</code> <p>Number of warmup iterations per batch.</p> <code>1</code> <code>train_clamped_n_iter</code> <code>int</code> <p>Number of clamped iterations per batch (train only).</p> <code>7</code> <code>train_free_n_iter</code> <code>int</code> <p>Number of free iterations per batch (train only).</p> <code>7</code> <code>eval_n_iter</code> <code>int</code> <p>Number of free iterations per batch (eval).</p> <code>14</code> <p>Attributes:</p> Name Type Description <code>orchestrator</code> <code>OrchestratorT</code> <p>The current orchestrator/model.</p> <code>state</code> <code>StateT</code> <p>The current mutable state.</p> <code>ctx</code> <code>dict[str, Any]</code> <p>A context dictionary with optimizer, optimizer state, and iteration counts. Keys are: - \"optimizer\" : GradientTransformation - \"optimizer_state\" : PyTree - \"warmup_iter\" : int - \"clamped_iter\" : int - \"free_iter\" : int - \"eval_iter\" : int</p> <p>Initialize dynamical trainer.</p>"},{"location":"reference/api/darnax/trainers/dynamical/#darnax.trainers.dynamical.DynamicalTrainer.orchestrator","title":"<code>orchestrator = orchestrator</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/trainers/dynamical/#darnax.trainers.dynamical.DynamicalTrainer.state","title":"<code>state = state</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/trainers/dynamical/#darnax.trainers.dynamical.DynamicalTrainer.ctx","title":"<code>ctx = self.validate_ctx(self.ctx)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/trainers/dynamical/#darnax.trainers.dynamical.DynamicalTrainer.validate_ctx","title":"<code>validate_ctx(ctx)</code>  <code>staticmethod</code>","text":"<p>Validate and normalize the context dictionary.</p> <p>Ensures required keys are present and returns a normalized copy.</p> <p>Parameters:</p> Name Type Description Default <code>ctx</code> <code>dict[str, Any]</code> <p>Context dictionary supplied at construction.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Normalized context dictionary.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any required key is missing.</p>"},{"location":"reference/api/darnax/trainers/hebbian_contrastive/","title":"darnax.trainers.hebbian_contrastive","text":""},{"location":"reference/api/darnax/trainers/hebbian_contrastive/#darnax.trainers.hebbian_contrastive.StateT","title":"<code>StateT = TypeVar('StateT', bound=State)</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/trainers/hebbian_contrastive/#darnax.trainers.hebbian_contrastive.OrchestratorT","title":"<code>OrchestratorT = TypeVar('OrchestratorT', bound=(AbstractOrchestrator[Any]))</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/trainers/hebbian_contrastive/#darnax.trainers.hebbian_contrastive.Ctx","title":"<code>Ctx = dict[str, Any]</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/trainers/hebbian_contrastive/#darnax.trainers.hebbian_contrastive.ContrastiveHebbianTrainer","title":"<code>ContrastiveHebbianTrainer(orchestrator, state, optimizer, optimizer_state, train_clamped_n_iter=7, train_free_n_iter=7, eval_n_iter=14)</code>","text":"<p>               Bases: <code>Trainer[OrchestratorT, StateT]</code>, <code>Generic[OrchestratorT, StateT]</code></p> <p>Minimal trainer that implements a contrastive hebbian learning rule.</p> <p>This trainer wires together an orchestrator and a per-batch <code>State</code>, performing free/clamped dynamics. Only free for evaluation. It assumes a local learning rule implemented by the orchestrator and an Optax optimizer for parameter updates.</p> <p>More specifically: - Clamped: Dynamical phase with both left and right fields - Free: Final dynamical phase where only the left fields are used</p> <p>During evaluation only free is performed.</p> <p>Parameters:</p> Name Type Description Default <code>orchestrator</code> <code>AbstractOrchestrator[StateT]</code> <p>The model/orchestrator. Treated as read-only during eval by convention.</p> required <code>state</code> <code>StateT</code> <p>Mutable per-batch state pytree (buffers, EMA, etc.).</p> required <code>optimizer</code> <code>GradientTransformation</code> <p>Optax transform (e.g., <code>optax.adam(...)</code>).</p> required <code>optimizer_state</code> <code>PyTree</code> <p>Optax optimizer state (from <code>optimizer.init(params)</code>).</p> required <code>train_clamped_n_iter</code> <code>int</code> <p>Number of clamped iterations per batch (train only).</p> <code>7</code> <code>train_free_n_iter</code> <code>int</code> <p>Number of free iterations per batch (train only).</p> <code>7</code> <code>eval_n_iter</code> <code>int</code> <p>Number of free iterations per batch (eval).</p> <code>14</code> <p>Attributes:</p> Name Type Description <code>orchestrator</code> <code>OrchestratorT</code> <p>The current orchestrator/model.</p> <code>state</code> <code>StateT</code> <p>The current mutable state.</p> <code>ctx</code> <code>dict[str, Any]</code> <p>A context dictionary with optimizer, optimizer state, and iteration counts. Keys are: - \"optimizer\" : GradientTransformation - \"optimizer_state\" : PyTree - \"clamped_iter\" : int - \"free_iter\" : int - \"eval_iter\" : int</p> <p>Initialize dynamical trainer.</p>"},{"location":"reference/api/darnax/trainers/hebbian_contrastive/#darnax.trainers.hebbian_contrastive.ContrastiveHebbianTrainer.orchestrator","title":"<code>orchestrator = orchestrator</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/trainers/hebbian_contrastive/#darnax.trainers.hebbian_contrastive.ContrastiveHebbianTrainer.state","title":"<code>state = state</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/trainers/hebbian_contrastive/#darnax.trainers.hebbian_contrastive.ContrastiveHebbianTrainer.ctx","title":"<code>ctx = self.validate_ctx(self.ctx)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/trainers/hebbian_contrastive/#darnax.trainers.hebbian_contrastive.ContrastiveHebbianTrainer.validate_ctx","title":"<code>validate_ctx(ctx)</code>  <code>staticmethod</code>","text":"<p>Validate and normalize the context dictionary.</p> <p>Ensures required keys are present and returns a normalized copy.</p> <p>Parameters:</p> Name Type Description Default <code>ctx</code> <code>dict[str, Any]</code> <p>Context dictionary supplied at construction.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Normalized context dictionary.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any required key is missing.</p>"},{"location":"reference/api/darnax/trainers/interface/","title":"darnax.trainers.interface","text":""},{"location":"reference/api/darnax/trainers/interface/#darnax.trainers.interface.StateT","title":"<code>StateT = TypeVar('StateT', bound=State)</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/trainers/interface/#darnax.trainers.interface.OrchestratorT","title":"<code>OrchestratorT = TypeVar('OrchestratorT', bound=(AbstractOrchestrator[Any]))</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/trainers/interface/#darnax.trainers.interface.Ctx","title":"<code>Ctx = dict[str, Any]</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/trainers/interface/#darnax.trainers.interface.TrainCore","title":"<code>TrainCore = Callable[[Array, Array, Array, OrchestratorT, StateT, dict[str, Any]], tuple[Array, OrchestratorT, StateT, Ctx]]</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/trainers/interface/#darnax.trainers.interface.EvalCore","title":"<code>EvalCore = Callable[[Array, Array, Array, OrchestratorT, StateT, dict[str, Any]], tuple[Array, StateT, Mapping[str, Array]]]</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/trainers/interface/#darnax.trainers.interface.Trainer","title":"<code>Trainer()</code>","text":"<p>               Bases: <code>ABC</code>, <code>Generic[OrchestratorT, StateT]</code></p> <p>Two-step trainer with a single context dictionary.</p> <p>This class exposes a minimal, stateful API (<code>train_step</code>, <code>eval_step</code>) wrapping pure, JIT-compiled static methods (<code>_train_step_impl</code>, <code>_eval_step_impl</code>). The class mutates its attributes by rebinding after each pure call.</p> <p>Parameters:</p> Name Type Description Default <code>orchestrator</code> <code>AbstractOrchestrator</code> <p>The model/orchestrator object. Treated as read-only inside eval by convention.</p> required <code>state</code> <code>State</code> <p>A mutable state pytree used by the step functions (e.g., buffers, EMA, counters).</p> required <code>ctx</code> <code>dict[str, Any]</code> <p>A single \"bag\" of step requirements: - Required:     - <code>\"optimizer\"</code> : object with update semantics (e.g., optax transform)     - <code>\"opt_state\"</code> : optimizer state pytree - Optional/common (conventions):     - <code>\"t\"</code> : global step counter (int or JAX scalar). If provided as int, it is       normalized to a JAX scalar once in :meth:<code>validate_ctx</code>.     - Any other immutable/static knobs (callables, Python scalars) that do not change       identity across steps.     - Any mutable arrays/pytree values needed by the steps (e.g., schedulers' internal       state) should be JAX arrays so they can change without recompilation.</p> required Notes <ul> <li><code>ctx</code> must keep a stable set of keys to avoid retraces.</li> <li>Non-array objects in <code>ctx</code> are treated as static under <code>filter_jit</code>; changing   their identity causes recompilation.</li> <li>Arrays (and pytrees of arrays) in <code>ctx</code> may change freely across steps without   triggering recompilation.</li> </ul> <p>Attributes:</p> Name Type Description <code>orchestrator</code> <code>AbstractOrchestrator</code> <code>state</code> <code>State</code> <code>ctx</code> <code>dict[str, Any]</code> <p>Initialize the jittable cores as None.</p>"},{"location":"reference/api/darnax/trainers/interface/#darnax.trainers.interface.Trainer.orchestrator","title":"<code>orchestrator</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/trainers/interface/#darnax.trainers.interface.Trainer.state","title":"<code>state</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/trainers/interface/#darnax.trainers.interface.Trainer.ctx","title":"<code>ctx</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/trainers/interface/#darnax.trainers.interface.Trainer.train_step","title":"<code>train_step(x, y, rng)</code>","text":"<p>Run one training step.</p> <p>Calls the pure, JIT-compiled :meth:<code>_train_step_impl</code>, then rebinds <code>orchestrator</code>, <code>state</code>, and <code>ctx</code> with the returned values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input batch.</p> required <code>y</code> <code>Array</code> <p>Target labels.</p> required <code>rng</code> <code>Array</code> <p>Random key.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Updated random key.</p> Notes <p>This method may update: - <code>orchestrator</code> (model parameters), - <code>state</code> (mutable buffers, EMA, etc.), - <code>ctx</code> (e.g., <code>\"opt_state\"</code>, schedulers, counters).</p>"},{"location":"reference/api/darnax/trainers/interface/#darnax.trainers.interface.Trainer.eval_step","title":"<code>eval_step(x, y, rng)</code>","text":"<p>Evaluate on one batch.</p> <p>Calls the pure, JIT-compiled :meth:<code>_eval_step_impl</code>, then rebinds <code>state</code>. Returns a mapping of metrics that are not stored in :attr:<code>ctx</code>.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input batch.</p> required <code>y</code> <code>Array</code> <p>Target labels.</p> required <code>rng</code> <code>Array</code> <p>Random key.</p> required <p>Returns:</p> Name Type Description <code>rng</code> <code>Array</code> <p>Updated random key.</p> <code>metrics</code> <code>Mapping[str, Array]</code> <p>Dictionary of metric arrays computed on this batch.</p> Notes <p>By convention, evaluation treats the orchestrator and static pieces as read-only. Only <code>state</code> may be updated (e.g., running statistics).</p>"},{"location":"reference/api/darnax/trainers/interface/#darnax.trainers.interface.Trainer.validate_ctx","title":"<code>validate_ctx(ctx)</code>  <code>staticmethod</code>","text":"<p>Validate and normalize the context dictionary.</p> <p>Called once in <code>__init__</code> to fail fast and to normalize counters.</p> <p>Parameters:</p> Name Type Description Default <code>ctx</code> <code>dict[str, Any]</code> <p>User-provided context dictionary.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A normalized copy of the context dictionary.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required keys are missing.</p> Notes <ul> <li>Ensures that <code>\"optimizer\"</code> and <code>\"opt_state\"</code> are present.</li> <li>Converts <code>ctx[\"t\"]</code> to a JAX scalar if it is a Python integer.</li> <li>Returns a plain <code>dict</code> to keep structure explicit and stable.</li> </ul>"},{"location":"reference/api/darnax/trainers/utils/","title":"darnax.trainers.utils","text":""},{"location":"reference/api/darnax/trainers/utils/#darnax.trainers.utils.scan_n","title":"<code>scan_n(f, init, n_iter, *f_args, **f_kwargs)</code>","text":"<p>Run a step function for <code>n_iter</code> iterations using <code>jax.lax.scan</code>.</p> <p>The step function is called as <code>f(carry, *f_args, **f_kwargs)</code> each iteration. Its per-step outputs (the second return value) are stacked along a new leading time dimension by <code>lax.scan</code> and returned.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>callable</code> <p>Step function with signature <code>f(carry, *f_args, **f_kwargs) -&gt; (new_carry, out)</code>. <code>carry</code> and <code>out</code> can be any pytrees of JAX arrays (e.g., tuples like <code>(rng, state)</code>, dictionaries, dataclasses).</p> required <code>init</code> <code>pytree</code> <p>Initial carry (pytree of arrays).</p> required <code>n_iter</code> <code>int</code> <p>Number of iterations to run.</p> required <code>*f_args</code> <code>Any</code> <p>Positional arguments forwarded to <code>f</code> on every step.</p> <code>()</code> <code>**f_kwargs</code> <code>Any</code> <p>Keyword arguments forwarded to <code>f</code> on every step.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>carry</code> <code>pytree</code> <p>Final carry after <code>n_iter</code> steps (e.g., final <code>(rng, state)</code>).</p> <code>outputs</code> <code>pytree or None</code> <p>Time-stacked outputs from each step (leading dimension = <code>n_iter</code>). If <code>f</code> returns <code>None</code> as the output, this will be <code>None</code>.</p> Notes <ul> <li>Implemented as <code>lax.scan(body, init, xs=None, length=n_iter)</code>.</li> <li>Non-array values in <code>f_args</code>/<code>f_kwargs</code> are treated as static;   changing their identity will retrace/JIT-recompile. Values that vary   should be JAX arrays.</li> <li><code>carry</code> may be any pytree, including tuples like <code>(rng, state)</code>.</li> </ul> <p>Examples:</p> <p>Carry as <code>(rng, state)</code> where both are pytrees:</p> <pre><code>&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt; from jax import random\n&gt;&gt;&gt; # toy \"state\" as a dict; in your code this is your State object/pytree\n&gt;&gt;&gt; def step(carry, lr):\n...     rng, state = carry\n...     rng, sub = random.split(rng)\n...     # pretend \"state['w']\" is a parameter; do a dummy update\n...     grad | None = None         # fake gradient\n...     new_w = state[\"w\"] - lr * grad\n...     new_state = {\"w\": new_w}\n...     loss = new_w ** 2             # fake loss to log\n...     return (rng, new_state), loss\n...\n&gt;&gt;&gt; init_rng = random.PRNGKey(0)\n&gt;&gt;&gt; init_state = {\"w\": jnp.array(3.0)}\n&gt;&gt;&gt; (final_rng, final_state), losses = scan_n(step, (init_rng, init_state), n_iter=5, lr=jnp.array(0.1))\n&gt;&gt;&gt; final_state[\"w\"]  # moved 5 steps of size 0.1 from 3.0\nArray(2.5, dtype=float32)\n&gt;&gt;&gt; losses.shape  # collected per-step outputs\n(5,)\n</code></pre>"},{"location":"reference/api/darnax/trainers/utils/#darnax.trainers.utils.batch_accuracy","title":"<code>batch_accuracy(y_true, y_pred)</code>","text":"<p>Compute accuracy with \u00b11 OVA labels (class = argmax along last dim).</p>"},{"location":"reference/api/darnax/utils/cont_perceptron_rule/","title":"darnax.utils.cont_perceptron_rule","text":"<p>Continouos Perceptron (OVA) backward update in JAX.</p> <p>This module implements a one-vs-all perceptron-style local update that returns a weight increment <code>\u0394W</code> to be added to a dense weight matrix.</p> <ul> <li>Supports batched inputs.</li> <li>Labels are continouos in <code>[-1, +1]</code> per class (one-vs-all coding).</li> <li><code>y_hat</code> are raw scores (pre-activation).</li> <li>No learning rate is applied here.</li> </ul> Shapes <p>x       : (d,) or (n, d) y       : (n, K) in [-1, +1] y_hat   : (n, K) raw scores margin  : broadcastable to (n, K) \u2014 e.g. scalar, (K,), or (n, K) returns : (d, K)  (\u0394W to add to weights)</p>"},{"location":"reference/api/darnax/utils/cont_perceptron_rule/#darnax.utils.cont_perceptron_rule.tanh_perceptron_rule_backward","title":"<code>tanh_perceptron_rule_backward(x, y, y_hat, tolerance)</code>","text":"<p>Multiclass (OVA) perceptron update (no learning rate).</p> <p>Applies an update when the tolerance condition is violated, i.e. <code>y - y_hat &lt;= tolerance</code> (ties count as mistakes). The result is a weight increment <code>\u0394W</code> aligned with a column-per-class layout.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input vector(s), shape <code>(d,)</code> or <code>(n, d)</code>.</p> required <code>y</code> <code>Array</code> <p>One-vs-all targets in <code>[-1, +1]</code>, shape <code>(n, K)</code>.</p> required <code>y_hat</code> <code>Array</code> <p>Pre-activations xW, shape <code>(n, K)</code>.</p> required <code>tolerance</code> <code>Array</code> <p>Tolerance. Weights are update only if error &gt; tolerance, broadcastable to <code>(n, K)</code>; e.g. a scalar, a per-class vector <code>(K,)</code>, or an array <code>(n, K)</code>.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Weight update <code>\u0394W</code> of shape <code>(d, K)</code> to add to the weights.</p> Notes <ul> <li>Batch-size normalization: the update is divided by <code>n**0.5</code> so that its   magnitude is invariant to the batch size.</li> <li>Fan-in/width normalization (<code>1/sqrt(d)</code>) is applied here.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt; x = jnp.array([[1., 0.], [0., 1.]])      # (n=2, d=2)\n&gt;&gt;&gt; y = jnp.array([[+1, -1], [-1, +1]])      # (2, K=2)\n&gt;&gt;&gt; y_hat = jnp.array([[0.2, -0.3], [0.1, -0.4]])\n&gt;&gt;&gt; margin = 0.0\n&gt;&gt;&gt; dW = perceptron_rule_backward(x, y, y_hat, margin)\n&gt;&gt;&gt; dW.shape\n(2, 2)\n</code></pre>"},{"location":"reference/api/darnax/utils/cont_perceptron_rule/#darnax.utils.cont_perceptron_rule.tanh_truncated_perceptron_rule_backward","title":"<code>tanh_truncated_perceptron_rule_backward(x, y, y_hat, margin, tolerance)</code>","text":"<p>Multiclass (OVA) perceptron update (no learning rate).</p> <p>It constitutes a variant of the rule for discrete units. Here we have continuous units, with tanh activation, that are treated as discrete if 1-|s_i| &lt; tolerance, and ignored otherwise. Applies an update when the margin condition is violated, i.e. <code>y * y_hat &lt;= margin</code> (ties count as mistakes). The result is a weight increment <code>\u0394W</code> aligned with a column-per-class layout.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input vector(s), shape <code>(d,)</code> or <code>(n, d)</code>.</p> required <code>y</code> <code>Array</code> <p>One-vs-all targets in <code>[-1, +1]</code>, shape <code>(n, K)</code>.</p> required <code>y_hat</code> <code>Array</code> <p>Raw scores (pre-activation), shape <code>(n, K)</code>.</p> required <code>margin</code> <code>Array</code> <p>Margin threshold, broadcastable to <code>(n, K)</code>; e.g. a scalar, a per-class vector <code>(K,)</code>, or an array <code>(n, K)</code>.</p> required <code>tolerance</code> <code>Array</code> <p>We update the weights only if 1-|s_i| &lt; tolerance</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Weight update <code>\u0394W</code> of shape <code>(d, K)</code> to add to the weights.</p> Notes <ul> <li>Batch-size normalization: the update is divided by <code>n**0.5</code> so that its   magnitude is invariant to the batch size.</li> <li>Fan-in/width normalization (<code>1/sqrt(d)</code>) is applied here.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt; x = jnp.array([[1., 0.], [0., 1.]])      # (n=2, d=2)\n&gt;&gt;&gt; y = jnp.array([[+1, -1], [-1, +1]])      # (2, K=2)\n&gt;&gt;&gt; y_hat = jnp.array([[0.2, -0.3], [0.1, -0.4]])\n&gt;&gt;&gt; margin = 0.0\n&gt;&gt;&gt; dW = perceptron_rule_backward(x, y, y_hat, margin, tolerance)\n&gt;&gt;&gt; dW.shape\n(2, 2)\n</code></pre>"},{"location":"reference/api/darnax/utils/default_list/","title":"darnax.utils.default_list","text":"<p>Default-filling list registered as a JAX PyTree.</p> <p><code>DefaultList</code> behaves like a Python <code>list</code> but supports logical defaults: gaps created by assigning/inserting past the current end are represented by a sentinel and materialized only on read. This keeps storage compact while preserving where defaults were implied.</p> Features <ul> <li>Works like <code>collections.abc.MutableSequence</code> (indexing, slicing, insert, del).</li> <li>Extending past the end fills with a sentinel; the actual default value is   produced only when accessed (<code>__getitem__</code>/<code>__iter__</code>/<code>to_list</code>).</li> <li>Slicing returns another <code>DefaultList</code> that preserves sentinel slots.</li> <li>Registered as a JAX PyTree (<code>tree_flatten</code>/<code>tree_unflatten</code>).</li> </ul> Notes <ul> <li>If <code>default_factory</code> is provided, every read of a default slot creates   a fresh value (no per-slot caching).</li> <li>Leaves in the PyTree include the sentinel; JAX transforms that map over   leaves should tolerate non-numeric entries when defaults are present.</li> </ul>"},{"location":"reference/api/darnax/utils/default_list/#darnax.utils.default_list.T","title":"<code>T = TypeVar('T')</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/utils/default_list/#darnax.utils.default_list.DefaultList","title":"<code>DefaultList(initial=None, *, default=None, default_factory=None)</code>","text":"<p>               Bases: <code>MutableSequence[T | None]</code>, <code>Generic[T]</code></p> <p>Default-filling mutable list, registered as a JAX PyTree.</p> <p>Assigning or inserting beyond the current length fills gaps with a sentinel; the concrete default value is produced only when read. Slicing returns another :class:<code>DefaultList</code> that preserves sentinel slots.</p> <p>Parameters:</p> Name Type Description Default <code>initial</code> <code>Iterable[T | None]</code> <p>Concrete values to seed the list. Defaults are not inserted unless indices are explicitly extended by assignment/insert.</p> <code>None</code> <code>default</code> <code>T | None</code> <p>Value returned for a default slot if <code>default_factory</code> is not set.</p> <code>None</code> <code>default_factory</code> <code>Callable[[], T | None]</code> <p>Zero-arg callable producing the value for a default slot on read. Takes precedence over <code>default</code>. Each read yields a fresh value.</p> <code>None</code> Notes <p>The public element type is <code>T | None</code> because materialized defaults may legitimately be <code>None</code>.</p> <p>Create a :class:<code>DefaultList</code>.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>default_factory</code> is not callable.</p>"},{"location":"reference/api/darnax/utils/default_list/#darnax.utils.default_list.DefaultList.tree_flatten","title":"<code>tree_flatten()</code>","text":"<p>Return children and aux data for the JAX PyTree protocol.</p> <p>Returns:</p> Type Description <code>tuple[tuple[Any, ...], tuple[Any, ...]]</code> <p>Children are the raw underlying items (sentinels preserved). Aux data contains <code>(default, default_factory)</code>.</p>"},{"location":"reference/api/darnax/utils/default_list/#darnax.utils.default_list.DefaultList.tree_unflatten","title":"<code>tree_unflatten(aux, children)</code>  <code>classmethod</code>","text":"<p>Rebuild from aux and children (JAX PyTree protocol).</p> <p>Parameters:</p> Name Type Description Default <code>aux</code> <code>tuple[Any, ...]</code> <p>The auxiliary data returned by :meth:<code>tree_flatten</code>.</p> required <code>children</code> <code>tuple[Any, ...]</code> <p>The raw items (may include sentinels).</p> required <p>Returns:</p> Type Description <code>DefaultList[T]</code> <p>A reconstructed list with identical contents and defaults.</p>"},{"location":"reference/api/darnax/utils/default_list/#darnax.utils.default_list.DefaultList.insert","title":"<code>insert(index, value)</code>","text":"<p>Insert at <code>index</code>; if beyond end, fill gap with defaults then append.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Insertion position. Negative indices are clamped like <code>list.insert</code>.</p> required <code>value</code> <code>T | None</code> <p>Value to insert.</p> required"},{"location":"reference/api/darnax/utils/default_list/#darnax.utils.default_list.DefaultList.append","title":"<code>append(value)</code>","text":"<p>Append a value.</p>"},{"location":"reference/api/darnax/utils/default_list/#darnax.utils.default_list.DefaultList.extend","title":"<code>extend(values)</code>","text":"<p>Extend from an iterable.</p>"},{"location":"reference/api/darnax/utils/default_list/#darnax.utils.default_list.DefaultList.to_list","title":"<code>to_list(*, filter_defaults=False)</code>","text":"<p>Materialize to a plain Python list.</p> <p>Parameters:</p> Name Type Description Default <code>filter_defaults</code> <code>bool</code> <p>If <code>True</code>, drop default slots entirely (positions are lost). Otherwise, materialize defaults into concrete values.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[T | None]</code> <p>Either a list containing concrete values and materialized defaults, or (if <code>filter_defaults=True</code>) only the explicitly set values.</p> Notes <p>With <code>default_factory</code>, materialized defaults are freshly created and are not cached per-slot.</p>"},{"location":"reference/api/darnax/utils/layermap_utils/","title":"darnax.utils.layermap_utils","text":""},{"location":"reference/api/darnax/utils/layermap_utils/#darnax.utils.layermap_utils.layermap_apply","title":"<code>layermap_apply(f, select_idxs, lmap)</code>","text":"<p>Apply a transformation to parameter arrays only in selected modules of a LayerMap.</p> <p>This variant:   1. Partitions the LayerMap into its trainable (array) and static parts using      :func:<code>equinox.partition</code>.   2. Iterates over each module <code>(i, j)</code> in the LayerMap.   3. For selected modules (where <code>select_idxs((i, j))</code> is True):      - Flattens their parameter pytree into a list of array leaves.      - Applies <code>f</code> independently to each array leaf.      - Reconstructs the module with :func:<code>jax.tree_util.tree_unflatten</code>.   4. Reassembles the modified parameters with the untouched statics via      :func:<code>equinox.combine</code>.</p> <p>This design is purely functional \u2014 no in-place edits, no side effects, and full compatibility with JAX transformations when the structure is static.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable[[Array], Any]</code> <p>Function applied to each array leaf (parameter) of the selected modules. Must be elementwise or shape-preserving if you plan to <code>jit</code> the caller. The function can include arbitrary JAX operations.</p> required <code>select_idxs</code> <code>Callable[[tuple[int, int]], bool]</code> <p>Predicate that decides which modules are transformed. Receives the coordinate pair <code>(i, j)</code> representing the receiver and sender indices in the LayerMap entry <code>lmap[i][j]</code>. It must return <code>True</code> for modules that should be processed.</p> <p>Example::     lambda ij: ij[0] == ij[1]   # Select diagonal modules     lambda ij: ij[0] &lt; ij[1]    # Select upper-triangular modules</p> required <code>lmap</code> <code>LayerMap</code> <p>A LayerMap (dict-of-dicts pytree) whose values are Equinox modules. Each module can itself contain submodules and arrays. The structure is assumed to be static (not changing between calls).</p> required <p>Returns:</p> Type Description <code>LayerMap</code> <p>A new LayerMap in which the array leaves of selected modules have been transformed by <code>f</code>. Non-selected modules and all non-array leaves (statics, metadata, etc.) remain unchanged.</p> Notes <p>\u2022 Flattening and unflattening is done per module, not globally. This avoids   any structural mismatch between custom PyTree nodes (like LayerMap) and   standard dicts.</p> <p>\u2022 This approach offers explicit control over how transformations are applied   to leaves, while keeping the logic JAX-compatible and transparent.</p> <p>\u2022 It is preferred over using :func:<code>jax.vmap</code> for this use case, since <code>vmap</code>   is designed for batching along array axes, not for traversing heterogeneous   pytree leaves with differing shapes or dtypes.</p> <p>\u2022 The function is side-effect free: the input <code>lmap</code> is never modified.</p> Example"},{"location":"reference/api/darnax/utils/layermap_utils/#darnax.utils.layermap_utils.layermap_apply--scale-all-parameters-of-diagonal-modules-by-05","title":"Scale all parameters of diagonal modules by 0.5","text":"<p>new_lmap = layermap_apply_params_only_flat( ...     f=lambda x: 0.5 * x, ...     select_idxs=lambda ij: ij[0] == ij[1], ...     lmap=lmap, ... )</p>"},{"location":"reference/api/darnax/utils/layermap_utils/#darnax.utils.layermap_utils.layermap_apply--convert-upper-triangular-module-parameters-to-float32","title":"Convert upper-triangular module parameters to float32","text":"<p>new_lmap = layermap_apply_params_only_flat( ...     f=lambda x: x.astype(jnp.float32), ...     select_idxs=lambda ij: ij[0] &lt; ij[1], ...     lmap=lmap, ... )</p>"},{"location":"reference/api/darnax/utils/layermap_utils/#darnax.utils.layermap_utils.layermap_apply--add-gaussian-noise-to-parameters-of-all-modules","title":"Add Gaussian noise to parameters of all modules","text":"<p>key = jax.random.PRNGKey(0) def add_noise(x): ...     noise = jax.random.normal(key, shape=x.shape) * 0.01 ...     return x + noise ... new_lmap = layermap_apply_params_only_flat(add_noise, lambda ij: True, lmap)</p>"},{"location":"reference/api/darnax/utils/perceptron_rule/","title":"darnax.utils.perceptron_rule","text":"<p>Perceptron (OVA) backward update in JAX.</p> <p>This module implements a one-vs-all perceptron-style local update that returns a weight increment <code>\u0394W</code> to be added to a dense weight matrix.</p> <ul> <li>Supports batched inputs.</li> <li>Labels are in <code>{-1, +1}</code> per class (one-vs-all coding).</li> <li><code>y_hat</code> are raw scores (pre-activation).</li> <li>No learning rate is applied here.</li> </ul> Shapes <p>x       : (d,) or (n, d) y       : (n, K) in {-1, +1} y_hat   : (n, K) raw scores margin  : broadcastable to (n, K) \u2014 e.g. scalar, (K,), or (n, K) returns : (d, K)  (\u0394W to add to weights)</p>"},{"location":"reference/api/darnax/utils/perceptron_rule/#darnax.utils.perceptron_rule.perceptron_rule_backward","title":"<code>perceptron_rule_backward(x, y, y_hat, margin, gate=None)</code>","text":"<p>Multiclass (OVA) perceptron update (no learning rate).</p> <p>Applies an update when the margin condition is violated, i.e. <code>y * y_hat &lt;= margin</code> (ties count as mistakes). The result is a weight increment <code>\u0394W</code> aligned with a column-per-class layout.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input vector(s), shape <code>(d,)</code> or <code>(n, d)</code>.</p> required <code>y</code> <code>Array</code> <p>One-vs-all targets in <code>{-1, +1}</code>, shape <code>(n, K)</code>.</p> required <code>y_hat</code> <code>Array</code> <p>Raw scores (pre-activation), shape <code>(n, K)</code>.</p> required <code>margin</code> <code>Array</code> <p>Margin threshold, broadcastable to <code>(n, K)</code>; e.g. a scalar, a per-class vector <code>(K,)</code>, or an array <code>(n, K)</code>.</p> required <code>gate</code> <code>Array | None</code> <p>Multiplicative gate applied to the update, default is <code>1.0</code>. Should have shape broadcastable to x shape.</p> <code>None</code> <p>Returns:</p> Type Description <code>Array</code> <p>Weight update <code>\u0394W</code> of shape <code>(d, K)</code> to add to the weights.</p> Notes <ul> <li>Batch-size normalization: the update is divided by <code>n**0.5</code> so that its   magnitude is invariant to the batch size.</li> <li>Fan-in/width normalization (<code>1/sqrt(d)</code>) is applied here.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt; x = jnp.array([[1., 0.], [0., 1.]])      # (n=2, d=2)\n&gt;&gt;&gt; y = jnp.array([[+1, -1], [-1, +1]])      # (2, K=2)\n&gt;&gt;&gt; y_hat = jnp.array([[0.2, -0.3], [0.1, -0.4]])\n&gt;&gt;&gt; margin = 0.0\n&gt;&gt;&gt; dW = perceptron_rule_backward(x, y, y_hat, margin)\n&gt;&gt;&gt; dW.shape\n(2, 2)\n</code></pre>"},{"location":"reference/api/darnax/utils/typing/","title":"darnax.utils.typing","text":""},{"location":"reference/api/darnax/utils/typing/#darnax.utils.typing.PyTree","title":"<code>PyTree = Any</code>  <code>module-attribute</code>","text":""},{"location":"tutorials/","title":"Tutorials","text":"<p>This section provides useful tutorial to get started using this library, from JAX and Equinox basics to a first functioning example of a classifier on Entangled-MNIST.</p> <p>Models and concepts from this library are based on the paper by Badalotti, Baldassi, M\u00e9zard, Scardecchia and Zecchina. On the final tutorial, we will be training a 1-layer model on a simple benchmark dataset.</p> <p>The purpose of this library, however, is more general, as it allows for the quick and intuitive implementation of diverse architectures and learning schemes, loosely based on the principles of the original model.</p> <p>See the first tutorial to get started.</p> <p>For issues or questions, reach out to one of the Contributors.</p>"},{"location":"tutorials/01_jax_and_equinox/","title":"01 \u2014 JAX, Equinox, and Pytrees","text":"<p>This library is based on two foundational tools: JAX and Equinox. Understanding their philosophy is essential for working with this codebase. Both libraries share a unifying principle: everything is a pytree.</p> <p>This tutorial provides a concise introduction to these ideas. Readers are encouraged to consult the official documentation of JAX and Equinox for further technical details.</p>"},{"location":"tutorials/01_jax_and_equinox/#jax-transformations-of-numerical-functions","title":"JAX: Transformations of Numerical Functions","text":"<p>JAX extends NumPy with support for automatic differentiation and compilation. Its design emphasizes functional programming and composable transformations.</p> <p>Key concepts include:</p> <ul> <li>Array programming: JAX arrays follow NumPy semantics while executing efficiently on CPU, GPU, or TPU.</li> <li> <p>Transformations: JAX operates by transforming functions:</p> </li> <li> <p><code>jax.jit</code> compiles Python functions to optimized machine code.</p> </li> <li><code>jax.grad</code> computes derivatives automatically.</li> <li><code>jax.vmap</code> vectorizes functions across batch dimensions.</li> <li><code>jax.pmap</code> parallelizes computations across multiple devices.</li> <li>Composability: Transformations may be freely combined. For example, one may compute gradients of a JIT-compiled function or vectorize a function that already involves differentiation.</li> </ul> <p>The emphasis is not on predefined models or layers, but on transformations of user-defined functions.</p> <p>JAX gives the user a lot of powers, but this comes at a cost. If you're new to jax, be sure to read the Sharp bits.</p>"},{"location":"tutorials/01_jax_and_equinox/#equinox-pytrees-as-models","title":"Equinox: Pytrees as Models","text":"<p>Equinox is a lightweight neural network library for JAX. Its primary contribution is a consistent interface for defining models as plain Python classes.</p> <p>The design principles of Equinox are:</p> <ul> <li>Simplicity: Models are standard Python objects.</li> <li>Transparency: Parameters are stored directly as object attributes.</li> <li>Compatibility: Models are implemented as pytrees, allowing them to participate seamlessly in JAX transformations.</li> <li>Functional style: Updates to parameters or state return new objects, rather than mutating existing ones.</li> </ul> <p>This perspective aligns with the philosophy of this library: abstractions remain minimal, while full compatibility with JAX is preserved.</p>"},{"location":"tutorials/01_jax_and_equinox/#pytrees-a-unifying-abstraction","title":"Pytrees: A Unifying Abstraction","text":"<p>A pytree is a nested structure composed of Python containers (lists, tuples, dictionaries, dataclasses, and similar types) whose leaves are JAX arrays or compatible objects.</p> <p>Examples of pytrees:</p> <pre><code>import jax.numpy as jnp\n\n# Dictionary with arrays\nx = {\"a\": jnp.ones((2, 2)), \"b\": jnp.zeros((3,))}\n\n# Tuple of arrays\ny = (jnp.arange(3), jnp.ones((2,)))\n\n# Nested structures\nz = [x, y]\n</code></pre> <p>Pytrees are central to JAX for two reasons:</p> <ol> <li>They generalize beyond single arrays to arbitrary nested data structures.</li> <li>They allow JAX transformations to operate uniformly over these structures.</li> </ol> <p>In practice:</p> <ul> <li>A model is a pytree.</li> <li>Parameters and optimizer states are pytrees.</li> <li>Data batches may also be represented as pytrees.</li> </ul> <p>This single abstraction ensures that JAX transformations (such as <code>grad</code> or <code>jit</code>) can be applied consistently, regardless of structural complexity.</p>"},{"location":"tutorials/01_jax_and_equinox/#example-a-linear-module","title":"Example: A Linear Module","text":"<p>Equinox makes use of the pytree abstraction by treating models as pytrees.</p> <pre><code>import jax\nimport jax.numpy as jnp\nimport equinox as eqx\n\nclass Linear(eqx.Module):\n    weight: jax.Array\n    bias: jax.Array\n\n    def __init__(self, in_dim, out_dim, key):\n        wkey, bkey = jax.random.split(key)\n        self.weight = jax.random.normal(wkey, (in_dim, out_dim))\n        self.bias = jax.random.normal(bkey, (out_dim,))\n\n    def __call__(self, x):\n        return x @ self.weight + self.bias\n\n# Instantiate a model\nmodel = Linear(2, 3, jax.random.PRNGKey(0))\n\n# Forward evaluation\nx = jnp.ones((5, 2))\ny = model(x)\n\n# Differentiation\nloss_fn = lambda m, x: jnp.mean(m(x))\ngrads = jax.grad(loss_fn)(model, x)\n</code></pre> <p>Here, the model is both:</p> <ul> <li>a Python object with fields (<code>weight</code>, <code>bias</code>), and</li> <li>a pytree, enabling JAX to compute gradients and apply transformations directly.</li> </ul>"},{"location":"tutorials/01_jax_and_equinox/#philosophy-of-this-library","title":"Philosophy of This Library","text":"<p>The present library is guided by the following principles:</p> <ol> <li>Universality of pytrees: all major components (modules, layermaps, states) are structured as pytrees.</li> <li>Functional style: computations are expressed as pure functions, and updates return new objects.</li> <li>Composability: any component should be compatible with JAX transformations such as <code>jit</code>, <code>grad</code>, or <code>vmap</code>.</li> <li>Minimal abstraction: the library extends JAX and Equinox without concealing them. Users are encouraged to understand and directly employ these underlying tools.</li> </ol>"},{"location":"tutorials/01_jax_and_equinox/#next-tutorial","title":"Next Tutorial","text":"<p>The next tutorial will discuss the first component of this library: states and modules.</p>"},{"location":"tutorials/02_states/","title":"02 \u2014 States","text":"<p>This tutorial introduces States: the data structures that hold the current condition of the network. Unlike the transient activations of standard feedforward networks, states here are persistent, dynamical variables. They evolve iteratively under the network\u2019s dynamics and represent the system\u2019s position in configuration space, not merely intermediate results of a forward pass.</p>"},{"location":"tutorials/02_states/#1-conceptual-overview","title":"1. Conceptual overview","text":"<p>A state records the evolving configuration of the network at each layer. In the theoretical formulation, states are denoted</p> \\[ s^{(l)} \\in \\{\\pm 1\\}^N, \\] <p>but in this library they are general JAX arrays of arbitrary shape and dtype.</p> <p>Key points:</p> <ul> <li>Layer\u2013state relation. Every layer of the network is associated with a state buffer. This includes the input and output layers.</li> <li> <p>Initialization. When a batch <code>(x, y)</code> is presented:</p> </li> <li> <p>the input buffer is set to <code>x</code>,</p> </li> <li>the output buffer may be set to <code>y</code>,</li> <li>all intermediate buffers are initialized to zeros.</li> <li>Dynamics. States evolve by iterative updates until convergence (a fixed point or a steady regime). Later tutorials will explain these dynamics in detail.</li> <li>Shape generality. State buffers are not restricted to vectors. They may be multi-dimensional, e.g. <code>(H, W, C)</code> for convolutional or image-like architectures. The abstraction supports arbitrary shapes per layer.</li> </ul>"},{"location":"tutorials/02_states/#2-api-responsibilities","title":"2. API responsibilities","text":"<p>A state is deliberately minimal in API design but aligned with JAX\u2019s functional style. Each buffer is a JAX array, and the object provides simple functional accessors:</p> <pre><code>class State(eqx.Module):\n    def __getitem__(self, key: Any) -&gt; Array: ...\n    def init(self, x: Array, y: Array | None = None) -&gt; Self: ...\n    def replace(self, value: PyTree) -&gt; Self: ...\n    def replace_val(self, idx: Any, value: Array) -&gt; Self: ...\n</code></pre> <ul> <li> <p><code>__getitem__(key)</code>   Retrieves the buffer associated with <code>key</code> (e.g., layer index).</p> </li> <li> <p><code>init(x, y=None)</code>   Functionally initializes the state for a batch. Resizes all buffers to the batch dimension of <code>x</code>, sets the input to <code>x</code>, and optionally sets the output to <code>y</code>.</p> </li> <li> <p><code>replace(value)</code>   Returns a new state with the entire collection of buffers replaced by <code>value</code>.</p> </li> <li> <p><code>replace_val(idx, value)</code>   Returns a new state with only the buffer at position <code>idx</code> replaced by <code>value</code>.</p> </li> </ul> <p>All updates are functional and produce new objects. This immutability is essential for compatibility with JAX transformations.</p>"},{"location":"tutorials/02_states/#3-a-concrete-implementation-sequentialstate","title":"3. A concrete implementation: <code>SequentialState</code>","text":"<p><code>SequentialState</code> implements a left-to-right sequence of buffers, indexed by integers:</p> <ul> <li><code>0</code> = input buffer</li> <li><code>1, 2, \u2026, L-2</code> = intermediate buffers</li> <li><code>L-1</code> (or <code>-1</code>) = output buffer</li> </ul> <p>Internally, it stores a list of arrays with shapes <code>(B, *size_l)</code>, where <code>B</code> is the batch size.</p> <pre><code>state = SequentialState(sizes=[(4,), (8,), (3,)])\n</code></pre> <p>At construction time, buffers are initialized with dummy batch size <code>B=1</code>. The <code>init</code> method resizes them to the real batch size provided by <code>x</code>.</p>"},{"location":"tutorials/02_states/#example-initialization","title":"Example: initialization","text":"<pre><code>import jax\nimport jax.numpy as jnp\nfrom darnax.states.sequential import SequentialState\n\nstate = SequentialState(sizes=[(4,), (8,), (3,)])\n\nx = jax.random.normal(jax.random.PRNGKey(0), (32, 4))\ny = jax.random.normal(jax.random.PRNGKey(1), (32, 3))\n\ns0 = state.init(x, y)\nassert s0[0].shape == (32, 4)  # input\nassert s0[1].shape == (32, 8)  # hidden\nassert s0[-1].shape == (32, 3) # output\n</code></pre>"},{"location":"tutorials/02_states/#4-why-a-global-state","title":"4. Why a global state?","text":"<p>A deliberate design decision is to represent the state of the entire network globally, rather than letting each layer enclose its own state.</p> <p>The reasons are:</p> <ol> <li>Shared storage. Different layers may share access to portions of the same underlying state (e.g., in convolutional networks, multiple filters may act on overlapping regions of a global image-like buffer).</li> <li>Topological flexibility. A global state can be organized as a single multidimensional array or structured container, with different layers responsible for reading and writing specific slices.</li> <li>Consistency. This design allows heterogeneous architectures (dense, convolutional, graph-like) to operate on the same formal object without modifying the layer abstraction.</li> </ol> <p>This choice reflects the dynamical perspective: the network evolves as a whole, not as a collection of isolated layer-local states.</p>"},{"location":"tutorials/02_states/#5-states-as-pytrees","title":"5. States as pytrees","text":"<p>Because states are Equinox modules, they are pytrees. This has important consequences:</p> <ul> <li>JAX transformations (<code>jit</code>, <code>grad</code>, <code>vmap</code>, <code>pmap</code>) work seamlessly over state objects.</li> <li>Updates remain functional: replacing or modifying buffers yields new pytree instances.</li> <li>Static fields (e.g., <code>dtype</code>) are excluded from the dynamic leaves, reducing recompilation overhead.</li> </ul> <p>Thus, states are simultaneously containers of arrays and first-class JAX objects.</p>"},{"location":"tutorials/02_states/#6-summary","title":"6. Summary","text":"<ul> <li>States represent persistent network conditions, not just transient activations.</li> <li>Every layer is associated with part of the state, including input and output.</li> <li>States can store arbitrary shapes, enabling general architectures (vector-based, convolutional, etc.).</li> <li>The API is simple: indexing, functional initialization, and functional replacement.</li> <li>The design emphasizes a global state abstraction, allowing layers to share and reuse portions of it.</li> <li>Being pytrees, states integrate naturally with JAX\u2019s functional transformations.</li> </ul>"},{"location":"tutorials/03_modules/","title":"03 \u2014 Modules (Layers and Adapters)","text":"<p>This tutorial introduces Modules, the core building blocks of the library. Modules are divided into two categories:</p> <ul> <li>Layers: stateful modules that read and update a slice of the global network state.</li> <li>Adapters: stateless modules that transform one layer\u2019s state into a message for another.</li> </ul> <p>Only layers own a state. Both families may carry parameters; some are trainable (producing non-zero updates), others fixed (producing zero updates).</p>"},{"location":"tutorials/03_modules/#1-conceptual-background","title":"1. Conceptual background","text":"<p>The library is inspired by the recurrent dynamical model described in Dynamical Learning in Deep Asymmetric Recurrent Neural Networks. In that setting, each neuron (or unit) maintains a binary state \\(s \\in \\{-1, 1\\}\\), updated iteratively according to its local field:</p> \\[ s_i \\;\\leftarrow\\; \\operatorname{sign}\\!\\Big(\\sum_{j \\neq i} J_{ji}\\, s_j + J_D s_i \\Big). \\] <p>Extending to a multilayer chain, each layer \\(l\\) with state \\(s^{(l)}\\) also receives excitatory inputs from its neighbors with coupling \\(\\lambda\\):</p> \\[ s^{(l)}_i \\;\\leftarrow\\; \\operatorname{sign}\\!\\Big(\\sum_{j \\neq i} J^{(l)}_{ji}\\, s^{(l)}_j \\;+\\; J_D s^{(l)}_i \\;+\\; \\lambda (s^{(l-1)}_i + s^{(l+1)}_i) \\Big). \\] <p>Our modules provide a software abstraction of this process.</p> <ul> <li>Layers compute the recurrent/self contributions (\\(J\\), \\(J_D\\)) and handle aggregation + activation.</li> <li>Adapters contribute the cross-layer terms (e.g., \\(\\lambda s^{(l\\pm 1)}\\)).</li> </ul> <p>In other words, for a layer \\(l\\) with current state \\(s^{(l)}\\), a typical pre-activation is</p> \\[ h_i \\;=\\; \\underbrace{\\sum_{j \\neq i} J_{ji}\\, s_j + J_D s_i}_{\\text{layer self-message}} \\;+\\; \\underbrace{\\lambda\\, s_i^{(l-1)} + \\lambda\\, s_i^{(l+1)}}_{\\text{adapter messages}} \\] <p>and the new state is \\(s^{(l)} \\leftarrow \\text{activation}(h)\\).</p> <ul> <li>The self part is computed by the layer\u2019s <code>__call__</code>.</li> <li>The cross-layer parts are produced by adapters\u2019 <code>__call__</code> on neighboring (or otherwise connected) states.</li> <li>The layer\u2019s <code>reduce</code> performs the final aggregation into \\(h\\).</li> </ul>"},{"location":"tutorials/03_modules/#2-interfaces","title":"2. Interfaces","text":"<p>All modules extend a common base:</p> <pre><code>class AbstractModule(eqx.Module, ABC):\n\n    @property\n    @abstractmethod\n    def has_state(self) -&gt; bool: ...\n\n    @abstractmethod\n    def __call__(self, x: Array, rng: Array | None = None) -&gt; Array: ...\n\n    @abstractmethod\n    def backward(self, x: Array, y: Array, y_hat: Array) -&gt; Self: ...\n</code></pre> <ul> <li><code>__call__</code>: forward computation (a message).</li> <li><code>backward</code>: returns a module-shaped update. These are not gradients; they are local plasticity rules.</li> </ul>"},{"location":"tutorials/03_modules/#layers","title":"Layers","text":"<pre><code>class Layer(AbstractModule, ABC):\n\n    @property\n    def has_state(self) -&gt; bool: return True\n\n    @abstractmethod\n    def activation(self, x: Array) -&gt; Array: ...\n\n    @abstractmethod\n    def reduce(self, h: PyTree) -&gt; Array: ...\n</code></pre> <ul> <li><code>activation</code>: nonlinearity applied to the aggregated field.</li> <li><code>reduce</code>: combines incoming messages into a single tensor.</li> </ul>"},{"location":"tutorials/03_modules/#adapters","title":"Adapters","text":"<pre><code>class Adapter(AbstractModule, ABC):\n\n    @property\n    def has_state(self) -&gt; bool: return False\n</code></pre> <p>Adapters transform a source state into a message for another layer.</p>"},{"location":"tutorials/03_modules/#3-why-layer-states-are-messages","title":"3. Why layer states are \u201cmessages\u201d","text":"<p>In this architecture, a layer\u2019s current state is not just a transient activation, but the signal it emits to the rest of the network. Every update step consists of:</p> <ol> <li>Each layer publishing its state as a message.</li> <li>Adapters converting these messages into forms suitable for their targets.</li> <li>Layers aggregating self-messages and incoming adapter messages into \\(h\\).</li> <li>Layers applying their activation to obtain the new state.</li> </ol> <p>Thus, the global state itself is the medium of communication: states are messages.</p>"},{"location":"tutorials/03_modules/#4-example-modules","title":"4. Example modules","text":""},{"location":"tutorials/03_modules/#recurrentdiscrete","title":"RecurrentDiscrete","text":"<pre><code>class RecurrentDiscrete(Layer):\n    J: Array\n    J_D: Array\n    threshold: Array\n\n    def activation(self, x: Array) -&gt; Array:\n        return jnp.where(x &gt;= 0, 1, -1).astype(x.dtype)\n\n    def __call__(self, x: Array, rng=None) -&gt; Array:\n        return x @ self.J\n\n    def reduce(self, h: PyTree) -&gt; Array:\n        return jnp.asarray(tree_reduce(operator.add, h))\n\n    def backward(self, x: Array, y: Array, y_hat: Array) -&gt; Self:\n        dJ = perceptron_rule_backward(x, y, y_hat, self.threshold)\n        zero_update = jax.tree.map(jnp.zeros_like, self)\n        return eqx.tree_at(lambda m: m.J, zero_update, dJ)\n</code></pre> <ul> <li><code>__call__</code>: computes the recurrent/self-message.</li> <li><code>reduce</code>: aggregates all incoming messages.</li> <li><code>activation</code>: enforces \u00b11 states.</li> <li><code>backward</code>: computes a local perceptron-like rule to update \\(J\\).</li> </ul>"},{"location":"tutorials/03_modules/#ferromagnetic-adapter","title":"Ferromagnetic adapter","text":"<pre><code>class Ferromagnetic(Adapter):\n    strength: Array\n\n    def __call__(self, x: Array, rng=None) -&gt; Array:\n        return x * self.strength\n\n    def backward(self, x, y, y_hat) -&gt; Self:\n        return tree_map(jnp.zeros_like, self)\n</code></pre> <p>A fixed adapter implementing terms like \\(\\lambda s^{(l-1)}\\). Since it has no trainable parameters, the <code>backward</code> methods returns an array of zeros, meaning that the strength lambda remains unchanged.</p>"},{"location":"tutorials/03_modules/#5-one-update-step","title":"5. One update step","text":"<pre><code># state is a sequential state as shown in the first tutorial.\n# layer is our recurrent (or any other) implementation of the Layer\n# left and right are two adapters\n\ndef one_step(state, layer, left, right, l=1):\n    # we select the state of `layer`\n    s_l = state[l]\n    # we compute the self/recurrent update (message)\n    msg_self = layer(s_l)\n    # we compute the message from the left (s[l-1]) through the adapter\n    msg_l = left(state[l-1])\n    # we compute the message from the right (s[l+1]) through the adapter\n    msg_r = right(state[l+1])\n    # we aggregate the computed messages directed to the layer\n    h = layer.reduce([msg_self, msg_l, msg_r])\n    # we apply the non linearity to the result\n    s_next_l = layer.activation(h)\n    # we update the state at position l with the new value\n    return state.replace_val(l, s_next_l), h\n</code></pre>"},{"location":"tutorials/03_modules/#6-training-with-optax","title":"6. Training with Optax","text":"<p>Unlike gradient-based deep learning, learning here uses local updates. Each module\u2019s <code>backward</code> returns a module-shaped update, which we feed to Optax as if it were a gradient. This lets us retain momentum, schedules, clipping, etc., while remaining gradient-free.</p> <p>Be sure to check Optax documentation.</p> <pre><code>import optax\n\n# we define an optimizer such as adam\noptim = optax.adam(1e-2)\n\n# we use adam across our layer and adapters\nparams = (layer, left, right)\nopt_state = optim.init(eqx.filter(params, eqx.is_inexact_array))\n\ndef train_step(params, opt_state, state):\n    layer, left, right = params\n\n    # we compute the new state and preactivation as above\n    s1, h1 = one_step(state, layer, left, right)\n\n    # the backward functions compute updates of each layer\n    upd_layer = layer.backward(s1[1], s1[1], h1)\n    upd_left  = left.backward (s1[0], s1[0], h1)\n    upd_right = right.backward(s1[2], s1[2], h1)\n\n    # we use equinox to insert updates in the correct shapes\n    # this is a typical equinox/optax pattern\n    pseudo_grads = (upd_layer, upd_left, upd_right)\n    grads_f = eqx.filter(pseudo_grads, eqx.is_inexact_array)\n    params_f = eqx.filter(params, eqx.is_inexact_array)\n\n    # optax computes the updated parameters\n    updates, opt_state = optim.update(grads_f, opt_state, params=params_f)\n\n    # we apply them to our modules\n    params = eqx.apply_updates(params, updates)\n    return params, opt_state, s1\n</code></pre> <p>A training loop repeatedly calls <code>train_step</code>. Non-trainable adapters contribute zero updates, so they remain fixed. As said, we won't have to deal with the wiring of each layer directly. This will be handled by the orchestrator. Specifically, we will just need to call backward from the orchestrator to have a single update pytree of all modules in our network. Jax is totally transparent to the structure of our network, as long as it is a pytree.</p> <p>We provide a useful abstraction for this, see <code>LayerMap</code>.</p>"},{"location":"tutorials/03_modules/#7-multi-dimensional-states","title":"7. Multi-dimensional states","text":"<p>States can have arbitrary shapes: vectors <code>(B, F)</code> or image-like <code>(B, H, W, C)</code>. Layers and adapters generalize naturally as long as their operations are defined on those shapes. The global-state design also supports shared buffers, where multiple layers operate on slices of a single array.</p>"},{"location":"tutorials/03_modules/#8-summary","title":"8. Summary","text":"<ul> <li>Layers are stateful modules: self-message \u2192 reduce \u2192 activation.</li> <li>Adapters are stateless, transforming states into messages.</li> <li>States are messages: each layer\u2019s current state is the signal it emits.</li> <li>Backward rules: modules return structured updates, not gradients.</li> <li>Optax integration: these updates are fed as pseudo-gradients to Optax, enabling optimizer dynamics without autodiff.</li> <li>Global state: supports vector and multi-dimensional architectures with shared memory.</li> </ul> <p>This architecture mirrors the philosophy of distributed, local, gradient-free learning in asymmetric recurrent networks.</p>"},{"location":"tutorials/04_layermaps/","title":"04 \u2014 LayerMaps","text":"<p>A LayerMap is a PyTree wrapper that organizes the full set of layers and adapters in your network. It provides a consistent way to index them, guarantees immutability of the structure, and integrates seamlessly with JAX/Equinox/Optax.</p>"},{"location":"tutorials/04_layermaps/#1-matrix-view","title":"1. Matrix view","text":"<p>A LayerMap is conceptually a square matrix indexed by layer IDs:</p> <ul> <li>Diagonal <code>(i, i)</code>: the i-th layer. Each layer is stateful and sends a message to itself (its recurrent/self term).</li> <li>Off-diagonal <code>(i, j)</code> with <code>i \u2260 j</code>: an adapter. It converts the j-th state into a message for the i-th layer.</li> </ul> <p>This gives the following interpretation:</p> <ul> <li>Lower triangle (<code>i &gt; j</code>): forward adapters, messages flowing left \u2192 right.</li> <li>Upper triangle (<code>i &lt; j</code>): backward adapters, messages flowing right \u2192 left.</li> <li>Row <code>j</code>: everything that contributes into layer <code>j</code>.</li> <li>Column <code>i</code>: everything that originates from the state of layer <code>i</code>.</li> <li>Row 0: all connections going from layers to the input (unused for now).</li> <li>Row L: all connections going from layers to the output, meaning every layer whose state is used for prediction.</li> <li>Column 0: Forward skip connections from the input to each layer.</li> <li>Column L: Backward skip connections from the output to each layer.</li> </ul> <p>This matches the convention in <code>SequentialState</code>: <code>s[0]</code> is the input state, and <code>s[L]</code> is the output state.</p> <p>This structure, being in the end a dictionary of dictionaries is well suited for sparsity. For example, if layer <code>j</code> is not connected to layer <code>i</code>, there is no adapter in the <code>(i, j)</code> position. The element is simply not present in the structure, only the relevant modules are present.</p>"},{"location":"tutorials/04_layermaps/#2-api-overview","title":"2. API overview","text":"<pre><code>lm = LayerMap.from_dict({...})\n\nlm[i]: dict                        # read-only mapping of neighbors for row i\nlm[i, j]: Module                   # single module at (i, j)\n(i, j) in lm: bool                 # check if edge exists\n\nlm.rows(): tuple[int, ...]         # all row indices\nlm.cols_of(i): tuple[int, ...]     # all column indices in row i\nlm.neighbors(i): dict[int, Module] # read-only mapping {j: module} for row i\n\nlm.row_items(): Iterator[int, Module]                # iterate (row, neighbors)\nlm.edge_items(): Iterator[tuple[int , int], Module]  # iterate ((i, j), module)\nlm.to_dict(): dict[int, dict[int, Module]]           # copy as a dict-of-dicts\n</code></pre> <p>The API is intentionally dict-like but read-only: once built, the structure cannot be mutated. You cannot add layers or edges later.</p>"},{"location":"tutorials/04_layermaps/#3-immutability-of-structure","title":"3. Immutability of structure","text":"<p>A LayerMap is frozen once created:</p> <ul> <li>Row/column indices (the \u201cshape\u201d of the map) are part of the static treedef.</li> <li>Modules on each edge can change their parameters (via updates), but the adjacency cannot change.</li> </ul> <p>This immutability is not arbitrary. It is a basic requirement in JAX:</p> <ul> <li>The shape and structure of PyTrees must be static across JIT-compiled functions.</li> <li>If you were allowed to add or remove layers after creation, JIT cache keys would break and the compiled computation graph would need to be rebuilt every time.</li> <li>By freezing the structure, we ensure stability of compiled functions and allow the optimizer (Optax) to work on the entire network consistently.</li> </ul> <p>Thus, in JAX, data changes are dynamic, but structure is static.</p>"},{"location":"tutorials/04_layermaps/#4-example-building-a-simple-layermap","title":"4. Example: building a simple LayerMap","text":"<pre><code>import jax\nfrom darnax.modules.recurrent import RecurrentDiscrete\nfrom darnax.modules.ferromagnetic import Ferromagnetic\nfrom darnax.layer_maps.sparse import LayerMap\n\nkey0, key1 = jax.random.split(jax.random.PRNGKey(0))\nF = 8\n\n# Define two layers\nlayer0 = RecurrentDiscrete(features=F, j_d=0.0, threshold=0.0, key=key0)\nlayer1 = RecurrentDiscrete(features=F, j_d=0.0, threshold=0.0, key=key1)\n\n# Define adapters\nfwd_10 = Ferromagnetic(features=F, strength=0.5)  # forward (0 -&gt; 1)\nbwd_01 = Ferromagnetic(features=F, strength=0.2)  # backward (1 -&gt; 0)\n\n# Build dict-of-dicts\nraw = {\n    0: {0: layer0, 1: bwd_01},\n    1: {0: fwd_10, 1: layer1},\n}\n\nlm = LayerMap.from_dict(raw, require_diagonal=True)\n\n# Access\nprint(lm[1, 1])   # layer1\nprint(lm[1, 0])   # forward adapter\nprint(lm[1].keys())  # neighbors of row 1: {0, 1}\n</code></pre>"},{"location":"tutorials/04_layermaps/#5-a-layermap-as-a-pytree","title":"5. A LayerMap as a PyTree","text":"<p>Because <code>LayerMap</code> is registered as a PyTree:</p> <ul> <li>The keys (rows, columns) are static.</li> <li>The modules (layers/adapters) are leaves.</li> <li>Arrays inside those modules are visible to JAX and Optax.</li> </ul> <p>This means you can treat the entire network as a single object:</p> <pre><code>import equinox as eqx, optax\n\nopt = optax.adam(1e-2)\nopt_state = opt.init(eqx.filter(lm, eqx.is_inexact_array))\n\n# Later in training\nupdates, opt_state = opt.update(grads, opt_state, params=lm)\nlm = eqx.apply_updates(lm, updates)\n</code></pre> <p>All parameters inside all layers/adapters are updated in one go.</p>"},{"location":"tutorials/04_layermaps/#6-summary","title":"6. Summary","text":"<ul> <li>LayerMap = a collection of layers (diagonal) and adapters (off-diagonal) with integer keys.</li> <li>Matrix view: rows = inputs to a layer, columns = outputs from a layer.</li> <li>Input/output rows and columns handle special roles.</li> <li>Immutable structure: you cannot add or remove layers once built. This ensures JAX stability (PyTree structure must be static under JIT).</li> <li>PyTree integration: treat the whole network as one object, pass it to Equinox/Optax, and every parameter is handled correctly.</li> </ul> <p>This design makes LayerMap a central abstraction: a static graph of modules whose parameters evolve dynamically during training, while its topology remains fixed.</p>"},{"location":"tutorials/04_layermaps/#7-an-ascii-art","title":"7. An ascii art","text":"<pre><code>LayerMap (rows = receivers, columns = senders)\n\n            columns (senders: states/messages from j) \u2192\n            0          1          2        ...        L-1         L\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\nr    0   \u2502[L00]     [A01]     [A02\u2191]   \u2026            [A0,L-1]   [A0L\u2191]  \u2502\no        \u2502layer0    back      back                  back       back    \u2502\nw        \u2502(input)   adapters  adapters              adapters   adapters\u2502\ns        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n(    1   \u2502[A10\u2193]    [L11]     [A12\u2191]   \u2026            [A1,L-1]   [A1L]   \u2502\nr        \u2502fwd\u2192      layer1    \u2191back                 back       back    \u2502\ne        \u2502adapters            adapters              adapters   adapters\u2502\nc        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\ne    2   \u2502[A20\u2193]    [A21\u2193]    [L22]    \u2026            [A2,L-1\u2191]  [A2L\u2191]  \u2502\ni        \u2502fwd\u2192      fwd\u2192      layer2                \u2191 back     \u2191 back  \u2502\nv        \u2502adapters  adapters                        adapters   adapters\u2502\ne        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\nr    \u2026   \u2502\u2026          \u2026        \u2026        \u2026            \u2026          \u2026       \u2502\ns        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n     L   \u2502[AL0\u2193]    [AL1\u2193]    [AL2\u2193]   \u2026            [AL,L-1\u2193]  [LL]    \u2502\n         \u2502fwd\u2192      fwd\u2192      fwd\u2192                  fwd\u2192       \u2191layerL \u2502\n         \u2502adapters  adapters  adapters              adapters   (output)\u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2191\n           rows (receivers: layer i to be updated)\n</code></pre> <p>Legend:</p> <ul> <li>Lii   : layer on the diagonal (stateful). L00 is the input-layer slot; LL is the output-layer slot.</li> <li>Aij\u2193  : adapter at (i,j) with i &gt; j (lower triangle) \u2014 forward message (from j \u2192 i).</li> <li>Aij\u2191  : adapter at (i,j) with i &lt; j (upper triangle) \u2014 backward message (from j \u2192 i).</li> </ul> <p>Row/Column intuition:</p> <ul> <li>Row i collects everything needed to update layer i: the diagonal Lii (self-message) plus all Aij that transform state j into a message for i.</li> <li>Column j lists everything that uses state j as a source: the diagonal Ljj plus all Aij that send j\u2019s state to other layers.</li> </ul> <p>Input/Output:</p> <ul> <li>First column (\u00b7,0): forward skip connections from the input state to every layer.</li> <li>Last column (\u00b7,L): backward skip connections from the output state to earlier layers.</li> <li>Last row (L,\u00b7): all contributors that feed directly into the output layer (final prediction).</li> </ul> <p>Structure:</p> <ul> <li>Diagonal = layers; off-diagonal = adapters.</li> <li>The LayerMap\u2019s structure (rows/cols and which edges exist) is immutable after creation.</li> </ul>"},{"location":"tutorials/05_orchestrators/","title":"05 \u2014 Orchestrators","text":"<p>Having introduced states, modules, and layermaps, we now come to the final structural element: the orchestrator. Where the previous abstractions describe what the network is, the orchestrator specifies how the network evolves in time. It is the execution engine of the architecture: at each iteration it routes messages, applies updates, and advances the global state.</p>"},{"location":"tutorials/05_orchestrators/#1-conceptual-role","title":"1. Conceptual role","text":"<p>The orchestrator governs the dynamical process that characterizes this family of recurrent networks.</p> <p>As everything in the library, we propose a simple interface for the object, described below, and a first implementation. In this case, we focus on the <code>SequentialOrchestrator</code>, that plays nicely with sequential states and the sequential nature of the layer map. In detail, at each step, this specific orchestrator does the following:</p> <ol> <li>Message collection \u2014 for each receiving layer \\(i\\), all modules \\((i,j)\\) in the LayerMap are evaluated on the current sender states \\(s^{(j)}\\).</li> <li>Aggregation and activation \u2014 the diagonal module \\((i,i)\\) aggregates the incoming messages into a pre-activation \\(h^{(i)}\\) via its <code>reduce</code> method, then applies its nonlinearity with <code>activation</code>, yielding the new state slice \\(s^{(i)}\\).</li> <li>State update \u2014 the new slice replaces the old one in the global state, producing the next global configuration.</li> <li>Learning updates \u2014 in the training regime, each module also provides a local parameter update through its <code>backward(x, y, y_hat)</code> method. These are gathered into a LayerMap-structured PyTree of updates.</li> </ol> <p>In this way, the orchestrator realizes the iterative dynamics described in the paper: a distributed, gradient-free learning mechanism in which information is exchanged locally and parameters are updated via local rules.</p>"},{"location":"tutorials/05_orchestrators/#2-two-phases-of-dynamics","title":"2. Two phases of dynamics","text":"<p>In line with the two-phase protocol introduced in the theoretical model, the orchestrator provides two distinct update functions:</p> <ul> <li> <p>Training phase (<code>step</code>)   All available messages are considered, both forward (lower triangle) and backward (upper triangle). This corresponds to the supervised or clamped regime where input and output information are both present.</p> </li> <li> <p>Inference phase (<code>step_inference</code>)   Only causal messages are retained: for receiver \\(i\\), senders \\(j &lt; i\\) are discarded. Thus, information from the output or \u201cfuture\u201d layers does not leak backward. This corresponds to the free relaxation regime in which the system stabilizes autonomously.</p> </li> </ul> <p>This explicit separation ensures that training and inference dynamics are clearly distinguished in the implementation.</p>"},{"location":"tutorials/05_orchestrators/#3-public-api","title":"3. Public API","text":"<p>All orchestrators subclass the following abstract interface:</p> <pre><code>class AbstractOrchestrator(eqx.Module):\n    lmap: LayerMap  # fixed topology (rows, columns, edges)\n\n    def step(self, state: StateT, *, rng: KeyArray) -&gt; tuple[StateT, KeyArray]:\n        \"\"\"Run one full update step (training phase).\"\"\"\n\n    def step_inference(self, state: StateT, *, rng: KeyArray) -&gt; tuple[StateT, KeyArray]:\n        \"\"\"Run one update step (inference phase, discarding rightward messages).\"\"\"\n\n    def predict(self, state: SequentialState, rng: KeyArray) -&gt; tuple[SequentialState, KeyArray]:\n        \"\"\"Update the output state s[-1].\"\"\"\n\n    def backward(self, state: StateT, rng: KeyArray) -&gt; LayerMap:\n        \"\"\"Compute module-local updates in a LayerMap-structured PyTree.\"\"\"\n</code></pre>"},{"location":"tutorials/05_orchestrators/#stepstate-rng","title":"<code>step(state, rng)</code>","text":"<ul> <li>Executes one synchronous update of the network using all messages, both backward and forward.</li> <li>Returns the updated state and an advanced random key.</li> <li>It does not compute messages for the last row (messages going towards the output). So the last component of the state is never updated when applying <code>step</code>. There is a specific method called <code>predict</code> that needs to be run after the end of the dynamics if you want the actual prediction of the model.</li> </ul>"},{"location":"tutorials/05_orchestrators/#step_inferencestate-rng","title":"<code>step_inference(state, rng)</code>","text":"<ul> <li>Executes one update considering only messages from \\(j \\geq i\\).</li> <li>Used for prediction after training, ensuring purely causal message passing.</li> <li>As step, it does not compute messages for the last row (messages going towards the output). So the last component of the state is never updated when applying <code>step</code>. There is a specific method called <code>predict</code> that needs to be run after the end of the dynamics if you want the actual prediction of the model.</li> </ul>"},{"location":"tutorials/05_orchestrators/#predictstate-rng","title":"<code>predict(state, rng)</code>","text":"<ul> <li>Executes the update of the output state via usual forward + aggregation + activation. It is useful to check the implementation of <code>OutputLayer</code> to actually understand what is happening, as it is simply a sink that aggregates prediction from its neighboring adapters.</li> <li>Separating internal state update and final prediction allows to save some operations (computing the update of the output state at every step is useless), and also simplifies the overall API, see Tutorial 6.</li> </ul>"},{"location":"tutorials/05_orchestrators/#backwardstate-rng","title":"<code>backward(state, rng)</code>","text":"<ul> <li>For each edge \\((i,j)\\), invokes <code>lmap[i,j].backward(x=state[j], y=state[i], y_hat=local_field)</code> to obtain a module-shaped update.</li> <li>Returns a LayerMap with the same static structure as the original, but whose leaves are parameter updates.</li> <li>This PyTree can be passed directly to Optax as if it were a gradient structure.</li> </ul>"},{"location":"tutorials/05_orchestrators/#4-structural-properties","title":"4. Structural properties","text":"<ul> <li>Static topology: The orchestrator\u2019s LayerMap has a fixed set of rows, columns, and edges. This immutability is necessary for JAX compatibility, as PyTree structures must remain constant across compiled functions.</li> <li>Dynamic values: Within this static skeleton, the array values of module parameters evolve freely during training.</li> <li>PyTree compliance: Because the orchestrator itself is an Equinox module, it is also a PyTree. Its parameters can be filtered, updated, and optimized exactly like any other object in the system.</li> <li>Transformation compatibility: The orchestrator is fully compatible with <code>jax.jit</code>, <code>jax.vmap</code>, and all other JAX transformations. Since the topology is static, compilation is stable; only array values trigger recompilation when their shapes change.</li> </ul>"},{"location":"tutorials/05_orchestrators/#5-typical-usage","title":"5. Typical usage","text":"<p>A typical training loop involving an orchestrator proceeds as follows:</p> <pre><code># Forward update (training regime)\nstate, rng = orchestrator.step(state, rng=rng)\n\n# Forward update (inference regime)\nstate, rng = orchestrator.step_inference(state, rng=rng)\n\n# Compute module-local updates\nupd_lmap = orchestrator.backward(state, rng=rng)\n\n# Apply updates with Optax\ngrads  = eqx.filter(upd_lmap, eqx.is_inexact_array)\nparams = eqx.filter(orchestrator.lmap, eqx.is_inexact_array)\ndeltas, opt_state = opt.update(grads, opt_state, params=params)\nnew_lmap = eqx.apply_updates(orchestrator.lmap, deltas)\n\n# Replace the LayerMap inside the orchestrator\norchestrator = eqx.tree_at(lambda o: o.lmap, orchestrator, new_lmap)\n</code></pre> <p>Here the updates are not gradients: they are the outcome of local learning rules defined at the module level. Optax is used purely as a robust update engine.</p>"},{"location":"tutorials/05_orchestrators/#6-more-complex-logic","title":"6. More complex logic","text":"<p>Right until now we described a specific instance of orchestrator, i.e. the <code>SequentialOrchestrator</code>. It plays nicely with both the sequantial state and the sequential implementation of the layer map.</p> <p>The only assumption about this structures is a notion of order in the network, meaning that the first layer comes first, the second comes second, etc...</p> <p>This is totally arbitrary, this library allows for any structure and logic in the network functioning, a first example might be a totally synchronous network, where there is no notion of order and all layers are treated in sync. It is also possible to define a group structure, where different layers belong to different groups, each handled concurrently. This might involve an extension of the <code>LayerMap</code> to allow for string keys to identify groups...</p> <p>Another option is to specialize architectures for speed and efficiency. Instead of working with dict of dicts and simple loops, we might want to decide to pad and stack layer states together to be handled in parallel. This is also an easy extension of <code>LayerMap</code> and <code>Orchestrator</code>.</p> <p>We might even put orchestrators inside single modules, to encapsulate a into a single object complex logic.</p>"},{"location":"tutorials/05_orchestrators/#7-summary","title":"7. Summary","text":"<ul> <li>The orchestrator advances the network\u2019s dynamics by routing messages, aggregating them, and updating the global state.</li> <li><code>step</code> executes the full supervised/clamped update; <code>step_inference</code> executes the free, causal update.</li> <li><code>backward</code> collects local updates into a LayerMap-shaped PyTree, aligned with the parameter structure, enabling seamless integration with Optax.</li> <li>The orchestrator is a PyTree with static structure: immutable topology, mutable parameter values. This guarantees full compatibility with JAX transformations and ensures efficient compilation.</li> </ul> <p>Through the orchestrator, the network acquires its temporal dimension: states evolve, messages flow, and local rules drive learning, exactly as described in the underlying theoretical framework.</p>"},{"location":"tutorials/06_simple_net_on_artificial_data/","title":"06 \u2014 Simple Training Tutorial","text":"<p>This mini-notebook shows an end\u2011to\u2011end training loop. Notice that it resembles plain pytorch in the sense that you need to write your own train_Step and eval_step. We will see:</p> <ul> <li>How to define a <code>SequentialOrchestrator</code> over a sparse <code>LayerMap</code>.</li> <li>How to update the model via <code>.backward(...)</code> used as pseudo\u2011gradients for Optax.</li> <li>How to define train_steps, eval_steps and update_steps.</li> </ul> <p>The goal here is clarity: well\u2011ordered cells, consistent dtypes/PRNG usage, and inline comments explaining each step. We are not taking full advantage of jax for now. For example here we never call Jit on any function. Also, the dynamics <code>orchestrator.step/step_inference</code> is well suited for <code>jax.lax.scan</code>, but here we will just use a regular python-side for loop.</p> <pre><code># --- Imports ---------------------------------------------------------------\nimport time\nfrom typing import Any\n\nimport jax\nimport jax.numpy as jnp\nimport equinox as eqx\nimport optax\n\nfrom darnax.orchestrators.sequential import SequentialOrchestrator\nfrom darnax.modules.fully_connected import FrozenFullyConnected, FullyConnected\nfrom darnax.modules.recurrent import RecurrentDiscrete\nfrom darnax.modules.input_output import OutputLayer\nfrom darnax.layer_maps.sparse import LayerMap\nfrom darnax.states.sequential import SequentialState\n</code></pre>"},{"location":"tutorials/06_simple_net_on_artificial_data/#utilities-metrics-summaries","title":"Utilities (metrics &amp; summaries)","text":"<p>Small helpers used for monitoring. Labels are OVA \u00b11 and predictions are decoded via <code>argmax</code> over the output scores.</p> <pre><code>def batch_accuracy(y_true: jnp.ndarray, y_pred: jnp.ndarray) -&gt; float:\n    \"\"\"Accuracy with \u00b11 OVA labels (class = argmax along last dim).\"\"\"\n    y_true_idx = jnp.argmax(y_true, axis=-1)\n    y_pred_idx = jnp.argmax(y_pred, axis=-1)\n    return float(jnp.mean(y_true_idx == y_pred_idx))\n\n\ndef print_state_summary(s_out: jnp.ndarray, header: str = \"Output state\") -&gt; None:\n    \"\"\"Quick shape/range summary for an output buffer.\"\"\"\n    smin = float(jnp.min(s_out))\n    smax = float(jnp.max(s_out))\n    smu = float(jnp.mean(s_out))\n    sstd = float(jnp.std(s_out))\n    print(\n        f\"{header}: shape={tuple(s_out.shape)}, range=({smin:.3f},{smax:.3f}), mean={smu:.3f}, std={sstd:.3f}\"\n    )\n</code></pre>"},{"location":"tutorials/06_simple_net_on_artificial_data/#dataset-binary-prototypes","title":"Dataset: Binary Prototypes.","text":"<p>This object defines the data that we will be training our model on. It is a very simple task. The object is a compact generator that creates: - <code>K</code> prototype vectors in <code>{\u22121,+1}^D</code>. - A dataset of size <code>N</code> by corrupting prototypes with random bit-flips with probability <code>p_noise</code>. - \u00b11 labels (<code>+1</code> on the correct class, <code>\u22121</code> elsewhere), that indicate which prototype the data is coming from.</p> <pre><code>class PrototypeData:\n    \"\"\"Binary prototype dataset with OVA \u00b11 labels.\"\"\"\n\n    RAND_THRESHOLD = 0.5\n\n    def __init__(\n        self,\n        key: jax.Array,\n        batch_size: int = 16,\n        num_prototypes: int = 10,\n        dim_prototypes: int = 100,\n        num_data: int = 1000,\n        p_noise: float = 0.3,\n    ):\n        assert 0 &lt;= p_noise &lt; self.RAND_THRESHOLD, f\"Invalid {p_noise=}\"\n        assert num_data &gt;= num_prototypes, f\"Invalid {num_data=}, {num_prototypes=}\"\n        assert batch_size &gt; 1, f\"Invalid {batch_size=}\"\n        self.num_prototypes = int(num_prototypes)\n        self.dim_prototypes = int(dim_prototypes)\n        self.num_data = int(num_data)\n        self.p_noise = float(p_noise)\n        self.batch_size = int(batch_size)\n        self.num_batches = -(-self.num_data // self.batch_size)  # ceil division\n\n        key_prototypes, key_data = jax.random.split(key)\n        self._create_prototypes(key_prototypes)\n        self._create_data(key_data)\n\n    def __iter__(self):\n        \"\"\"Yield batches `(x, y)` as in the original implementation.\"\"\"\n        return zip(\n            jnp.array_split(self.x, self.num_batches),\n            jnp.array_split(self.y, self.num_batches),\n            strict=True,\n        )\n\n    def _create_prototypes(self, key: jax.Array) -&gt; None:\n        \"\"\"Generate \u00b11 prototypes (float32).\"\"\"\n        # Use rademacher \u2192 {\u22121,+1} with explicit float32 dtype.\n        self.prototypes = jax.random.rademacher(\n            key, shape=(self.num_prototypes, self.dim_prototypes), dtype=jnp.float32\n        )\n\n    def _create_data(self, key: jax.Array) -&gt; None:\n        \"\"\"Generate dataset by repeating prototypes and flipping signs with prob. `p_noise`.\"\"\"\n        # Build OVA labels: +1 on diag, \u22121 elsewhere, then repeat to length N.\n        self.y = jnp.full(\n            shape=(self.num_prototypes, self.num_prototypes), fill_value=-1.0, dtype=jnp.float32\n        )\n        self.y = self.y.at[jnp.diag_indices_from(self.y)].set(1.0)\n        self.y = jnp.repeat(\n            self.y,\n            self.num_data // self.num_prototypes + 1,\n            axis=0,\n            total_repeat_length=self.num_data,\n        )\n\n        # Repeat prototypes to length N, then flip signs with probability p_noise.\n        key, carry = jax.random.split(key)\n        self.x = jnp.repeat(\n            self.prototypes,\n            self.num_data // self.num_prototypes + 1,\n            axis=0,\n            total_repeat_length=self.num_data,\n        )\n        flip_mask = jax.random.bernoulli(carry, p=1 - self.p_noise, shape=self.x.shape) * 2.0 - 1.0\n        self.x = self.x * flip_mask\n        # Shuffle x and y in sync.\n        shuffle = jax.random.permutation(key, self.num_data)\n        self.x = self.x[shuffle].astype(jnp.float32)\n        self.y = self.y[shuffle].astype(jnp.float32)\n</code></pre>"},{"location":"tutorials/06_simple_net_on_artificial_data/#build-model-state","title":"Build Model &amp; State","text":"<p>Here we define a simple model with one hidden layer and fully-connected adapters, similar to a perceptron with one hidden layer, except that the hidden layer has internal recurrency.</p> <p>The topology of the layer map is the following: - Layer 1 (hidden) receives from input (0, forth), itself (1, recurrent), and labels (2, back). - Layer 2 (output) receives from hidden (1) and itself (2).</p> <p>Some first comments:</p> <ul> <li>Notice that we do not define a input layer, that would correspond to layer 0 in the receivers. This is totally fine and intended. The input in this case is simply a sent message and never updated. If we dont define layer 0 in the receivers, the first component of the state is fixed.</li> <li>You should inspect the implementation of the OutputLayer. It does not have an internal state, parameters, and the <code>__call__</code> function returns an array of zeros. It is basically a sink that aggregates messages from all layers that contribute to the output and sums them. This behaviour can change in the future with the definition of new OutputLayers with a more complex logic, but for now it is basically an aggregator.</li> </ul> <pre><code>DIM_DATA = 100\nNUM_DATA = 1000\nNUM_LABELS = 10\nDIM_HIDDEN = 256\nTHRESHOLD_OUT = 3.5\nTHRESHOLD_IN = 3.5\nTHRESHOLD_J = 0.5\nSTRENGTH_BACK = 0.3\nSTRENGTH_FORTH = 1.0\nJ_D = 0.5\n\n# Global state with three buffers: input (0), hidden (1), output/labels (2)\nstate = SequentialState((DIM_DATA, DIM_HIDDEN, NUM_LABELS))\n\n# Distinct keys per module to avoid accidental correlations.\nmaster_key = jax.random.key(seed=44)\nkeys = jax.random.split(master_key, num=5)\n\nlayer_map = {\n    # Hidden row (1): from input (0), self (1), and labels (2)\n    1: {\n        0: FullyConnected(\n            in_features=DIM_DATA,\n            out_features=DIM_HIDDEN,\n            strength=STRENGTH_FORTH,\n            threshold=THRESHOLD_IN,\n            key=keys[0],\n        ),\n        1: RecurrentDiscrete(features=DIM_HIDDEN, j_d=J_D, threshold=THRESHOLD_J, key=keys[1]),\n        2: FrozenFullyConnected(\n            in_features=NUM_LABELS,\n            out_features=DIM_HIDDEN,\n            strength=STRENGTH_BACK,\n            threshold=0.0,\n            key=keys[2],\n        ),\n    },\n    # Output row (2): from hidden (1), and itself (2)\n    2: {\n        1: FullyConnected(\n            in_features=DIM_HIDDEN,\n            out_features=NUM_LABELS,\n            strength=1.0,\n            threshold=THRESHOLD_OUT,\n            key=keys[3],\n        ),\n        2: OutputLayer(),  # the 2-2 __call__ is a vector of zeros, it does not contribute\n    },\n}\nlayer_map = LayerMap.from_dict(layer_map)\n\n# Trainable orchestrator built from the fixed topology.\norchestrator = SequentialOrchestrator(layers=layer_map)\n</code></pre>"},{"location":"tutorials/06_simple_net_on_artificial_data/#optimizer","title":"Optimizer","text":"<p>We choose Adam with <code>lr=5e-3</code>. As you can see we can call <code>eqx.filter</code> directly on the orchestrator, since it is a pytree. We also show how to update the model with the function <code>update(orchestrator, state, optimizer)</code>.</p> <pre><code>optimizer = optax.adam(5e-3)\nopt_state = optimizer.init(eqx.filter(orchestrator, eqx.is_inexact_array))\n\n\ndef update(\n    orchestrator: SequentialOrchestrator, state: SequentialState, optimizer, optimizer_state, rng\n) -&gt; tuple[SequentialOrchestrator, Any]:\n    \"\"\"Compute and applies the updates and returns the updated model.\n\n    It also returns the optimizer state, typed as Any for now.\n    \"\"\"\n    # 1) Local deltas (orchestrator-shaped deltas).\n    grads = orchestrator.backward(state, rng=rng)\n\n    # 2) Optax over the orchestrator (tree structures match by construction).\n    # This is common equinox + optax pattern, used in the same way when training\n    # \"regular\" deep learning models.\n    # First we filter fields with equinox and then we give them to the optimizer.\n    # This allows us to handle complex pytrees with static fields seamlessly during\n    # training.\n    params = eqx.filter(orchestrator, eqx.is_inexact_array)\n    grads = eqx.filter(grads, eqx.is_inexact_array)\n\n    # 3) We compute the updates and apply them to our model\n    updates, opt_state = optimizer.update(grads, optimizer_state, params=params)\n    orchestrator = eqx.apply_updates(orchestrator, updates)\n\n    # 4) We return the updated model and the optimizer state\n    return orchestrator, opt_state\n</code></pre>"},{"location":"tutorials/06_simple_net_on_artificial_data/#dynamics-helpers","title":"Dynamics helpers","text":"<p>We now define two simple functions that run during training, they are extremely simple. During run_dynamics_training we have two phases: a first one where we compute all messages and run the dynamics with both forward and backward messages. We run this phase for a fixed number of steps. Then, we do a second phase where we suppress all messages \"going backward\", we run this dynamics for a fixed number of steps and we obtain a second fixed point s^*.</p> <p>During inference, we only run the dynamics with suppressed messages from the right.</p> <pre><code>def run_dynamics_training(\n    orch: SequentialOrchestrator,\n    s,\n    rng: jax.Array,\n    steps: int,\n):\n    \"\"\"Run `steps` iterations using ALL messages (clamped phase).\n\n    Note: Kept identical to your working version for consistency.\n    \"\"\"\n    for _ in range(steps):\n        s, rng = orch.step(s, rng=rng)\n    for _ in range(steps):\n        s, rng = orch.step_inference(s, rng=rng)\n    return s, rng\n\n\ndef run_dynamics_inference(\n    orch: SequentialOrchestrator,\n    s,\n    rng: jax.Array,\n    steps: int,\n):\n    \"\"\"Run `steps` iterations discarding rightward/backward messages (free phase).\"\"\"\n    for _ in range(steps):\n        s, rng = orch.step_inference(s, rng=rng)\n    return s, rng\n</code></pre>"},{"location":"tutorials/06_simple_net_on_artificial_data/#train-step","title":"Train step","text":"<p>This function summarizes the whole training protocol, for a single batch.</p> <p>Protocol per batch: 1. Initialize/clamp the global state with <code>(x, y)</code>. 2. Run training dynamics for <code>2 * T_train</code> steps. 3. Update the model with <code>update</code>, as seen before.</p> <pre><code>def train_step(\n    orch: SequentialOrchestrator,\n    s: SequentialState,\n    x: jnp.ndarray,\n    y: jnp.ndarray,\n    rng: jax.Array,\n    *,\n    opt_state,\n    optimizer,\n    t_train: int = 3,\n):\n    # 1) Clamp current batch (inputs &amp; labels).\n    s = s.init(x, y)\n\n    # 2) Training dynamics (kept as-is).\n    s, rng = run_dynamics_training(orch, s, rng, steps=t_train)\n\n    # 3) Update the model\n    rng, update_key = jax.random.split(rng)\n    orch, opt_state = update(orch, s, optimizer, opt_state, update_key)\n    return orch, rng, opt_state\n</code></pre>"},{"location":"tutorials/06_simple_net_on_artificial_data/#eval-step","title":"Eval step","text":"<p>Initializes with inputs only (labels are just for metrics), then runs the inference dynamics and computes metrics.</p> <pre><code>def eval_step(\n    orch: SequentialOrchestrator,\n    s: SequentialState,\n    x: jnp.ndarray,\n    y: jnp.ndarray,\n    rng: jax.Array,\n    *,\n    t_eval: int = 5,\n) -&gt; tuple[SequentialOrchestrator, SequentialState, dict, jax.Array]:\n    s = s.init(x, None)\n    s, rng = run_dynamics_inference(orch, s, rng, steps=t_eval)\n    s, rng = orchestrator.predict(s, rng)\n    y_pred = s[-1]\n    metrics = {\"acc\": batch_accuracy(y, y_pred)}\n    return metrics, rng\n</code></pre>"},{"location":"tutorials/06_simple_net_on_artificial_data/#run-the-training","title":"Run the training","text":"<p>Finally, we build a small <code>PrototypeData</code> stream, train for a few epochs using <code>train_step</code> per batch, and evaluate on the same stream.</p> <pre><code># Constants\nP_NOISE = 0.3\nBATCH_SIZE = 16\nEPOCHS = 3\nT_TRAIN = 10  # training dynamics steps per batch\nT_EVAL = 10  # short inference steps for monitoring\n\n# RNGs\nmaster_key = jax.random.key(59)\nmaster_key, data_key = jax.random.split(master_key)\n\n# Data\ndata = PrototypeData(\n    key=data_key,\n    batch_size=BATCH_SIZE,\n    num_prototypes=NUM_LABELS,\n    dim_prototypes=DIM_DATA,\n    num_data=NUM_DATA,\n    p_noise=P_NOISE,\n)\nprint(f\"Dataset: x.shape={tuple(data.x.shape)}  y.shape={tuple(data.y.shape)}\")\n\n# Training config\n\nhistory = {\"acc\": []}\n\nfor epoch in range(1, EPOCHS + 1):\n    t0 = time.time()\n    print(f\"\\n=== Epoch {epoch}/{EPOCHS} ===\")\n    for x_batch, y_batch in data:\n        # Keep batch arrays float32 (consistency)\n        master_key, train_key, eval_key = jax.random.split(master_key, num=3)\n        orchestrator, master_key, opt_state = train_step(\n            orchestrator,\n            state,\n            x_batch,\n            y_batch,\n            rng=train_key,\n            opt_state=opt_state,\n            optimizer=optimizer,\n            t_train=T_TRAIN,\n        )\n        metrics, rng = eval_step(orchestrator, state, x_batch, y_batch, eval_key)\n    history[\"acc\"].append(metrics[\"acc\"])\n    print(f\"Epoch {epoch} done in {time.time()-t0:.2f}s\")\n    print(f\"Mean accuracy={float(jnp.mean(jnp.array(history['acc']))):.3f}\")\n</code></pre>"},{"location":"tutorials/06_simple_net_on_artificial_data/#final-evaluation-demo","title":"Final evaluation (demo)","text":"<p>Single pass over the same iterator; replace with a held\u2011out set in practice.</p> <pre><code>eval_acc = []\nfor x_b, y_b in data:\n    x_batch = x_b.astype(jnp.float32)\n    y_batch = y_b.astype(jnp.float32)\n    master_key, step_key = jax.random.split(master_key)\n    metrics, master_key = eval_step(\n        orchestrator,\n        state,\n        x_batch,\n        y_batch,\n        rng=step_key,\n        t_eval=T_EVAL,\n    )\n    eval_acc.append(metrics[\"acc\"])\n\nprint(\"\\n=== Final evaluation summary ===\")\nprint(f\"Accuracy={float(jnp.mean(jnp.array(eval_acc))):.3f}\")\n</code></pre>"},{"location":"tutorials/07_optimized_training_on_mnist/","title":"07 \u2014 Training on MNIST with darnax (JAX-first, CPU)","text":"<p>This tutorial is for readers who want to understand how darnax is used in practice: how a network is assembled from modules, how the orchestrator runs recurrent dynamics, and how local plasticity integrates with Equinox/Optax to update parameters without backpropagation.</p> <p>We\u2019ll implement a compact, JAX-friendly training loop:</p> <ul> <li>Only the outer steps are <code>jit</code>-compiled: <code>train_step</code>, <code>eval_step</code>.</li> <li>Recurrent dynamics use <code>jax.lax.scan</code> (no Python loops inside compiled code).</li> <li>The dataset iterator is slice-based (avoid <code>array_split</code>), better for accelerators and CPUs.</li> <li>We stay on CPU to keep the focus on design; the code is accelerator-ready.</li> </ul> <pre><code># --- Imports ---------------------------------------------------------------\nfrom __future__ import annotations\n\nimport logging\nimport time\nfrom typing import TYPE_CHECKING\n\nimport equinox as eqx\nimport jax\nimport jax.numpy as jnp\nimport optax\nfrom datasets import load_dataset\n\nfrom darnax.layer_maps.sparse import LayerMap\nfrom darnax.modules.fully_connected import FrozenFullyConnected, FullyConnected\nfrom darnax.modules.input_output import OutputLayer\nfrom darnax.modules.recurrent import RecurrentDiscrete\nfrom darnax.orchestrators.sequential import SequentialOrchestrator\nfrom darnax.states.sequential import SequentialState\n\nif TYPE_CHECKING:\n    from collections.abc import Iterator\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\n</code></pre>"},{"location":"tutorials/07_optimized_training_on_mnist/#1-what-darnax-gives-you","title":"1) What darnax gives you","text":"<p>darnax decomposes a model into modules connected by a LayerMap. At runtime, an orchestrator applies a message-passing schedule over a state vector.</p> <ul> <li>Modules (edges/diagonals) consume a sender buffer and emit a message to a receiver.   Some modules are trainable; some are frozen; some are \u201cdiagonal\u201d (operate on a buffer itself).</li> <li>The LayerMap defines the fixed topology: for each receiver row <code>i</code>, which senders <code>j</code>   contribute, and with which module.</li> <li>The SequentialOrchestrator drives the update order (left\u2192right, recurrent self, right\u2192left   as needed) and exposes:</li> <li><code>step</code>: full dynamics (all messages allowed).</li> <li><code>step_inference</code>: inference dynamics (typically suppress \u201cbackward\u201d messages).</li> <li><code>backward</code>: compute local parameter deltas from the current state (no backprop).</li> <li><code>predict</code>: produce output scores in the final buffer.</li> <li>The State is a fixed-shape tuple of buffers <code>(input, hidden, output)</code>. You clamp   inputs (and possibly labels) by writing them into the state, then run dynamics to a fixed point.</li> </ul>"},{"location":"tutorials/07_optimized_training_on_mnist/#2-a-tiny-metric-helper","title":"2) A tiny metric helper","text":"<p>Labels are One-Vs-All (OVA) in \u00b11. We decode predictions via <code>argmax</code>.</p> <pre><code>def batch_accuracy(y_true: jnp.ndarray, y_pred: jnp.ndarray) -&gt; float:\n    \"\"\"Accuracy with \u00b11 OVA labels (class = argmax along last dim).\"\"\"\n    y_true_idx = jnp.argmax(y_true, axis=-1)\n    y_pred_idx = jnp.argmax(y_pred, axis=-1)\n    return jnp.mean((y_true_idx == y_pred_idx).astype(jnp.float32))\n</code></pre>"},{"location":"tutorials/07_optimized_training_on_mnist/#3-dataset-object-designed-for-jax","title":"3) Dataset object designed for JAX","text":"<p>We keep the data pipeline deliberately simple to highlight the model mechanics:</p> <ul> <li>Materialize once: training subset and full test split live as device arrays.</li> <li>Shared projection: an optional linear projection (same matrix for train/test) reduces   dimensionality and can be followed by a <code>sign</code> transform (Entangled-MNIST style).</li> <li>Slice-based batches: iterators yield contiguous chunks; no list materialization or   <code>array_split</code>.</li> <li>Label scaling (your original choice):</li> <li>true class \u2192 <code>+\u221aC / 2</code></li> <li>others \u2192 <code>\u22120.5</code></li> </ul> <p>This scaling biases the output field to favor the target class during clamped dynamics.</p> <pre><code>class MNISTData:\n    \"\"\"MNIST dataset with optional linear projection and sign; slice-based iterators.\n\n    Design:\n      - Single in-memory materialization (train subset, full test).\n      - Shared projection across splits.\n      - Deterministic batch slicing (precomputed ranges).\n    \"\"\"\n\n    TOTAL_SIZE_PER_CLASS = 5900  # train split\n    TEST_SIZE_PER_CLASS = 1000  # test split\n    NUM_CLASSES = 10\n    FLAT_DIM = 28 * 28\n\n    def __init__(\n        self,\n        key: jax.Array,\n        batch_size: int = 64,\n        linear_projection: int | None = 100,\n        apply_sign_transform: bool = True,\n        num_images_per_class: int = TOTAL_SIZE_PER_CLASS,\n    ):\n        \"\"\"Initialize the dataset object.\"\"\"\n        # Lightweight validation; fail fast on easy mistakes.\n        if not (linear_projection is None or isinstance(linear_projection, int)):\n            raise TypeError(\"`linear_projection` must be `None` or `int`.\")\n        if batch_size &lt;= 1:\n            raise ValueError(f\"Invalid batch_size={batch_size!r}; must be &gt; 1.\")\n        if not (0 &lt; num_images_per_class &lt;= self.TOTAL_SIZE_PER_CLASS):\n            raise ValueError(f\"`num_images_per_class` must be in [1, {self.TOTAL_SIZE_PER_CLASS}]\")\n\n        self.linear_projection = linear_projection\n        self.apply_sign_transform = bool(apply_sign_transform)\n\n        self.num_data = int(num_images_per_class) * self.NUM_CLASSES\n        self.batch_size = int(batch_size)\n        self.num_batches = -(-self.num_data // self.batch_size)  # ceil div\n\n        # Build arrays once.\n        self._create_dataset(key)\n\n        # Precompute slicing ranges for train/eval.\n        self._train_bounds = [\n            (i * self.batch_size, min((i + 1) * self.batch_size, self.num_data))\n            for i in range(self.num_batches)\n        ]\n        self.num_eval_data = int(self.x_eval.shape[0])\n        self.num_eval_batches = -(-self.num_eval_data // self.batch_size)\n        self._eval_bounds = [\n            (i * self.batch_size, min((i + 1) * self.batch_size, self.num_eval_data))\n            for i in range(self.num_eval_batches)\n        ]\n\n    # ------------------------------- Public API ------------------------------- #\n    def __iter__(self) -&gt; Iterator[tuple[jax.Array, jax.Array]]:\n        \"\"\"Yield `(x, y)` training batches by contiguous slicing.\"\"\"\n        for lo, hi in self._train_bounds:\n            yield self.x[lo:hi], self.y[lo:hi]\n\n    def iter_eval(self) -&gt; Iterator[tuple[jax.Array, jax.Array]]:\n        \"\"\"Yield `(x_eval, y_eval)` validation batches (full test split).\"\"\"\n        for lo, hi in self._eval_bounds:\n            yield self.x_eval[lo:hi], self.y_eval[lo:hi]\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the number of batches.\"\"\"\n        return self.num_batches\n\n    # ------------------------------ Internals ------------------------------ #\n    @staticmethod\n    def _load_mnist_split(split: str) -&gt; tuple[jax.Array, jax.Array]:\n        \"\"\"Load MNIST split and flatten to (N, 784).\"\"\"\n        assert split in [\"train\", \"test\"]\n        ds = load_dataset(\"mnist\")\n        x = jnp.asarray([jnp.array(im) for im in ds[split][\"image\"]], dtype=jnp.float32)\n        y = jnp.asarray(ds[split][\"label\"], dtype=jnp.int32)\n        x = x.reshape(x.shape[0], -1) / 255.0\n        return x, y\n\n    @staticmethod\n    def _labels_to_pm1_scaled(y_scalar: jax.Array, num_classes: int) -&gt; jax.Array:\n        \"\"\"Original scaling: +\u221aC/2 at the true class, \u22120.5 elsewhere.\"\"\"\n        one_hot = jax.nn.one_hot(y_scalar, num_classes, dtype=jnp.float32)\n        return one_hot * (num_classes**0.5 / 2.0) - 0.5\n\n    @staticmethod\n    def _random_projection_matrix(key: jax.Array, out_dim: int, in_dim: int) -&gt; jax.Array:\n        \"\"\"Gaussian projection with variance 1/in_dim to keep outputs ~unit variance.\"\"\"\n        return jax.random.normal(key, (out_dim, in_dim), dtype=jnp.float32) / jnp.sqrt(in_dim)\n\n    @staticmethod\n    def _take_per_class(\n        key: jax.Array, x: jax.Array, y: jax.Array, k_per_class: int\n    ) -&gt; tuple[jax.Array, jax.Array]:\n        \"\"\"Uniformly sample `k_per_class` examples for each class 0..9.\"\"\"\n        xs, ys = [], []\n        for cls in range(MNISTData.NUM_CLASSES):\n            key, sub = jax.random.split(key)\n            idx = jnp.where(y == cls)[0]\n            if k_per_class &gt; idx.shape[0]:\n                raise ValueError(\n                    f\"Requested {k_per_class} for class {cls}, but only {idx.shape[0]} available.\"\n                )\n            perm = jax.random.permutation(sub, idx.shape[0])\n            take = idx[perm[:k_per_class]]\n            xs.append(x[take])\n            ys.append(y[take])\n        return jnp.concatenate(xs, axis=0), jnp.concatenate(ys, axis=0)\n\n    def _maybe_project_and_sign(self, w: jax.Array | None, x: jax.Array) -&gt; jax.Array:\n        \"\"\"Apply optional linear projection + optional sign nonlinearity (Entangled-MNIST).\"\"\"\n        if w is not None:\n            x = (x @ w.T).astype(jnp.float32)\n        if self.apply_sign_transform:\n            sgn = jnp.sign(x)\n            x = jnp.where(sgn == 0, jnp.array(-1.0, dtype=sgn.dtype), sgn)\n        return x\n\n    def _create_dataset(self, key: jax.Array) -&gt; None:\n        \"\"\"Materialize train subset and full test split with consistent preprocessing.\"\"\"\n        key_sample, key_proj, key_shuf_tr = jax.random.split(key, 3)\n\n        # Load raw splits.\n        x_tr_all, y_tr_all = self._load_mnist_split(\"train\")\n        x_ev_all, y_ev_scalar = self._load_mnist_split(\"test\")\n\n        # Uniform per-class sampling.\n        k_train = self.num_data // self.NUM_CLASSES\n        x_tr, y_tr_scalar = self._take_per_class(key_sample, x_tr_all, y_tr_all, k_train)\n\n        # Shared projection/sign across splits.\n        w = (\n            self._random_projection_matrix(key_proj, int(self.linear_projection), x_tr.shape[-1])\n            if self.linear_projection is not None\n            else None\n        )\n        x_tr = self._maybe_project_and_sign(w, x_tr)\n        x_ev = self._maybe_project_and_sign(w, x_ev_all)\n\n        # Labels with original scaling; shuffle train only.\n        y_tr = self._labels_to_pm1_scaled(y_tr_scalar, self.NUM_CLASSES)\n        perm_tr = jax.random.permutation(key_shuf_tr, x_tr.shape[0])\n        self.x, self.y = x_tr[perm_tr], y_tr[perm_tr]\n\n        self.x_eval = x_ev\n        self.y_eval = self._labels_to_pm1_scaled(y_ev_scalar, self.NUM_CLASSES)\n\n        # Convenience metadata.\n        self.input_dim = int(self.x.shape[1])\n</code></pre>"},{"location":"tutorials/07_optimized_training_on_mnist/#4-model-topology-as-a-layermap","title":"4) Model topology as a LayerMap","text":"<p>We build a minimal network with one hidden layer and an output sink:</p> <ul> <li>Receiver row 1 (hidden) gets messages from:</li> <li>0 (input) via <code>FullyConnected</code> (forward path)</li> <li>1 (itself) via <code>RecurrentDiscrete</code> (internal recurrency)</li> <li>2 (labels) via <code>FrozenFullyConnected</code> (backward/clamping path)</li> <li>Receiver row 2 (output) gets:</li> <li>1 (hidden) via <code>FullyConnected</code> (readout)</li> <li>2 (itself) via <code>OutputLayer</code> (diagonal sink/aggregator, returns zeros)</li> </ul> <p>The SequentialState is <code>(input, hidden, output)</code> with fixed sizes. The SequentialOrchestrator knows how to: - aggregate edge messages for each receiver, - apply diagonal modules, - and run the chosen schedule (<code>step</code>, <code>step_inference</code>, <code>predict</code>, <code>backward</code>).</p> <pre><code>DIM_DATA = 100\nNUM_LABELS = MNISTData.NUM_CLASSES\nDIM_HIDDEN = 300\n\nTHRESHOLD_OUT = 1.0\nTHRESHOLD_IN = 1.0\nTHRESHOLD_J = 1.0\nSTRENGTH_BACK = 0.5\nSTRENGTH_FORTH = 5.0\nJ_D = 0.5\n\n# Global state with three buffers: input (0), hidden (1), output/labels (2)\nstate = SequentialState((DIM_DATA, DIM_HIDDEN, NUM_LABELS))\n\n# Independent keys for each module (avoid accidental correlations).\nmaster_key = jax.random.key(seed=44)\nkeys = jax.random.split(master_key, num=5)\n\nlayer_map = {\n    1: {  # Hidden row receives from input, itself, and labels\n        0: FullyConnected(\n            in_features=DIM_DATA,\n            out_features=DIM_HIDDEN,\n            strength=STRENGTH_FORTH,\n            threshold=THRESHOLD_IN,\n            key=keys[0],\n        ),\n        1: RecurrentDiscrete(\n            features=DIM_HIDDEN,\n            j_d=J_D,\n            threshold=THRESHOLD_J,\n            key=keys[1],\n        ),\n        2: FrozenFullyConnected(  # clamping/teaching signal, not trainable\n            in_features=NUM_LABELS,\n            out_features=DIM_HIDDEN,\n            strength=STRENGTH_BACK,\n            threshold=0.0,\n            key=keys[2],\n        ),\n    },\n    2: {  # Output row receives from hidden and aggregates\n        1: FullyConnected(\n            in_features=DIM_HIDDEN,\n            out_features=NUM_LABELS,\n            strength=1.0,\n            threshold=THRESHOLD_OUT,\n            key=keys[3],\n        ),\n        2: OutputLayer(),  # diagonal sink: produces zeros; acts as aggregator anchor\n    },\n}\nlayer_map = LayerMap.from_dict(layer_map)\n\n# Trainable orchestrator built from the fixed topology.\norchestrator = SequentialOrchestrator(layers=layer_map)\nlogger.info(\"Model initialized with SequentialOrchestrator.\")\n</code></pre>"},{"location":"tutorials/07_optimized_training_on_mnist/#5-optimizer-and-the-no-backprop-update","title":"5) Optimizer and the \u201cno-backprop\u201d update","text":"<p>darnax does not use backpropagation here. Instead:</p> <ol> <li>Run recurrent dynamics with the current batch clamped (inputs + labels in the state).</li> <li>Call <code>orchestrator.backward(state, rng)</code> to get local deltas for every trainable module.</li> <li>Apply those deltas using Optax\u2014this gives you the familiar optimizer ergonomics.</li> </ol> <p>Notes for JAX compilation: - We pass the optimizer object as an argument to the jitted functions.   Under <code>eqx.filter_jit</code>, non-array args are static. Reusing the same instance prevents retracing. - Only the optimizer state (arrays) flows through the jitted code.</p> <pre><code>optimizer = optax.adam(2e-3)\nopt_state = optimizer.init(eqx.filter(orchestrator, eqx.is_inexact_array))\n\n\ndef _apply_update(\n    orch: SequentialOrchestrator,\n    s: SequentialState,\n    opt_state,\n    rng: jax.Array,\n    optimizer,\n):\n    \"\"\"Compute local deltas via .backward, then apply Optax updates.\n\n    Why separate this helper?\n      - Clear separation of concerns (dynamics vs parameter updates).\n      - Easier to unit-test and profile independently.\n    \"\"\"\n    grads = orch.backward(s, rng=rng)  # local deltas, tree-shaped like `orch`\n    params = eqx.filter(orch, eqx.is_inexact_array)  # trainable leaves\n    grads = eqx.filter(grads, eqx.is_inexact_array)  # drop non-arrays from grads\n\n    updates, opt_state = optimizer.update(grads, opt_state, params=params)\n    orch = eqx.apply_updates(orch, updates)\n    return orch, opt_state\n</code></pre>"},{"location":"tutorials/07_optimized_training_on_mnist/#6-dynamics-with-laxscan-not-jitted-directly","title":"6) Dynamics with <code>lax.scan</code> (not jitted directly)","text":"<p>The orchestrator exposes one-step transitions: - <code>step(s, rng)</code> \u2192 (s\u2019, rng\u2019): full dynamics (includes backward/label messages). - <code>step_inference(s, rng)</code> \u2192 (s\u2019, rng\u2019): inference-only dynamics (suppress backward messages).</p> <p>We wrap those into scans. These helpers are not jitted on their own; they are traced as part of the outer jitted steps. That keeps the code modular and the compiled graph clean.</p> <pre><code>def _scan_steps(fn, s: SequentialState, rng: jax.Array, steps: int):\n    \"\"\"Scan `steps` times a (s, rng)-&gt;(s, rng) transition.\"\"\"\n\n    def body(carry, _):\n        s, rng = carry\n        s, rng = fn(s, rng=rng)\n        return (s, rng), None\n\n    (s, rng), _ = jax.lax.scan(body, (s, rng), xs=None, length=steps)\n    return s, rng\n\n\ndef run_dynamics_training(\n    orch: SequentialOrchestrator,\n    s: SequentialState,\n    rng: jax.Array,\n    steps: int,\n):\n    \"\"\"Clamped phase (full dynamics) followed by a short free relaxation (inference).\"\"\"\n    s, rng = _scan_steps(orch.step, s, rng, steps)  # clamped\n    s, rng = _scan_steps(orch.step_inference, s, rng, steps)  # free\n    return s, rng\n\n\ndef run_dynamics_inference(\n    orch: SequentialOrchestrator,\n    s: SequentialState,\n    rng: jax.Array,\n    steps: int,\n):\n    \"\"\"Inference-only relaxation to a fixed point.\"\"\"\n    s, rng = _scan_steps(orch.step_inference, s, rng, 2 * steps)\n    return s, rng\n</code></pre>"},{"location":"tutorials/07_optimized_training_on_mnist/#7-outer-steps-the-only-jit-compiled-functions","title":"7) Outer steps (the only <code>jit</code>-compiled functions)","text":"<p>We jit only the functions that are called many times and represent the outer boundary of our computation:</p> <ul> <li> <p><code>train_step</code> (per batch):   1) write <code>(x, y)</code> into the state (clamp),   2) run clamped + free dynamics,   3) compute local deltas and apply the Optax update.</p> </li> <li> <p><code>eval_step</code> (per batch):   1) write <code>x</code> only,   2) run free dynamics,   3) <code>predict</code> and compute accuracy.</p> </li> </ul> <p>JIT boundary discipline: - Static args (optimizer object, Python ints like <code>t_train</code>) trigger retraces only if they   change. Keep them fixed during a run.</p> <pre><code>@eqx.filter_jit\ndef train_step(\n    orch: SequentialOrchestrator,\n    s: SequentialState,\n    x: jnp.ndarray,\n    y: jnp.ndarray,\n    rng: jax.Array,\n    *,\n    opt_state,\n    optimizer,\n    t_train: int = 3,\n):\n    \"\"\"Perform a train step in a single batch.\"\"\"\n    # 1) Clamp inputs + labels into the global state.\n    s = s.init(x, y)\n\n    # 2) Recurrent dynamics: clamped phase then free relaxation.\n    s, rng = run_dynamics_training(orch, s, rng, steps=t_train)\n\n    # 3) Local deltas + Optax update.\n    rng, update_key = jax.random.split(rng)\n    orch, opt_state = _apply_update(orch, s, opt_state, update_key, optimizer)\n    return orch, rng, opt_state\n\n\n@eqx.filter_jit\ndef eval_step(\n    orch: SequentialOrchestrator,\n    s: SequentialState,\n    x: jnp.ndarray,\n    y: jnp.ndarray,\n    rng: jax.Array,\n    *,\n    t_eval: int = 5,\n) -&gt; tuple[float, jax.Array]:\n    \"\"\"Perform a validation step on a single batch.\"\"\"\n    # 1) Clamp inputs only (labels aren't used by dynamics here).\n    s = s.init(x, None)\n\n    # 2) Free relaxation to a fixed point.\n    s, rng = run_dynamics_inference(orch, s, rng, steps=t_eval)\n\n    # 3) Predict scores from the settled state and measure accuracy.\n    s, rng = orch.predict(s, rng)\n    y_pred = s[-1]\n    acc = batch_accuracy(y, y_pred)\n    return acc, rng\n</code></pre>"},{"location":"tutorials/07_optimized_training_on_mnist/#8-training-loop-cpu","title":"8) Training loop (CPU)","text":"<p>The Python epoch loop shepherds data and RNG. All heavy lifting happens inside the two jitted steps above. Practical guidance:</p> <ul> <li>Keep array shapes/dtypes and the pytrees\u2019 structures stable across calls.</li> <li>Reuse the same optimizer instance; pass its state through the jitted code.</li> <li>If you change <code>t_train</code>/<code>t_eval</code> between calls, expect a retrace (they are static).</li> </ul> <pre><code># Experiment knobs\nNUM_IMAGES_PER_CLASS = 5400\nAPPLY_SIGN_TRANSFORM = True\nBATCH_SIZE = 16\nEPOCHS = 5\nT_TRAIN = 10  # clamped + free steps per batch\nT_EVAL = 10  # inference steps multiplier (2*T_EVAL iterations)\n\n# RNGs\nmaster_key = jax.random.key(59)\nmaster_key, data_key = jax.random.split(master_key)\n\n# Data\ndata = MNISTData(\n    key=data_key,\n    batch_size=BATCH_SIZE,\n    linear_projection=DIM_DATA,\n    apply_sign_transform=APPLY_SIGN_TRANSFORM,\n    num_images_per_class=NUM_IMAGES_PER_CLASS,\n)\nprint(f\"Dataset ready \u2014 x.shape={tuple(data.x.shape)}, y.shape={tuple(data.y.shape)}\")\n\n# Train &amp; evaluate\nfor epoch in range(1, EPOCHS + 1):\n    t0 = time.time()\n    print(f\"\\n=== Epoch {epoch}/{EPOCHS} ===\")\n\n    # Training epoch\n    for x_batch, y_batch in data:\n        master_key, step_key = jax.random.split(master_key)\n        orchestrator, master_key, opt_state = train_step(\n            orchestrator,\n            state,\n            x_batch,\n            y_batch,\n            rng=step_key,\n            opt_state=opt_state,\n            optimizer=optimizer,  # static in the JIT sense; same instance every call\n            t_train=T_TRAIN,\n        )\n\n    # Evaluation epoch (full test split)\n    accs = []\n    for x_b, y_b in data.iter_eval():\n        master_key, step_key = jax.random.split(master_key)\n        acc, master_key = eval_step(\n            orchestrator,\n            state,\n            x_b.astype(jnp.float32),\n            y_b.astype(jnp.float32),\n            rng=step_key,\n            t_eval=T_EVAL,\n        )\n        accs.append(acc)\n\n    acc_epoch = float(jnp.mean(jnp.array(accs)))\n    print(f\"Eval Accuracy = {acc_epoch:.3f}  |  epoch time: {time.time() - t0:.2f}s\")\n</code></pre>"},{"location":"tutorials/07_optimized_training_on_mnist/#9-one-line-final-report","title":"9) One-line final report","text":"<p>This is just to have a single scalar you can grep from logs or compare across runs.</p> <pre><code>final_accs = []\nfor x_b, y_b in data.iter_eval():\n    master_key, step_key = jax.random.split(master_key)\n    acc, master_key = eval_step(\n        orchestrator,\n        state,\n        x_b.astype(jnp.float32),\n        y_b.astype(jnp.float32),\n        rng=step_key,\n        t_eval=T_EVAL,\n    )\n    final_accs.append(acc)\n\nprint(\"\\n=== Final evaluation summary ===\")\nprint(f\"Accuracy = {float(jnp.mean(jnp.array(final_accs))):.3f}\")\n</code></pre>"},{"location":"tutorials/07_optimized_training_on_mnist/#10-recap-next-steps","title":"10) Recap &amp; next steps","text":"<p>You just trained a recurrent, locally-plastic network on MNIST using darnax:</p> <ul> <li>You declared topology with a <code>LayerMap</code>, not a layer stack.</li> <li>A state of fixed buffers <code>(input, hidden, output)</code> was clamped and then   relaxed to a fixed point by the orchestrator.</li> <li>You updated parameters using local deltas (<code>orchestrator.backward</code>) funneled through Optax.</li> <li>You JIT-compiled the outer loop only, using <code>lax.scan</code> for inner dynamics.</li> </ul> <p>If you\u2019re serious about scaling this:</p> <ul> <li>Parallel orchestrators: swap <code>SequentialOrchestrator</code> for a parallel flavor when your   graphs grow (careful with data dependencies).</li> <li>Topology as data: generate <code>LayerMap</code> programmatically (e.g., blocks, conv-like bands).</li> <li>Per-block scalings: match initialization and LR magnitudes to each path\u2019s fan-in/out.</li> <li>Profiling: dump HLO for <code>train_step</code>/<code>eval_step</code>, sanity-check fusion and shape stability.</li> </ul> <p>Don\u2019t just accept the defaults\u2014pressure-test the schedule and the rules. If a path doesn\u2019t pull its weight (e.g., backward clamp too weak/strong), instrument it and fix it.</p>"},{"location":"tutorials/08_trainer_on_mnist/","title":"08 \u2014 Training on MNIST with darnax using <code>DynamicalTrainer</code> (JAX-first, CPU)","text":"<p>We employ the same network/topology as the previous tutorial\u2014but we employ an object called DynamicalTrainer that encapsulates the training and validation logic in a pytorch-lightning style. Many trainers are available.</p> <pre><code>from __future__ import annotations\n\nimport logging\nfrom typing import TYPE_CHECKING\n\nimport jax\nimport equinox as eqx\nimport jax.numpy as jnp\nimport optax\nfrom datasets import load_dataset\n\nfrom darnax.layer_maps.sparse import LayerMap\nfrom darnax.modules.fully_connected import FrozenFullyConnected, FullyConnected\nfrom darnax.modules.input_output import OutputLayer\nfrom darnax.modules.recurrent import RecurrentDiscrete\nfrom darnax.orchestrators.sequential import SequentialOrchestrator\nfrom darnax.states.sequential import SequentialState\nfrom darnax.trainers.dynamical import DynamicalTrainer\n\nif TYPE_CHECKING:\n    from collections.abc import Iterator\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\n</code></pre>"},{"location":"tutorials/08_trainer_on_mnist/#accuracy-helper-1-ova-labels","title":"Accuracy helper (\u00b11 OVA labels)","text":"<pre><code>def batch_accuracy(y_true: jnp.ndarray, y_pred: jnp.ndarray) -&gt; float:\n    \"\"\"Compute the accuracy for a given batch.\"\"\"\n    y_true_idx = jnp.argmax(y_true, axis=-1)\n    y_pred_idx = jnp.argmax(y_pred, axis=-1)\n    return jnp.mean((y_true_idx == y_pred_idx).astype(jnp.float32))\n</code></pre>"},{"location":"tutorials/08_trainer_on_mnist/#minimal-mnist-data-projection-optional-sign","title":"Minimal MNIST data (projection + optional sign)","text":"<pre><code>class MNISTData:\n    \"\"\"Simple implementation of MNIST dataset.\"\"\"\n\n    NUM_CLASSES = 10\n    FLAT_DIM = 28 * 28\n    TOTAL_SIZE_PER_CLASS = 5900\n    TEST_SIZE_PER_CLASS = 1000\n\n    def __init__(\n        self,\n        key: jax.Array,\n        batch_size: int = 64,\n        linear_projection: int | None = 100,\n        apply_sign_transform: bool = True,\n        num_images_per_class: int = TOTAL_SIZE_PER_CLASS,\n    ):\n        \"\"\"Initialize MNIST.\"\"\"\n        if batch_size &lt;= 1:\n            raise ValueError(\"batch_size must be &gt; 1\")\n        if not (0 &lt; num_images_per_class &lt;= self.TOTAL_SIZE_PER_CLASS):\n            raise ValueError(\"num_images_per_class out of range\")\n        self.linear_projection = linear_projection\n        self.apply_sign_transform = bool(apply_sign_transform)\n        self.num_data = int(num_images_per_class) * self.NUM_CLASSES\n        self.batch_size = int(batch_size)\n        self.num_batches = -(-self.num_data // self.batch_size)\n        self._build(key)\n        self._train_bounds = [\n            (i * self.batch_size, min((i + 1) * self.batch_size, self.num_data))\n            for i in range(self.num_batches)\n        ]\n        self.num_eval_data = int(self.x_eval.shape[0])\n        self.num_eval_batches = -(-self.num_eval_data // self.batch_size)\n        self._eval_bounds = [\n            (i * self.batch_size, min((i + 1) * self.batch_size, self.num_eval_data))\n            for i in range(self.num_eval_batches)\n        ]\n\n    # public API\n    def __iter__(self) -&gt; Iterator[tuple[jax.Array, jax.Array]]:\n        for lo, hi in self._train_bounds:\n            yield self.x[lo:hi], self.y[lo:hi]\n\n    def iter_eval(self) -&gt; Iterator[tuple[jax.Array, jax.Array]]:\n        for lo, hi in self._eval_bounds:\n            yield self.x_eval[lo:hi], self.y_eval[lo:hi]\n\n    def __len__(self) -&gt; int:\n        return self.num_batches\n\n    # internals\n    @staticmethod\n    def _load(split: str) -&gt; tuple[jax.Array, jax.Array]:\n        ds = load_dataset(\"mnist\")\n        x = jnp.asarray([jnp.array(im) for im in ds[split][\"image\"]], dtype=jnp.float32)\n        y = jnp.asarray(ds[split][\"label\"], dtype=jnp.int32)\n        x = x.reshape(x.shape[0], -1) / 255.0\n        return x, y\n\n    @staticmethod\n    def _labels_to_pm1_scaled(y_scalar: jax.Array, C: int) -&gt; jax.Array:\n        one_hot = jax.nn.one_hot(y_scalar, C, dtype=jnp.float32)\n        return one_hot * (C**0.5 / 2.0) - 0.5\n\n    @staticmethod\n    def _rand_proj(key: jax.Array, out_dim: int, in_dim: int) -&gt; jax.Array:\n        return jax.random.normal(key, (out_dim, in_dim), dtype=jnp.float32) / jnp.sqrt(in_dim)\n\n    @staticmethod\n    def _take_per_class(key: jax.Array, x: jax.Array, y: jax.Array, k: int):\n        xs, ys = [], []\n        for cls in range(MNISTData.NUM_CLASSES):\n            key, sub = jax.random.split(key)\n            idx = jnp.where(y == cls)[0]\n            perm = jax.random.permutation(sub, idx.shape[0])\n            take = idx[perm[:k]]\n            xs.append(x[take])\n            ys.append(y[take])\n        return jnp.concatenate(xs, 0), jnp.concatenate(ys, 0)\n\n    def _maybe_proj_sign(self, w: jax.Array | None, x: jax.Array) -&gt; jax.Array:\n        if w is not None:\n            x = (x @ w.T).astype(jnp.float32)\n        if self.apply_sign_transform:\n            sgn = jnp.sign(x)\n            x = jnp.where(sgn == 0, jnp.array(-1.0, dtype=sgn.dtype), sgn)\n        return x\n\n    def _build(self, key: jax.Array) -&gt; None:\n        key_sample, key_proj, key_shuffle = jax.random.split(key, 3)\n        x_tr_all, y_tr_all = self._load(\"train\")\n        x_ev_all, y_ev_scalar = self._load(\"test\")\n        k_train = self.num_data // self.NUM_CLASSES\n        x_tr, y_tr_scalar = self._take_per_class(key_sample, x_tr_all, y_tr_all, k_train)\n        w = (\n            self._rand_proj(key_proj, int(self.linear_projection), x_tr.shape[-1])\n            if self.linear_projection\n            else None\n        )\n        x_tr = self._maybe_proj_sign(w, x_tr)\n        x_ev = self._maybe_proj_sign(w, x_ev_all)\n        y_tr = self._labels_to_pm1_scaled(y_tr_scalar, self.NUM_CLASSES)\n        perm_tr = jax.random.permutation(key_shuffle, x_tr.shape[0])\n        self.x, self.y = x_tr[perm_tr], y_tr[perm_tr]\n        self.x_eval = x_ev\n        self.y_eval = self._labels_to_pm1_scaled(y_ev_scalar, self.NUM_CLASSES)\n        self.input_dim = int(self.x.shape[1])\n</code></pre>"},{"location":"tutorials/08_trainer_on_mnist/#model-topology-layermap-orchestrator","title":"Model topology (LayerMap + Orchestrator)","text":"<p>One hidden layer: - 0\u21921 forward (FullyConnected) - 1\u21921 recurrent (RecurrentDiscrete) - 2\u21921 teacher/clamp (FrozenFullyConnected) - 1\u21922 readout (FullyConnected) - 2\u21922 aggregator (OutputLayer)</p> <pre><code>DIM_DATA = 100\nNUM_LABELS = MNISTData.NUM_CLASSES\nDIM_HIDDEN = 300\n\nTHRESHOLD_OUT = 1.0\nTHRESHOLD_IN = 1.0\nTHRESHOLD_J = 1.0\nSTRENGTH_BACK = 0.5\nSTRENGTH_FORTH = 5.0\nJ_D = 0.5\n\nstate = SequentialState((DIM_DATA, DIM_HIDDEN, NUM_LABELS))\n\nmaster_key = jax.random.key(44)\nkeys = jax.random.split(master_key, 5)\n\nlayer_map = {\n    1: {\n        0: FullyConnected(\n            in_features=DIM_DATA,\n            out_features=DIM_HIDDEN,\n            strength=STRENGTH_FORTH,\n            threshold=THRESHOLD_IN,\n            key=keys[0],\n        ),\n        1: RecurrentDiscrete(\n            features=DIM_HIDDEN,\n            j_d=J_D,\n            threshold=THRESHOLD_J,\n            key=keys[1],\n        ),\n        2: FrozenFullyConnected(\n            in_features=NUM_LABELS,\n            out_features=DIM_HIDDEN,\n            strength=STRENGTH_BACK,\n            threshold=0.0,\n            key=keys[2],\n        ),\n    },\n    2: {\n        1: FullyConnected(\n            in_features=DIM_HIDDEN,\n            out_features=NUM_LABELS,\n            strength=1.0,\n            threshold=THRESHOLD_OUT,\n            key=keys[3],\n        ),\n        2: OutputLayer(),\n    },\n}\nlayer_map = LayerMap.from_dict(layer_map)\norchestrator = SequentialOrchestrator(layers=layer_map)\nlogger.info(\"Model initialized with SequentialOrchestrator.\")\n</code></pre>"},{"location":"tutorials/08_trainer_on_mnist/#optimizer-dynamicaltrainer","title":"Optimizer &amp; <code>DynamicalTrainer</code>","text":"<p>The trainer owns Optax state and RNG and exposes <code>fit_epoch</code> / <code>eval_epoch</code>.</p> <pre><code># Experiment knobs\nNUM_IMAGES_PER_CLASS = 5400\nAPPLY_SIGN_TRANSFORM = True\nBATCH_SIZE = 16\nEPOCHS = 5\nT_WARMUP = 1  # warmup phase\nT_FREE = 7  # free phase iterations\nT_CLAMPED = 7  # clamped phase iterations\nT_EVAL = 14  # validation phase\n\n# RNGs\nrun_key = jax.random.key(59)\nrun_key, data_key = jax.random.split(run_key)\n\n# Data\ndata = MNISTData(\n    key=data_key,\n    batch_size=BATCH_SIZE,\n    linear_projection=DIM_DATA,\n    apply_sign_transform=APPLY_SIGN_TRANSFORM,\n    num_images_per_class=NUM_IMAGES_PER_CLASS,\n)\nprint(f\"Dataset \u2014 x.shape={tuple(data.x.shape)}, y.shape={tuple(data.y.shape)}\")\n\n# Optimizer\noptimizer = optax.adam(2e-3)\noptimizer_state = optimizer.init(eqx.filter(orchestrator, eqx.is_inexact_array))\n\n# Trainer\ntrainer = DynamicalTrainer(\n    orchestrator=orchestrator,\n    state=state,\n    optimizer=optimizer,\n    optimizer_state=optimizer_state,\n    warmup_n_iter=T_WARMUP,\n    train_clamped_n_iter=T_CLAMPED,\n    train_free_n_iter=T_FREE,\n    eval_n_iter=T_EVAL,\n)\n</code></pre>"},{"location":"tutorials/08_trainer_on_mnist/#fit-compact-epoch-loop","title":"Fit (compact epoch loop)","text":"<pre><code>logs = []\nrun_key, rng = jax.random.split(run_key)\nlogger.info(\"Starting training...\")\nfor epoch in range(1, EPOCHS + 1):\n    logger.info(f\"{epoch=}\")\n    for x, y in data:\n        rng = trainer.train_step(x, y, rng)\n    eval_accuracies = []\n    for x, y in data.iter_eval():\n        rng, metrics = trainer.eval_step(x, y, rng)\n        eval_accuracies.append(metrics[\"accuracy\"])\n    print(f\"Eval accuracy: {sum(eval_accuracies) / len(eval_accuracies)}\")\n</code></pre>"},{"location":"tutorials/08_trainer_on_mnist/#final-evaluation","title":"Final evaluation","text":"<pre><code>print(\"\\n=== Final evaluation ===\")\nfor x, y in data.iter_eval():\n    rng, metrics = trainer.eval_step(x, y, rng)\n    eval_accuracies.append(metrics[\"accuracy\"])\nprint(f\"Eval accuracy: {sum(eval_accuracies) / len(eval_accuracies)}\")\n</code></pre>"},{"location":"tutorials/08_trainer_on_mnist/#recap","title":"Recap","text":"<p>You kept the LayerMap/State/Orchestrator design and local-plasticity updates, but moved outer-loop mechanics into a thin, reusable <code>DynamicalTrainer</code>: a stable API (<code>train_step</code>, <code>eval_step</code>).</p>"},{"location":"tutorials/09_sota_on_mnist/","title":"Sparsity, learning rate maps and weight decay","text":"<p>This notebook-as-a-script trains a sparse recurrent model with darnax on MNIST.</p> <p>We show how to: - build a sparse recurrent architecture, - train it with SGD using different learning rates per module, - apply custom weight decay on sparse weight matrices. - train a separate torch classifier on the final representations</p> <pre><code>import copy\nfrom collections.abc import Mapping\nfrom copy import deepcopy\nfrom typing import Any\n\nimport equinox as eqx\nimport jax\nimport jax.numpy as jnp\nimport jax.tree_util as jtu\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport optax\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\n\nfrom darnax.datasets.classification.mnist import Mnist\nfrom darnax.layer_maps.sparse import LayerMap\nfrom darnax.modules.fully_connected import (\n    FrozenRescaledFullyConnected,\n    FullyConnected,\n    SparseFullyConnected,\n)\nfrom darnax.modules.input_output import OutputLayer\nfrom darnax.modules.recurrent import SparseRecurrentDiscrete\nfrom darnax.orchestrators.sequential import SequentialOrchestrator\nfrom darnax.states.sequential import SequentialState\nfrom darnax.trainers.dynamical import DynamicalTrainer\nfrom darnax.utils.typing import PyTree\n</code></pre>"},{"location":"tutorials/09_sota_on_mnist/#2-experiment-configuration-params","title":"2. Experiment configuration (<code>PARAMS</code>)","text":"<p>We provide config in a simple Python dictionary.</p> <pre><code>PARAMS: dict[str, Any] = {\n    \"master_seed\": 0,\n    \"epochs\": 20,\n    \"model\": {\n        \"kwargs\": {\n            \"seed\": 44,\n            \"dim_data\": 784,  # input dimension\n            \"dim_hidden\": 2000,  # hidden size\n            \"num_labels\": 10,  # number of classes\n            \"sparsity\": 0.99,  # recurrent sparsity\n            \"sparsity_win\": 0.9,  # input sparsity\n            \"strength_forth\": 5.0,\n            \"strength_back\": 1.3,\n            \"j_d\": 0.95,  # self-coupling in recurrent J\n            \"threshold_in\": 1.78,\n            \"threshold_out\": 7.0,\n            \"threshold_j\": 1.78,\n            \"threshold_back\": 0.0,\n        }\n    },\n    \"data\": {\n        \"kwargs\": {\n            \"batch_size\": 16,\n            \"linear_projection\": None,\n            \"num_images_per_class\": None,\n            \"label_mode\": \"pm1\",\n            \"x_transform\": \"identity\",\n            \"flatten\": True,\n        },\n    },\n    \"optimizer\": {\n        \"learning_rate_win\": 0.159,  # input layer\n        \"learning_rate_j\": 0.058,  # recurrent J\n        \"learning_rate_wout\": 0.17,  # output layer\n        \"weight_decay_win\": 0.01,\n        \"weight_decay_j\": 0.00006,\n        \"weight_decay_wout\": 0.02,\n    },\n    \"trainer\": {\n        \"kwargs\": {\n            \"warmup_n_iter\": 1,\n            \"train_clamped_n_iter\": 5,\n            \"train_free_n_iter\": 5,\n            \"eval_n_iter\": 5,\n        },\n    },\n    \"torch_clf\": {\n        \"enabled\": True,\n        \"use_bias\": False,\n        \"optimizer\": \"adam\",\n        \"lr\": 1e-3,\n        \"weight_decay\": 0.0,\n        \"batch_size\": 256,\n        \"epochs\": 20,\n    },\n}\n</code></pre>"},{"location":"tutorials/09_sota_on_mnist/#3-building-the-sparse-recurrent-model","title":"3. Building the sparse recurrent model","text":"<p>We construct a <code>SequentialOrchestrator</code> with: - sparse input adapter (<code>SparseFullyConnected</code>), - sparse recurrent core (<code>SparseRecurrentDiscrete</code>), - dense, fixed, feedback - dense output layer</p> <pre><code>def build_model(\n    seed: int,\n    dim_data: int,\n    dim_hidden: int,\n    sparsity: float,\n    sparsity_win: float,\n    num_labels: int,\n    strength_forth: float,\n    strength_back: float,\n    threshold_in: float,\n    threshold_out: float,\n    threshold_back: float,\n    threshold_j: float,\n    j_d: float,\n) -&gt; tuple[SequentialState, SequentialOrchestrator]:\n    \"\"\"Build a sparse recurrent model with input, recurrent, output modules.\n\n    The model has:\n    - a sparse input adapter ``SparseFullyConnected``,\n    - a sparse recurrent core ``SparseRecurrentDiscrete``,\n    - a feedback adapter\n    - an output adapter followed by ``OutputLayer``.\n\n    Parameters\n    ----------\n    seed:\n        PRNG seed for JAX initializations.\n    dim_data:\n        Input dimension.\n    dim_hidden:\n        Size of the recurrent layer.\n    sparsity:\n        Fraction of zero entries in the recurrent J matrix.\n    sparsity_win:\n        Fraction of zero entries in the input matrix W_in.\n    num_labels:\n        Number of output classes.\n    strength_forth:\n        Scaling factor for forward (input \u2192 hidden) couplings.\n    strength_back:\n        Scaling factor for feedback (output \u2192 hidden) couplings.\n    threshold_in:\n        Threshold for the input adapter dynamics.\n    threshold_out:\n        Threshold for the output adapter dynamics.\n    threshold_back:\n        Threshold for the feedback adapter dynamics.\n    threshold_j:\n        Threshold for the recurrent core.\n    j_d:\n        Diagonal self\u2013coupling strength for the recurrent core.\n\n    Returns\n    -------\n    state:\n        Initial ``SequentialState`` with appropriate shapes.\n    orchestrator:\n        ``SequentialOrchestrator`` wiring all modules together.\n\n    \"\"\"\n    state = SequentialState((dim_data, dim_hidden, num_labels))\n\n    master_key = jax.random.key(seed)\n    keys = jax.random.split(master_key, num=5)\n\n    layer_map = {\n        1: {\n            0: SparseFullyConnected(\n                in_features=dim_data,\n                out_features=dim_hidden,\n                strength=strength_forth,\n                threshold=threshold_in,\n                sparsity=sparsity_win,\n                key=keys[0],\n            ),\n            1: SparseRecurrentDiscrete(\n                features=dim_hidden,\n                j_d=j_d,\n                sparsity=sparsity,\n                threshold=threshold_j,\n                key=keys[1],\n            ),\n            2: FrozenRescaledFullyConnected(\n                in_features=num_labels,\n                out_features=dim_hidden,\n                strength=strength_back,\n                threshold=threshold_back,\n                key=keys[2],\n            ),\n        },\n        2: {\n            1: FullyConnected(\n                in_features=dim_hidden,\n                out_features=num_labels,\n                strength=1.0,\n                threshold=threshold_out,\n                key=keys[3],\n            ),\n            2: OutputLayer(),\n        },\n    }\n\n    layer_map = LayerMap.from_dict(layer_map)\n    orchestrator = SequentialOrchestrator(layers=layer_map)\n\n    return state, orchestrator\n</code></pre>"},{"location":"tutorials/09_sota_on_mnist/#4-parameter-labels-for-optaxmulti_transform","title":"4. Parameter labels for <code>optax.multi_transform</code>","text":"<p>We want different learning rates for different submodules. This helper builds a tree of labels matching the parameter PyTree.</p> <pre><code>def make_lr_map_v2(\n    model: SequentialOrchestrator,\n    overrides: Mapping[tuple[int, int], str] | None = None,\n    default_label: str = \"default\",\n) -&gt; PyTree:\n    \"\"\"Build a PyTree of parameter labels for ``optax.multi_transform``.\n\n    Parameters\n    ----------\n    model:\n        Orchestrator model (must have a ``.lmap`` field).\n    overrides:\n        Optional mapping from ``(layer_idx, position_idx)`` to a label string.\n        Example: ``{(1, 0): \"w_in\", (1, 1): \"j\", (2, 1): \"w_out\"}``.\n    default_label:\n        Label assigned to all parameters not specified in ``overrides``.\n\n    Returns\n    -------\n    labels:\n        PyTree matching the parameter structure, each leaf a string label.\n\n    \"\"\"\n    params, _ = eqx.partition(model, eqx.is_inexact_array)\n\n    def like(tree, value: str):\n        \"\"\"Broadcast a scalar label to a tree with the same structure.\"\"\"\n        return jtu.tree_map(lambda _: value, tree, is_leaf=eqx.is_array)\n\n    labels = jtu.tree_map(lambda _: default_label, params, is_leaf=eqx.is_array)\n\n    if overrides:\n        for (i, j), label in overrides.items():\n            labels = eqx.tree_at(\n                lambda m: m.lmap[i][j],\n                labels,\n                replace=like(params.lmap[i][j], label),\n            )\n\n    return labels\n</code></pre>"},{"location":"tutorials/09_sota_on_mnist/#5-custom-sparse-aware-weight-decay","title":"5. Custom sparse-aware weight decay","text":"<p>We apply exponential decay to: - input weights <code>W_in</code>, - recurrent weights <code>J</code> (respecting the sparsity mask), - output weights <code>W_out</code>.</p> <p>Decay factors are rescaled by learning rates and layer sizes.</p> <pre><code>def decay(\n    orchestrator: SequentialOrchestrator,\n    config: dict[str, Any],\n) -&gt; SequentialOrchestrator:\n    \"\"\"Apply exponential weight decay to input, recurrent, and output matrices.\n\n    The decay factors are rescaled by the corresponding learning rates and by\n    the layer dimensions, to match the effective SGD step size.\n\n    Parameters\n    ----------\n    orchestrator:\n        Current model orchestrator.\n    config:\n        Parameter dictionary (expects ``\"optimizer\"`` and\n        ``\"model\"][\"kwargs\"]`` keys).\n\n    Returns\n    -------\n    new_orch:\n        New orchestrator with decayed weights.\n\n    \"\"\"\n    new_orch = orchestrator\n\n    dim_data = config[\"model\"][\"kwargs\"][\"dim_data\"]\n    dim_hidden = config[\"model\"][\"kwargs\"][\"dim_hidden\"]\n\n    # Input weights W_in\n    W_in = jnp.asarray(new_orch.lmap[1][0].W)\n    rescaling_win = (\n        config[\"optimizer\"][\"weight_decay_win\"]\n        / (1 - config[\"model\"][\"kwargs\"][\"sparsity_win\"]) ** 0.5\n        * (\n            (1 - 0.90) ** 0.5\n        )  # the optimal hyperparamaters were found before this rescaling was introduced\n        * config[\"optimizer\"][\"learning_rate_win\"]\n        / (dim_data**0.5)\n    )\n    W_in_new = W_in * (1.0 - rescaling_win)\n    new_orch = eqx.tree_at(lambda o: o.lmap[1][0].W, new_orch, W_in_new)\n\n    # Recurrent J (masked: we do not decay excluded entries)\n    J = jnp.asarray(new_orch.lmap[1][1].J)\n    mask = jnp.asarray(new_orch.lmap[1][1]._mask)\n    rescaling_j = (\n        config[\"optimizer\"][\"weight_decay_j\"]\n        * config[\"optimizer\"][\"learning_rate_j\"]\n        / (1 - config[\"model\"][\"kwargs\"][\"sparsity\"]) ** 0.5\n        * (\n            (1 - 0.90) ** 0.5\n        )  # the optimal hyperparamaters were found before this rescaling was introduced\n        / (dim_hidden**0.5)\n    )\n    J_new = J * (1.0 - rescaling_j * mask)\n    new_orch = eqx.tree_at(lambda o: o.lmap[1][1].J, new_orch, J_new)\n\n    # Output weights W_out\n    W_out = jnp.asarray(new_orch.lmap[2][1].W)\n    rescaling_wout = (\n        config[\"optimizer\"][\"weight_decay_wout\"]\n        * config[\"optimizer\"][\"learning_rate_wout\"]\n        / (dim_hidden**0.5)\n    )\n    W_out_new = W_out * (1.0 - rescaling_wout)\n    new_orch = eqx.tree_at(lambda o: o.lmap[2][1].W, new_orch, W_out_new)\n\n    return new_orch\n</code></pre>"},{"location":"tutorials/09_sota_on_mnist/#6-training-loop-jax-model","title":"6. Training loop (JAX model)","text":"<p>We now define the main training function:</p> <ul> <li>build model, dataset, trainer,</li> <li>create a multi-transform optimizer with module-wise learning rates,</li> <li>rescale learning rates by sparsity,</li> <li>train for a fixed number of epochs,</li> <li>log only train and eval accuracy (printed + returned).</li> </ul> <pre><code>def train_once(params: dict[str, Any]) -&gt; tuple[dict[str, list[float]], Any, Any, jax.Array]:\n    \"\"\"Train the sparse recurrent model for a fixed number of epochs.\n\n    Parameters\n    ----------\n    params:\n        Experiment configuration dictionary (``PARAMS``).\n\n    Returns\n    -------\n    history:\n        Dictionary with lists of training and evaluation accuracies.\n    trainer:\n        Trained trainer object.\n    ds:\n        Dataset object used for training and evaluation.\n    key:\n        Final JAX PRNG key after training.\n\n    \"\"\"\n    cfg = deepcopy(params)\n    key = jax.random.key(cfg.get(\"master_seed\", 0))\n\n    # Build model and dataset\n    state, orchestrator = build_model(**cfg[\"model\"][\"kwargs\"])\n\n    ds = Mnist(**cfg[\"data\"][\"kwargs\"])\n    key, data_key = jax.random.split(key)\n    ds.build(data_key)\n\n    # Learning-rate map for optax.multi_transform\n    lr_map = make_lr_map_v2(\n        orchestrator,\n        overrides={(1, 0): \"w_in\", (1, 1): \"j\", (2, 1): \"w_out\"},\n    )\n\n    lr_win = cfg[\"optimizer\"][\"learning_rate_win\"]\n    lr_wout = cfg[\"optimizer\"][\"learning_rate_wout\"]\n    lr_j = cfg[\"optimizer\"][\"learning_rate_j\"]\n\n    # Rescale learning rates to account for sparsity\n    sparsity = cfg[\"model\"][\"kwargs\"][\"sparsity\"]\n    sparsity_win = cfg[\"model\"][\"kwargs\"][\"sparsity_win\"]\n    lr_j /= jnp.sqrt(1.0 - sparsity)\n    lr_win /= jnp.sqrt(1.0 - sparsity_win)\n\n    optimizer = optax.multi_transform(\n        {\n            \"default\": optax.sgd(learning_rate=0.0),\n            \"w_in\": optax.sgd(learning_rate=lr_win),\n            \"w_out\": optax.sgd(learning_rate=lr_wout),\n            \"j\": optax.sgd(learning_rate=lr_j),\n        },\n        lr_map,\n    )\n    opt_state = optimizer.init(eqx.filter(orchestrator, eqx.is_inexact_array))\n\n    trainer = DynamicalTrainer(\n        orchestrator=orchestrator,\n        state=state,\n        optimizer=optimizer,\n        optimizer_state=opt_state,\n        **cfg[\"trainer\"][\"kwargs\"],\n    )\n\n    history: dict[str, list[float]] = {\n        \"train_acc\": [],\n        \"eval_acc\": [],\n    }\n\n    num_epochs = int(cfg[\"epochs\"])\n\n    for epoch in range(0, num_epochs + 1):\n        # ---- Train ----\n        if epoch != 0:\n            for xb, yb in ds:\n                key = trainer.train_step(xb, yb, key)\n                trainer.orchestrator = decay(trainer.orchestrator, cfg)\n\n        # ---- Eval on test split ----\n        accs_eval = []\n        for xb, yb in ds.iter_test():\n            key, metrics = trainer.eval_step(xb, yb, key)\n            accs_eval.append(metrics[\"accuracy\"])\n\n        acc_eval = float(jnp.mean(jnp.array(accs_eval))) if accs_eval else float(\"nan\")\n\n        # ---- Eval on train split ----\n        accs_train = []\n        for xb, yb in ds:\n            key, metrics = trainer.eval_step(xb, yb, key)\n            accs_train.append(metrics[\"accuracy\"])\n        acc_train = float(jnp.mean(jnp.array(accs_train))) if accs_train else float(\"nan\")\n\n        history[\"train_acc\"].append(acc_train)\n        history[\"eval_acc\"].append(acc_eval)\n\n        print(f\"Epoch {epoch:03d} | train_acc={acc_train:.4f} | eval_acc={acc_eval:.4f}\")\n\n    return history, trainer, ds, key\n</code></pre>"},{"location":"tutorials/09_sota_on_mnist/#7-optional-pytorch-linear-classifier-on-learned-representations","title":"7. Optional: PyTorch linear classifier on learned representations","text":"<p>We keep the option to train a simple linear probe on frozen JAX representations. This is useful for evaluating representation quality.</p> <pre><code>def train_torch_linear_probe(\n    trainer: Any,\n    ds: Any,\n    cfg: dict[str, Any],\n    key: jax.Array,\n) -&gt; None:\n    \"\"\"Train a linear classifier on frozen JAX representations using PyTorch.\n\n    Parameters\n    ----------\n    trainer:\n        Trained darnax trainer (provides ``.state.representations``).\n    ds:\n        Dataset object with train and test iterators.\n    cfg:\n        Experiment config with ``\"model\"`` and ``\"torch_clf\"`` sections.\n    key:\n        JAX PRNG key (updated during evaluation passes).\n\n    \"\"\"\n    if \"torch_clf\" not in cfg or not cfg[\"torch_clf\"].get(\"enabled\", False):\n        print(\"Torch linear probe is disabled in PARAMS['torch_clf'].\")\n        return\n\n    torch_clf_cfg = cfg[\"torch_clf\"]\n    print(\"Starting PyTorch linear classifier training...\")\n\n    if \"master_seed\" in cfg:\n        torch.manual_seed(int(cfg[\"master_seed\"]))\n\n    # Feature extraction: train split\n    train_reps = []\n    train_labels = []\n    for xb, yb in ds:\n        key, _ = trainer.eval_step(xb, yb, key)\n        reps = trainer.state[-2]\n        train_reps.append(copy.deepcopy(np.array(reps)))\n        yb_np = np.array(copy.deepcopy(yb))\n        yb_np = np.argmax(yb_np, axis=-1)\n        train_labels.append(copy.deepcopy(yb_np))\n\n    # Feature extraction: test split\n    test_reps = []\n    test_labels = []\n    for xb, yb in ds.iter_test():\n        key, _ = trainer.eval_step(xb, yb, key)\n        reps = trainer.state[-2]\n        test_reps.append(copy.deepcopy(np.array(reps)))\n        yb_np = np.array(copy.deepcopy(yb))\n        yb_np = np.argmax(yb_np, axis=-1)\n        test_labels.append(copy.deepcopy(yb_np))\n\n    features_train = torch.from_numpy(np.concatenate(train_reps, axis=0)).float()\n    labels_train = torch.from_numpy(np.concatenate(train_labels, axis=0)).long()\n    features_test = torch.from_numpy(np.concatenate(test_reps, axis=0)).float()\n    labels_test = torch.from_numpy(np.concatenate(test_labels, axis=0)).long()\n\n    input_dim = int(features_train.shape[1])\n    num_classes = int(cfg[\"model\"][\"kwargs\"][\"num_labels\"])\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = nn.Linear(input_dim, num_classes, bias=torch_clf_cfg.get(\"use_bias\", False)).to(device)\n    criterion = nn.CrossEntropyLoss()\n    opt_name = torch_clf_cfg.get(\"optimizer\", \"adam\").lower()\n    opt_class = {\"adam\": torch.optim.Adam, \"sgd\": torch.optim.SGD}[opt_name]\n    optimizer = opt_class(\n        model.parameters(),\n        lr=float(torch_clf_cfg[\"lr\"]),\n        weight_decay=float(torch_clf_cfg[\"weight_decay\"]),\n    )\n\n    batch_size = int(torch_clf_cfg[\"batch_size\"])\n    train_loader = DataLoader(\n        TensorDataset(features_train, labels_train),\n        batch_size=batch_size,\n        shuffle=True,\n    )\n\n    epochs_clf = int(torch_clf_cfg[\"epochs\"])\n\n    for e in range(epochs_clf):\n        model.train()\n        total_loss = 0.0\n        correct = 0\n        total = 0\n        for xb_t, yb_t in train_loader:\n            xb_t = xb_t.to(device)\n            yb_t = yb_t.to(device)\n            optimizer.zero_grad()\n            logits = model(xb_t)\n            loss = criterion(logits, yb_t)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item() * yb_t.size(0)\n            pred = logits.argmax(dim=1)\n            correct += (pred == yb_t).sum().item()\n            total += yb_t.size(0)\n\n        train_loss = total_loss / max(1, total)\n        train_acc = correct / max(1, total)\n\n        model.eval()\n        with torch.no_grad():\n            logits_eval = model(features_test.to(device))\n            eval_loss = criterion(logits_eval, labels_test.to(device)).item()\n            pred_eval = logits_eval.argmax(dim=1)\n            eval_acc = (pred_eval == labels_test.to(device)).float().mean().item()\n\n        print(\n            f\"[Torch Clf] Epoch {e:03d} | \"\n            f\"train_acc={train_acc:.4f} | eval_acc={eval_acc:.4f} | \"\n            f\"train_loss={train_loss:.4f} | eval_loss={eval_loss:.4f}\"\n        )\n\n    print(\"PyTorch linear classifier training complete.\")\n</code></pre>"},{"location":"tutorials/09_sota_on_mnist/#8-run-training-and-plot-accuracies","title":"8. Run training and plot accuracies","text":"<p>You can execute this block to train the model and visualize train vs. eval accuracy over epochs.</p> <pre><code>if __name__ == \"__main__\":\n    history, trainer, ds, key = train_once(PARAMS)\n\n    plt.figure(figsize=(6, 4))\n    plt.plot(history[\"train_acc\"], label=\"train\")\n    plt.plot(history[\"eval_acc\"], label=\"eval\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.title(\"Sparse recurrent model: train vs eval accuracy\")\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n\n    if PARAMS[\"torch_clf\"][\"enabled\"]:\n        train_torch_linear_probe(trainer, ds, PARAMS, key)\n</code></pre>"}]}