{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"darnax","text":"<p>Deep Asymmetric Recurrent Networks in JAX.</p> <p>darnax is a research library for building and experimenting with asymmetric recurrent neural networks and their learning dynamics. It is inspired by recent work by Badalotti, Baldassi, M\u00e9zard, Scardecchia and Zecchina, showing that simple, distributed plasticity rules can give rise to powerful learning behaviors without relying on backpropagation.</p> <p>The library provides:</p> <ul> <li>Composable modules based on Equinox, with clean support for sparse and structured connectivity. They can be easily extended to accomodate for complex network structures (convolutional, transformers-like, etc...). They also naturally support layers of varying shapes and connectivity.</li> <li>Orchestrators for running recurrent dynamics, either sequentially or in parallel.</li> <li>Local update rules implementing gradient-free plasticity mechanisms.</li> <li>Jax speed and transparency. Everything is a pytree, whether you are building a simple 1 hidden-layer model or a complex interconnected structure, the training logic remains the same.</li> <li>Natural integration with optax. Despite not relying on explicit gradients, the models can be naturally optimized with Optax.</li> </ul> <p>darnax is not a framework chasing SOTA benchmarks. It is a sandbox for exploring recurrent dynamics as a computational primitive \u2014 bridging machine learning, theoretical neuroscience, and statistical physics. This is also a work-in-progress, and contributions are more than welcome!</p> <p>\ud83d\udc49 Check the Tutorials to get started, or browse the API Reference for details.</p>"},{"location":"reference/","title":"Reference","text":"<p>Auto-generated API lives here.</p> <ul> <li>\ud83d\udc49 API Index</li> </ul>"},{"location":"reference/api/","title":"API Index","text":"<p>Auto-generated. Top-level packages mirror the directory layout.</p>"},{"location":"reference/api/#packages","title":"Packages","text":"<ul> <li>darnax.layer_maps</li> <li>darnax.modules</li> <li>darnax.orchestrators</li> <li>darnax.states</li> <li>darnax.utils</li> </ul>"},{"location":"reference/api/darnax/","title":"darnax","text":""},{"location":"reference/api/darnax/#subpackages","title":"Subpackages","text":"<ul> <li>darnax.layer_maps</li> <li>darnax.modules</li> <li>darnax.orchestrators</li> <li>darnax.states</li> <li>darnax.utils</li> </ul>"},{"location":"reference/api/darnax/layer_maps/","title":"darnax.layer_maps","text":""},{"location":"reference/api/darnax/layer_maps/#modules","title":"Modules","text":"<ul> <li>darnax.layer_maps.sparse</li> </ul>"},{"location":"reference/api/darnax/modules/","title":"darnax.modules","text":""},{"location":"reference/api/darnax/modules/#modules","title":"Modules","text":"<ul> <li>darnax.modules.debug</li> <li>darnax.modules.ferromagnetic</li> <li>darnax.modules.fully_connected</li> <li>darnax.modules.input_output</li> <li>darnax.modules.interfaces</li> <li>darnax.modules.recurrent</li> </ul>"},{"location":"reference/api/darnax/orchestrators/","title":"darnax.orchestrators","text":""},{"location":"reference/api/darnax/orchestrators/#modules","title":"Modules","text":"<ul> <li>darnax.orchestrators.interface</li> <li>darnax.orchestrators.sequential</li> </ul>"},{"location":"reference/api/darnax/states/","title":"darnax.states","text":""},{"location":"reference/api/darnax/states/#modules","title":"Modules","text":"<ul> <li>darnax.states.interface</li> <li>darnax.states.sequential</li> </ul>"},{"location":"reference/api/darnax/utils/","title":"darnax.utils","text":""},{"location":"reference/api/darnax/utils/#modules","title":"Modules","text":"<ul> <li>darnax.utils.default_list</li> <li>darnax.utils.perceptron_rule</li> <li>darnax.utils.typing</li> </ul>"},{"location":"reference/api/darnax/layer_maps/sparse/","title":"darnax.layer_maps.sparse","text":"<p>LayerMap: a static, PyTree-friendly adjacency of modules.</p> <p><code>LayerMap</code> wraps a nested dict-of-dicts that maps receiver rows <code>i</code> to neighbors (columns) <code>j</code> \u2192 module, i.e. it represents edges <code>(i, j)</code>. The structure (row/column keys and their order) is static for JIT stability, while the values (modules and their parameters) are dynamic PyTree leaves.</p> Design goals <ul> <li>Keep a clear nested dict API while making the structure part of the treedef.</li> <li>Flatten through Equinox/JAX modules so inner arrays are visible to JAX/Optax.</li> <li>Forbid structural mutation after construction (frozen dataclass, read-only views).</li> </ul> Conventions <ul> <li>Rows and, within each row, columns are sorted once at construction.</li> <li>Keys are integers (layer indices). Edge <code>(i, j)</code> connects sender/neighbor   <code>j</code> into receiver <code>i</code> (lower-triangular including the diagonal is common).</li> <li>The diagonal policy can be enforced: every row must have its <code>(i, i)</code> self-edge.</li> </ul> PyTree behavior <p><code>tree_flatten</code> returns all modules in deterministic row-major order as children, plus static aux data describing the key layout. JAX/Equinox then flattens module parameters further, so optimizers and transforms \"see\" the arrays inside.</p>"},{"location":"reference/api/darnax/layer_maps/sparse/#darnax.layer_maps.sparse.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/layer_maps/sparse/#darnax.layer_maps.sparse.LayerMap","title":"<code>LayerMap(_data, _rows, _ndim=2)</code>  <code>dataclass</code>","text":"<p>PyTree wrapper around a dict-of-dicts with static keys and non-static values.</p> <p>Parameters:</p> Name Type Description Default <code>_data</code> <code>dict[int, dict[int, AbstractModule]]</code> <p>Internal mapping from row <code>i</code> \u2192 (col <code>j</code> \u2192 module). Keys are sorted.</p> required <code>_rows</code> <code>tuple[int, ...]</code> <p>All row keys in sorted order (becomes part of the treedef).</p> required <code>_ndim</code> <code>int</code> <p>Tuple key arity (<code>(i, j)</code>). Exposed mainly for reconstruction.</p> <code>2</code> Notes <ul> <li>The dataclass is frozen to prevent structural mutation after creation.   Use :meth:<code>to_dict</code> to obtain a mutable deep copy if you truly need one.</li> <li>Read-only accessors (e.g., :meth:<code>neighbors</code>) return <code>MappingProxyType</code>.</li> <li>The keys (rows/cols) are included in the PyTree aux data, so the layout   is static under JIT; values (modules) are the dynamic leaves.</li> </ul> Public type <p>Values are typed as :class:<code>~darnax.modules.interfaces.AbstractModule</code> so any concrete layer/adapter subtype can be stored.</p>"},{"location":"reference/api/darnax/layer_maps/sparse/#darnax.layer_maps.sparse.LayerMap.from_dict","title":"<code>from_dict(mapping, *, require_diagonal=True)</code>  <code>staticmethod</code>","text":"<p>Construct a LayerMap from a nested mapping.</p> <p>Parameters:</p> Name Type Description Default <code>mapping</code> <code>Mapping[int, Mapping[int, AbstractModule]]</code> <p>Nested mapping from row <code>i</code> \u2192 (col <code>j</code> \u2192 module).</p> required <code>require_diagonal</code> <code>bool</code> <p>If <code>True</code>, enforce that each present row <code>i</code> has an explicit self-edge <code>(i, i)</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>LayerMap</code> <p>A new instance with rows and per-row columns sorted deterministically.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If <code>require_diagonal=True</code> and some <code>(i, i)</code> is missing.</p>"},{"location":"reference/api/darnax/layer_maps/sparse/#darnax.layer_maps.sparse.LayerMap.tree_flatten","title":"<code>tree_flatten()</code>","text":"<p>Deconstruct into children and aux (PyTree protocol).</p> <p>Returns:</p> Name Type Description <code>children</code> <code>tuple[AbstractModule, ...]</code> <p>Modules in deterministic row-major order; JAX/Equinox will flatten their parameter fields further.</p> <code>aux</code> <code>tuple[Any, ...]</code> <p>Static metadata: <code>(rows, cols_per_row, ndim)</code> to reconstruct the treedef.</p> Notes <p>We intentionally do not include keys as children; keys are part of the static aux data so JIT sees a stable structure even when values change.</p>"},{"location":"reference/api/darnax/layer_maps/sparse/#darnax.layer_maps.sparse.LayerMap.tree_unflatten","title":"<code>tree_unflatten(aux, children)</code>  <code>classmethod</code>","text":"<p>Reconstruct from aux and children (PyTree protocol).</p> <p>Parameters:</p> Name Type Description Default <code>aux</code> <code>tuple[Any, ...]</code> <p>The static metadata returned by :meth:<code>tree_flatten</code>: <code>(rows, cols_per_row, ndim)</code>.</p> required <code>children</code> <code>Iterable[AbstractModule]</code> <p>Modules in the exact row-major order produced by :meth:<code>tree_flatten</code>.</p> required <p>Returns:</p> Type Description <code>LayerMap</code> <p>A new instance with the same static key layout and provided values.</p>"},{"location":"reference/api/darnax/layer_maps/sparse/#darnax.layer_maps.sparse.LayerMap.rows","title":"<code>rows()</code>","text":"<p>All row indices in sorted order (static).</p>"},{"location":"reference/api/darnax/layer_maps/sparse/#darnax.layer_maps.sparse.LayerMap.cols_of","title":"<code>cols_of(i)</code>","text":"<p>All column indices of row <code>i</code> in sorted order (static for a given map).</p>"},{"location":"reference/api/darnax/layer_maps/sparse/#darnax.layer_maps.sparse.LayerMap.neighbors","title":"<code>neighbors(i)</code>","text":"<p>Read-only mapping of neighbors (<code>col \u2192 module</code>) for row <code>i</code>.</p>"},{"location":"reference/api/darnax/layer_maps/sparse/#darnax.layer_maps.sparse.LayerMap.row_items","title":"<code>row_items(skip_last=False, forward_only=False)</code>","text":"<p>Iterate over rows with deterministic ordering and read-only views.</p> <p>Parameters:</p> Name Type Description Default <code>skip_last</code> <code>bool</code> <p>If <code>True</code>, omit the last receiver row (useful when the output row is sink-only).</p> <code>False</code> <code>forward_only</code> <code>bool</code> <p>If <code>True</code>, keep only edges <code>(i, j)</code> with <code>j &lt;= i</code> (i.e., lower-triangular including the diagonal), which is a common \u201cfeed-forward\u201d scheduling constraint.</p> <code>False</code> <p>Yields:</p> Type Description <code>(row, neighbors) : tuple[int, Mapping[int, AbstractModule]]</code> <p>The row index and a read-only mapping of its neighbors.</p>"},{"location":"reference/api/darnax/layer_maps/sparse/#darnax.layer_maps.sparse.LayerMap.edge_items","title":"<code>edge_items()</code>","text":"<p>Iterate over edges in deterministic row-major order.</p> <p>Yields:</p> Type Description <code>((i, j), module) : tuple[tuple[int, int], AbstractModule]</code> <p>Edge key and its module.</p>"},{"location":"reference/api/darnax/layer_maps/sparse/#darnax.layer_maps.sparse.LayerMap.to_dict","title":"<code>to_dict()</code>","text":"<p>Deep copy as a plain, mutable dict-of-dicts.</p> <p>Returns:</p> Type Description <code>dict[int, dict[int, AbstractModule]]</code> <p>A new mapping with the same keys and module values. Mutations on this result do not affect the original <code>LayerMap</code>.</p>"},{"location":"reference/api/darnax/modules/debug/","title":"darnax.modules.debug","text":"<p>Debug/test modules for Darnax.</p> <p>This module provides tiny implementations used to exercise the orchestration and documentation pipeline:</p> <ul> <li>:class:<code>DebugLayer</code>: a stateful layer with a single scalar parameter.</li> <li>:class:<code>DebugAdapter</code>: a stateless adapter with a single scalar parameter.</li> </ul> <p>They are intentionally simple and not meant for learning quality; they exist to validate wiring, typing, and update flows.</p>"},{"location":"reference/api/darnax/modules/debug/#darnax.modules.debug.DebugLayer","title":"<code>DebugLayer()</code>","text":"<p>               Bases: <code>Layer</code></p> <p>A minimal trainable layer for testing and debugging.</p> <p>Multiplies inputs elementwise by a learnable scalar <code>w</code>. The activation (:meth:<code>activation</code>) is the sign function, but the default forward pass (:meth:<code>__call__</code>) deliberately returns <code>w * x</code> to keep behavior simple.</p> <p>Attributes:</p> Name Type Description <code>w</code> <code>Array</code> <p>Learnable weight, shape <code>(1,)</code>, dtype <code>float32</code>.</p> <code>a</code> <code>bool</code> <p>Static flag (non-PyTree) to mark the object as a layer. Kept for debugging/demo purposes only.</p> Notes <ul> <li>This class is stateful (as all :class:<code>~darnax.modules.interfaces.Layer</code>).</li> <li>The local update (:meth:<code>backward</code>) is a no-op that returns <code>self</code>; it   exists to exercise the training loop, not to learn.</li> </ul> <p>Initialize parameters.</p> Notes <p>Sets <code>w = 1.0</code> (<code>float32</code>) and <code>a = True</code>.</p>"},{"location":"reference/api/darnax/modules/debug/#darnax.modules.debug.DebugLayer.w","title":"<code>w = jnp.ones((1,), dtype=(jnp.float32))</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/debug/#darnax.modules.debug.DebugLayer.a","title":"<code>a = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/debug/#darnax.modules.debug.DebugLayer.activation","title":"<code>activation(x)</code>","text":"<p>Apply the layer\u2019s activation function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Pre-activation tensor.</p> required <p>Returns:</p> Type Description <code>Array</code> <p><code>sign(x)</code>.</p>"},{"location":"reference/api/darnax/modules/debug/#darnax.modules.debug.DebugLayer.reduce","title":"<code>reduce(h)</code>","text":"<p>Aggregate incoming messages into a single tensor.</p> <p>Parameters:</p> Name Type Description Default <code>h</code> <code>PyTree</code> <p>PyTree of arrays to be summed.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Sum over all leaves in <code>h</code> via an associative reduction.</p> Notes <p>Uses :func:<code>jax.tree.reduce_associative</code> with <code>operator.add</code>.</p>"},{"location":"reference/api/darnax/modules/debug/#darnax.modules.debug.DebugLayer.backward","title":"<code>backward(x, y, y_hat)</code>","text":"<p>Return a no-op parameter update.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Forward input (unused).</p> required <code>y</code> <code>Array</code> <p>Target/supervision (unused).</p> required <code>y_hat</code> <code>Array</code> <p>Prediction (unused).</p> required <p>Returns:</p> Type Description <code>Self</code> <p><code>self</code> \u2014 identity update (no change).</p>"},{"location":"reference/api/darnax/modules/debug/#darnax.modules.debug.DebugAdapter","title":"<code>DebugAdapter()</code>","text":"<p>               Bases: <code>Adapter</code></p> <p>A minimal stateless mapping for testing and debugging.</p> <p>Multiplies inputs elementwise by a scalar <code>w</code>. Carries no persistent state and returns a no-op update in :meth:<code>backward</code>.</p> <p>Attributes:</p> Name Type Description <code>w</code> <code>Array</code> <p>Weight, shape <code>(1,)</code>, dtype <code>float32</code>.</p> <code>a</code> <code>bool</code> <p>Static flag (non-PyTree) to mark the object as an adapter.</p> <p>Initialize parameters.</p> Notes <p>Sets <code>w = 1.0</code> (<code>float32</code>) and <code>a = False</code>.</p>"},{"location":"reference/api/darnax/modules/debug/#darnax.modules.debug.DebugAdapter.w","title":"<code>w = jnp.ones((1,), dtype=(jnp.float32))</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/debug/#darnax.modules.debug.DebugAdapter.a","title":"<code>a = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/debug/#darnax.modules.debug.DebugAdapter.backward","title":"<code>backward(x, y, y_hat)</code>","text":"<p>Return a no-op parameter update.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Forward input (unused).</p> required <code>y</code> <code>Array</code> <p>Target/supervision (unused).</p> required <code>y_hat</code> <code>Array</code> <p>Prediction (unused).</p> required <p>Returns:</p> Type Description <code>Self</code> <p><code>self</code> \u2014 identity update (no change).</p>"},{"location":"reference/api/darnax/modules/ferromagnetic/","title":"darnax.modules.ferromagnetic","text":"<p>Adapters: fixed linear couplings.</p> <p>This module defines :class:<code>Ferromagnetic</code>, a stateless adapter that scales signals elementwise by a fixed coupling <code>strength</code> (scalar or per-feature).</p> <p>Adapters in Darnax are Equinox <code>Module</code>s and thus PyTrees; they should not hold persistent state and typically return zero updates in :meth:<code>backward</code>.</p>"},{"location":"reference/api/darnax/modules/ferromagnetic/#darnax.modules.ferromagnetic.KeyArray","title":"<code>KeyArray = Array</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/modules/ferromagnetic/#darnax.modules.ferromagnetic.Ferromagnetic","title":"<code>Ferromagnetic(features, strength, dtype=jnp.float32)</code>","text":"<p>               Bases: <code>Adapter</code></p> <p>Elementwise scaling adapter with fixed coupling strength.</p> <p>Multiplies inputs by a constant coupling vector <code>strength</code>. This module is stateless and non-trainable: its :meth:<code>backward</code> returns a zero-shaped update PyTree.</p> <p>Attributes:</p> Name Type Description <code>strength</code> <code>Array</code> <p>Coupling strengths, shape <code>(features,)</code>; may originate from a scalar broadcast to that shape at construction time.</p> <p>Construct the adapter with scalar or per-feature strength.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>int</code> <p>Number of features; determines the length of <code>strength</code>.</p> required <code>strength</code> <code>ArrayLike</code> <p>Either a scalar (broadcast to <code>(features,)</code>) or a 1D array of length <code>features</code>.</p> required <code>dtype</code> <code>DTypeLike</code> <p>Dtype for the internal <code>strength</code> array. Default is <code>float32</code>.</p> <code>float32</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>strength</code> is neither a scalar nor a 1D array of length <code>features</code>.</p>"},{"location":"reference/api/darnax/modules/ferromagnetic/#darnax.modules.ferromagnetic.Ferromagnetic.strength","title":"<code>strength = self._set_shape(strength, features, dtype)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/ferromagnetic/#darnax.modules.ferromagnetic.Ferromagnetic.backward","title":"<code>backward(x, y, y_hat)</code>","text":"<p>Return a zero update to indicate non-trainability.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Forward input (unused).</p> required <code>y</code> <code>Array</code> <p>Target/supervision (unused).</p> required <code>y_hat</code> <code>Array</code> <p>Prediction (unused).</p> required <p>Returns:</p> Type Description <code>Self</code> <p>A PyTree matching <code>self</code> where all leaves are zeros.</p>"},{"location":"reference/api/darnax/modules/fully_connected/","title":"darnax.modules.fully_connected","text":"<p>Fully connected adapters.</p> <p>This module provides two Equinox-based adapters:</p> <ul> <li>:class:<code>FullyConnected</code>: trainable affine map with per-output scaling and   a local perceptron-style update (only <code>W</code> is updated).</li> <li>:class:<code>FrozenFullyConnected</code>: same forward as <code>FullyConnected</code> but returns   a zero update (useful for inference or ablation).</li> </ul> <p>Both classes are stateless in the runtime sense (no persistent state across steps) but parameterized (PyTrees with trainable weights).</p>"},{"location":"reference/api/darnax/modules/fully_connected/#darnax.modules.fully_connected.KeyArray","title":"<code>KeyArray = Array</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/modules/fully_connected/#darnax.modules.fully_connected.FullyConnected","title":"<code>FullyConnected(in_features, out_features, strength, threshold, key, dtype=jnp.float32)</code>","text":"<p>               Bases: <code>Adapter</code></p> <p>Fully connected trainable adapter <code>y = (x @ W) * strength</code>.</p> <p>A dense linear projection followed by an elementwise per-output scaling. Learning uses a local perceptron-style rule parameterized by a per-output <code>threshold</code>; only <code>W</code> receives updates, while <code>strength</code> and <code>threshold</code> act as (learnable-if-you-want) hyperparameters that are not updated by :meth:<code>backward</code>.</p> <p>Attributes:</p> Name Type Description <code>W</code> <code>Array</code> <p>Weight matrix with shape <code>(in_features, out_features)</code>.</p> <code>strength</code> <code>Array</code> <p>Per-output scale, shape <code>(out_features,)</code>; broadcast across the last dimension of the forward output.</p> <code>threshold</code> <code>Array</code> <p>Per-output margin used by the local update rule, shape <code>(out_features,)</code>.</p> Notes <ul> <li>Adapters are stateless per the Darnax interface, but they may carry   trainable parameters. This class advertises trainability through <code>W</code>.</li> <li>The local rule is supplied by   :func:<code>darnax.utils.perceptron_rule.perceptron_rule_backward</code> and is not   required to be a gradient.</li> </ul> <p>Initialize weights and per-output scale/threshold.</p> <p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>int</code> <p>Input dimensionality.</p> required <code>out_features</code> <code>int</code> <p>Output dimensionality.</p> required <code>strength</code> <code>float or ArrayLike</code> <p>Scalar (broadcast to <code>(out_features,)</code>) or a vector of length <code>out_features</code> providing the per-output scaling.</p> required <code>threshold</code> <code>float or ArrayLike</code> <p>Scalar or vector of length <code>out_features</code> with the per-output margins used by the local update rule.</p> required <code>key</code> <code>Array</code> <p>JAX PRNG key to initialize <code>W</code> with Gaussian entries scaled by <code>1/sqrt(in_features)</code>.</p> required <code>dtype</code> <code>DTypeLike</code> <p>Dtype for parameters (default: <code>jnp.float32</code>).</p> <code>float32</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>strength</code> or <code>threshold</code> is neither a scalar nor a 1D array of the expected length.</p>"},{"location":"reference/api/darnax/modules/fully_connected/#darnax.modules.fully_connected.FullyConnected.strength","title":"<code>strength = self._set_shape(strength, out_features, dtype)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/fully_connected/#darnax.modules.fully_connected.FullyConnected.threshold","title":"<code>threshold = self._set_shape(threshold, out_features, dtype)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/fully_connected/#darnax.modules.fully_connected.FullyConnected.W","title":"<code>W = jax.random.normal(key, (in_features, out_features), dtype=dtype) * self.strength / jnp.sqrt(in_features)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/fully_connected/#darnax.modules.fully_connected.FullyConnected.backward","title":"<code>backward(x, y, y_hat)</code>","text":"<p>Return a module-shaped local update where only <code>\u0394W</code> is set.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Forward input(s), shape <code>(..., in_features)</code>.</p> required <code>y</code> <code>Array</code> <p>Supervision signal/targets, broadcast-compatible with <code>y_hat</code>.</p> required <code>y_hat</code> <code>Array</code> <p>Current prediction/logits, broadcast-compatible with <code>y</code>.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>A PyTree with the same structure as <code>self</code> where: - <code>W</code> holds the update <code>\u0394W</code> from the local rule, - <code>strength</code> and <code>threshold</code> leaves are zeros.</p> Notes <p>Calls :func:<code>darnax.utils.perceptron_rule.perceptron_rule_backward</code> with the stored per-output <code>threshold</code>.</p>"},{"location":"reference/api/darnax/modules/fully_connected/#darnax.modules.fully_connected.FrozenFullyConnected","title":"<code>FrozenFullyConnected(in_features, out_features, strength, threshold, key, dtype=jnp.float32)</code>","text":"<p>               Bases: <code>FullyConnected</code></p> <p>Fully connected adapter with frozen parameters.</p> <p>Same forward behavior as :class:<code>FullyConnected</code>, but :meth:<code>backward</code> returns zeros for all leaves. Useful for inference-only deployments or to ablate learning of a particular edge type in a graph.</p>"},{"location":"reference/api/darnax/modules/fully_connected/#darnax.modules.fully_connected.FrozenFullyConnected.backward","title":"<code>backward(x, y, y_hat)</code>","text":"<p>Return zero update for all parameters.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Forward input (unused).</p> required <code>y</code> <code>Array</code> <p>Target/supervision (unused).</p> required <code>y_hat</code> <code>Array</code> <p>Prediction/logits (unused).</p> required <p>Returns:</p> Type Description <code>Self</code> <p>PyTree of zeros with the same structure as <code>self</code>.</p>"},{"location":"reference/api/darnax/modules/input_output/","title":"darnax.modules.input_output","text":"<p>Output layer utilities.</p> <p>This module defines :class:<code>OutputLayer</code>, a lightweight aggregation layer meant to sit at the boundary of the network. It provides:</p> <ul> <li>identity activation (no nonlinearity),</li> <li>identity forward pass (conceptually; see Notes),</li> <li>a reduce method that elementwise-sums all array leaves in a PyTree of   predictions,</li> <li>a no-op local update (:meth:<code>backward</code>) since there are no trainable   parameters.</li> </ul> Notes <p>Despite being conceptually stateless, :class:<code>OutputLayer</code> subclasses :class:<code>darnax.modules.interfaces.Layer</code> to reuse the orchestration contract, including the <code>reduce</code> interface.</p>"},{"location":"reference/api/darnax/modules/input_output/#darnax.modules.input_output.KeyArray","title":"<code>KeyArray = Array</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/modules/input_output/#darnax.modules.input_output.OutputLayer","title":"<code>OutputLayer</code>","text":"<p>               Bases: <code>Layer</code></p> <p>Simple output layer that aggregates predictions via summation.</p> <p>The layer leaves activations unchanged and defines a <code>reduce</code> that elementwise-sums a PyTree of predictions. Its backward pass is a no-op because it has no trainable parameters.</p> Notes <p>The current implementation of :meth:<code>__call__</code> returns <code>zeros_like(x)</code>.</p>"},{"location":"reference/api/darnax/modules/input_output/#darnax.modules.input_output.OutputLayer.reduce","title":"<code>reduce(h)</code>","text":"<p>Elementwise-sum all array leaves in a PyTree of predictions.</p> <p>Parameters:</p> Name Type Description Default <code>h</code> <code>PyTree</code> <p>PyTree whose leaves are arrays with identical shapes and dtypes.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>The elementwise sum across all leaves.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>h</code> has no leaves (as per <code>tree_reduce</code> semantics).</p> Notes <p>Uses :func:<code>jax.tree_util.tree_reduce</code> with :data:<code>operator.add</code>.</p>"},{"location":"reference/api/darnax/modules/input_output/#darnax.modules.input_output.OutputLayer.activation","title":"<code>activation(x)</code>","text":"<p>Identity activation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Array</code> <p><code>x</code> unchanged.</p>"},{"location":"reference/api/darnax/modules/input_output/#darnax.modules.input_output.OutputLayer.backward","title":"<code>backward(x, y, y_hat)</code>","text":"<p>No-op local update.</p> <p>This layer has no trainable parameters, so it returns itself unchanged.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Forward input (unused).</p> required <code>y</code> <code>Array</code> <p>Target/supervision (unused).</p> required <code>y_hat</code> <code>Array</code> <p>Prediction (unused).</p> required <p>Returns:</p> Type Description <code>Self</code> <p><code>self</code> (no parameter updates).</p>"},{"location":"reference/api/darnax/modules/interfaces/","title":"darnax.modules.interfaces","text":"<p>Interfaces for Darnax modules.</p> <p>This module defines the abstract contracts implemented by all computational components in Darnax:</p> <ul> <li><code>AbstractModule</code> is the common base for anything callable during the   recurrent dynamics (layers and adapters).</li> <li><code>Layer</code> is a stateful module that aggregates incoming messages and   applies an activation.</li> <li><code>Adapter</code> is a stateless mapping between layers (e.g., projections,   reshapes, or wiring).</li> </ul> <p>All classes are Equinox <code>Module</code>s (i.e., PyTrees) and are compatible with JAX transformations (<code>jit</code>, <code>vmap</code>, <code>grad</code>/custom rules, etc.).</p>"},{"location":"reference/api/darnax/modules/interfaces/#darnax.modules.interfaces.KeyArray","title":"<code>KeyArray = Array</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/modules/interfaces/#darnax.modules.interfaces.AbstractModule","title":"<code>AbstractModule</code>","text":"<p>               Bases: <code>Module</code>, <code>ABC</code></p> <p>Base class for layers and adapters.</p> <p>Subclasses must implement a pure functional forward pass and a local learning rule via :meth:<code>backward</code>. The object itself is a PyTree: parameter fields are leaves, and nested modules are subtrees. This makes instances compatible with Equinox/Optax update flows.</p> Notes <ul> <li>The forward pass must not mutate parameters or hidden state.   Any persistent state updates are orchestrated outside the call via the   training loop (see tutorials).</li> <li><code>rng</code> is optional; when provided, it should be a JAX PRNG key   (<code>KeyArray</code>). If <code>None</code>, the module must behave deterministically.</li> </ul>"},{"location":"reference/api/darnax/modules/interfaces/#darnax.modules.interfaces.AbstractModule.has_state","title":"<code>has_state</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Whether the module carries persistent state.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the module owns persistent state (e.g., activations, running stats, internal buffers) that is managed by the orchestrator/training loop; <code>False</code> otherwise.</p>"},{"location":"reference/api/darnax/modules/interfaces/#darnax.modules.interfaces.AbstractModule.backward","title":"<code>backward(x, y, y_hat)</code>  <code>abstractmethod</code>","text":"<p>Compute a local parameter update (same PyTree structure).</p> <p>This method implements a local plasticity rule that produces a PyTree of updates aligned with the module's parameter structure. The returned object is typically consumed by an optimizer (e.g., Optax) or combined with other updates by the orchestrator.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Inputs seen at the forward call (may be cached by the caller).</p> required <code>y</code> <code>Array</code> <p>Supervision signal or target associated with the current step. Shape must be compatible with the module's output space.</p> required <code>y_hat</code> <code>Array</code> <p>The module's (or readout's) current prediction.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>A PyTree with the same structure as this module, containing per-parameter updates (e.g., <code>dW</code>, <code>db</code>). Implementations may return zeros for non-trainable fields.</p> Notes <p>The update is not required to be a gradient. It can be any local rule (e.g., perceptron-style, Hebbian/anti-Hebbian) as long as the shape and PyTree layout match the module parameters.</p>"},{"location":"reference/api/darnax/modules/interfaces/#darnax.modules.interfaces.Layer","title":"<code>Layer</code>","text":"<p>               Bases: <code>AbstractModule</code>, <code>ABC</code></p> <p>A stateful, trainable layer that reduces messages then applies an activation.</p> <p>Layers are the primary compute units in Darnax. They typically: (1) aggregate incoming messages from upstream modules with :meth:<code>reduce</code>, and (2) transform the aggregate via :meth:<code>activation</code>.</p> Notes <ul> <li>Layers are stateful by design (e.g., carry hidden activations or   buffers across steps); the orchestrator is responsible for when/how state   is read/written. The State object carries the state.</li> <li>The forward pass should be purely functional with respect to parameters   and external state; do not mutate in-place.</li> </ul>"},{"location":"reference/api/darnax/modules/interfaces/#darnax.modules.interfaces.Layer.has_state","title":"<code>has_state</code>  <code>property</code>","text":"<p>Whether the layer carries persistent state.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Always <code>True</code> for layers.</p>"},{"location":"reference/api/darnax/modules/interfaces/#darnax.modules.interfaces.Layer.activation","title":"<code>activation(x)</code>  <code>abstractmethod</code>","text":"<p>Apply the layer\u2019s activation function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Pre-activation tensor (e.g., the result of :meth:<code>reduce</code>).</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Post-activation tensor. Typically the same shape as <code>x</code>.</p>"},{"location":"reference/api/darnax/modules/interfaces/#darnax.modules.interfaces.Layer.reduce","title":"<code>reduce(h)</code>  <code>abstractmethod</code>","text":"<p>Aggregate incoming messages into a single tensor.</p> <p>Parameters:</p> Name Type Description Default <code>h</code> <code>PyTree</code> <p>Collection of incoming messages from neighbors/upstream modules. Implementations define the exact structure; common reducers include sum, mean, or structured/sparse contractions.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Aggregated input to be passed to :meth:<code>activation</code>.</p> <p>Examples:</p> <p>A sum reducer over message leaves::</p> <pre><code>leaves = jax.tree_util.tree_leaves(h)\nx = jnp.sum(jnp.stack(leaves, axis=0), axis=0)\nreturn x\n</code></pre>"},{"location":"reference/api/darnax/modules/interfaces/#darnax.modules.interfaces.Adapter","title":"<code>Adapter</code>","text":"<p>               Bases: <code>AbstractModule</code>, <code>ABC</code></p> <p>A stateless mapping between layers.</p> <p>Adapters connect layers (e.g., linear projections, reshapes, sparsifying maps). They must not carry persistent state and should behave as pure functions of inputs and parameters (plus optional RNG).</p> Notes <ul> <li>Use adapters to express wiring and shape changes between layers.</li> <li>Because adapters are stateless, orchestration can freely reorder or   parallelize them as long as data dependencies are respected.</li> </ul>"},{"location":"reference/api/darnax/modules/interfaces/#darnax.modules.interfaces.Adapter.has_state","title":"<code>has_state</code>  <code>property</code>","text":"<p>Whether the adapter carries persistent state.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Always <code>False</code> for adapters.</p>"},{"location":"reference/api/darnax/modules/recurrent/","title":"darnax.modules.recurrent","text":"<p>Binary (\u00b11) recurrent layer with dense couplings.</p> <p>This module defines :class:<code>RecurrentDiscrete</code>, a fully connected recurrent layer whose states live in <code>{-1, +1}</code>. The coupling matrix <code>J</code> has a controllable diagonal <code>J_D</code> (self-couplings), and learning uses a local, perceptron-style rule with per-unit margins (<code>threshold</code>).</p> Design <ul> <li>State space: discrete, <code>s_i \u2208 {-1, +1}</code>.</li> <li>Couplings: dense matrix <code>J \u2208 \u211d^{d\u00d7d}</code>, diagonal forced to <code>J_D</code>.</li> <li>Forward: pre-activation <code>h = x @ J</code> (no in-place mutation).</li> <li>Activation: hard sign with ties to <code>+1</code>.</li> <li>Learning: :func:<code>darnax.utils.perceptron_rule.perceptron_rule_backward</code>   produces <code>\u0394J</code>; the diagonal update is masked to zero so self-couplings   remain fixed at <code>J_D</code>.</li> </ul> Notes <p>This class is an Equinox <code>Module</code> (a PyTree). Parameters are leaves and can be updated via Optax or custom update code. The orchestrator controls when to call <code>activation</code> vs forwarding pre-activations.</p> See Also <p>darnax.modules.interfaces.Layer darnax.utils.perceptron_rule.perceptron_rule_backward</p>"},{"location":"reference/api/darnax/modules/recurrent/#darnax.modules.recurrent.KeyArray","title":"<code>KeyArray = jax.Array</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/modules/recurrent/#darnax.modules.recurrent.RecurrentDiscrete","title":"<code>RecurrentDiscrete(features, j_d, threshold, key, dtype=jnp.float32)</code>","text":"<p>               Bases: <code>Layer</code></p> <p>Binary (\u00b11) recurrent layer with dense couplings.</p> <p>The layer keeps a dense coupling matrix <code>J</code> (with fixed diagonal <code>J_D</code>), a per-unit margin <code>threshold</code> for local updates, and an internal diagonal mask used to zero out self-updates during learning.</p> <p>Attributes:</p> Name Type Description <code>J</code> <code>Array</code> <p>Coupling matrix with shape <code>(features, features)</code>.</p> <code>J_D</code> <code>Array</code> <p>Diagonal self-couplings, shape <code>(features,)</code>. Mirrors <code>jnp.diag(J)</code> and is kept fixed by masking during updates.</p> <code>threshold</code> <code>Array</code> <p>Per-unit margin used by the local perceptron-style rule, shape <code>(features,)</code>.</p> <code>_mask</code> <code>Array</code> <p>Binary matrix (<code>1 - I</code>) that zeroes the diagonal of <code>\u0394J</code> before applying updates. Same shape and dtype as <code>J</code>.</p> <p>Construct the layer parameters.</p> <p>Initializes a dense coupling matrix <code>J</code> with i.i.d. Gaussian entries scaled by <code>1/sqrt(features)</code> and sets its diagonal to <code>j_d</code>. Stores per-unit margins in <code>threshold</code> and a diagonal masking matrix to keep self-couplings fixed during learning.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>int</code> <p>Number of units (dimension <code>d</code>). Shapes are derived from this.</p> required <code>j_d</code> <code>ArrayLike</code> <p>Self-couplings (diagonal of <code>J</code>). Either a scalar (broadcast to <code>(features,)</code>) or a vector of length <code>features</code>.</p> required <code>threshold</code> <code>ArrayLike</code> <p>Per-unit margins for the local update rule. Scalar or vector of length <code>features</code>.</p> required <code>key</code> <code>KeyArray</code> <p>JAX PRNG key used to initialize the off-diagonal entries of <code>J</code>.</p> required <code>dtype</code> <code>DTypeLike</code> <p>Parameter dtype, by default <code>jnp.float32</code>.</p> <code>float32</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>j_d</code> or <code>threshold</code> is not scalar or a 1D vector with length <code>features</code>.</p>"},{"location":"reference/api/darnax/modules/recurrent/#darnax.modules.recurrent.RecurrentDiscrete.J","title":"<code>J = J</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/recurrent/#darnax.modules.recurrent.RecurrentDiscrete.J_D","title":"<code>J_D = j_d_vec</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/recurrent/#darnax.modules.recurrent.RecurrentDiscrete.threshold","title":"<code>threshold = thresh_vec</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/recurrent/#darnax.modules.recurrent.RecurrentDiscrete.activation","title":"<code>activation(x)</code>","text":"<p>Hard-sign activation mapping ties to <code>+1</code>.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Pre-activation tensor.</p> required <p>Returns:</p> Type Description <code>Array</code> <p><code>(+1)</code> where <code>x &gt;= 0</code> and <code>(-1)</code> otherwise, cast to <code>x.dtype</code>.</p> Notes <p>This function is separate from :meth:<code>__call__</code> so orchestrators can decide when to discretize (e.g., training vs inference dynamics).</p>"},{"location":"reference/api/darnax/modules/recurrent/#darnax.modules.recurrent.RecurrentDiscrete.reduce","title":"<code>reduce(h)</code>","text":"<p>Aggregate incoming messages by summation.</p> <p>Parameters:</p> Name Type Description Default <code>h</code> <code>PyTree</code> <p>PyTree of arrays (e.g., messages from neighbors) to be summed.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Elementwise sum over all leaves in <code>h</code>.</p> Notes <p>Uses :func:<code>jax.tree_util.tree_reduce</code> with :data:<code>operator.add</code>.</p>"},{"location":"reference/api/darnax/modules/recurrent/#darnax.modules.recurrent.RecurrentDiscrete.backward","title":"<code>backward(x, y, y_hat)</code>","text":"<p>Compute a module-shaped local update.</p> <p>Produces a PyTree of updates where only <code>J</code> receives a nonzero <code>\u0394J</code>; all other fields are zero. The diagonal of <code>\u0394J</code> is masked to zero so self-couplings stay fixed at <code>J_D</code>.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Inputs used to produce the current prediction. Shape <code>(features,)</code> or <code>(batch, features)</code>.</p> required <code>y</code> <code>Array</code> <p>Supervision signal/targets, broadcast-compatible with <code>y_hat</code>.</p> required <code>y_hat</code> <code>Array</code> <p>Current prediction/logits, broadcast-compatible with <code>y</code>.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>A PyTree with the same structure as <code>self</code> where: - <code>J</code> contains <code>\u0394J</code> (diagonal zeroed), - all other leaves are zeros.</p> Notes <p>Calls :func:<code>darnax.utils.perceptron_rule.perceptron_rule_backward</code> with the stored per-unit <code>threshold</code>. The rule is local and need not be a true gradient.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; upd = layer.backward(x, y, y_hat)\n&gt;&gt;&gt; new_params = eqx.tree_at(lambda m: m.J, layer, layer.J + lr * upd.J)\n</code></pre>"},{"location":"reference/api/darnax/orchestrators/interface/","title":"darnax.orchestrators.interface","text":"<p>Abstract orchestrator interface.</p> <p>An orchestrator schedules message passing, state updates, and local learning over a :class:<code>~darnax.layer_maps.sparse.LayerMap</code>. It owns no parameters itself; it routes tensors through modules, updates a global :class:<code>~darnax.states.interface.State</code>, and returns parameter updates shaped like the layermap.</p> Responsibilities <ul> <li>step: run one forward/update pass that refreshes non-output buffers   (keeps the output buffer unchanged). Typically used inside a recurrent loop.</li> <li>step_inference: like <code>step</code> but restricted to a one-sided schedule   (e.g., skip \u201cright-going\u201d messages) for cheaper partial updates; output   remains unchanged.</li> <li>predict: compute the output buffer from the current internal buffers   (no learning).</li> <li>backward: compute a PyTree of parameter updates with the same structure   as the layermap (suitable for Optax <code>update/apply_updates</code>).</li> </ul> RNG handling <p>Each method accepts an RNG key and returns a new RNG key. Implementations should split the key to ensure reproducible, side-effect-free randomness.</p> See Also <p>tutorials/05_orchestrators.md tutorials/06_simple_net_on_artificial_data.md</p>"},{"location":"reference/api/darnax/orchestrators/interface/#darnax.orchestrators.interface.ModuleT","title":"<code>ModuleT = TypeVar('ModuleT', bound='AbstractModule')</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/orchestrators/interface/#darnax.orchestrators.interface.StateT","title":"<code>StateT = TypeVar('StateT', bound='State')</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/orchestrators/interface/#darnax.orchestrators.interface.KeyArray","title":"<code>KeyArray = jax.Array</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/orchestrators/interface/#darnax.orchestrators.interface.AbstractOrchestrator","title":"<code>AbstractOrchestrator</code>","text":"<p>               Bases: <code>Module</code>, <code>Generic[StateT]</code></p> <p>Handle communication and scheduling over a :class:<code>LayerMap</code>.</p> <p>The orchestrator defines how modules exchange messages (graph traversal, fan-in reduction, activation timing) and when learning rules are applied. Concrete subclasses encode the schedule (e.g., left\u2192right passes, recurrent sweeps, or sparsity-aware traversals).</p> <p>Attributes:</p> Name Type Description <code>lmap</code> <code>LayerMap</code> <p>The static adjacency (rows/cols and order). Module parameters inside the layermap are PyTree leaves visible to JAX/Optax; the key layout is static for JIT stability.</p> Notes <ul> <li>Methods are expected to be pure with respect to inputs: given a state   and RNG, they return a new state and a new RNG (no in-place mutation).</li> <li>Use :func:<code>jax.random.split</code> to manage randomness. Determinism under JIT   requires not reusing RNG keys.</li> <li><code>backward</code> returns updates (same PyTree shape as <code>lmap</code>), not new   parameters. Applying those updates is the optimizer\u2019s job.</li> </ul>"},{"location":"reference/api/darnax/orchestrators/interface/#darnax.orchestrators.interface.AbstractOrchestrator.lmap","title":"<code>lmap</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/orchestrators/interface/#darnax.orchestrators.interface.AbstractOrchestrator.step","title":"<code>step(state, *, rng)</code>  <code>abstractmethod</code>","text":"<p>Run one forward/update step without touching the output buffer.</p> <p>Typical use is inside a recurrent loop to evolve hidden buffers while deferring the explicit output computation to :meth:<code>predict</code>.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>StateT</code> <p>Current global state (input at index <code>0</code>, output at <code>-1</code>).</p> required <code>rng</code> <code>Array</code> <p>PRNG key for any stochastic modules. Implementations should split the key internally.</p> required <p>Returns:</p> Type Description <code>(new_state, new_rng) : tuple[StateT, jax.Array]</code> <p>The updated state (same type) and a fresh RNG key.</p> Notes <ul> <li>Must not modify <code>state[-1]</code> (the output buffer).</li> <li>Scheduling is defined by the subclass (e.g., process edges with   <code>j &lt;= i</code> only, or a full sweep excluding output).</li> </ul>"},{"location":"reference/api/darnax/orchestrators/interface/#darnax.orchestrators.interface.AbstractOrchestrator.step_inference","title":"<code>step_inference(state, *, rng)</code>  <code>abstractmethod</code>","text":"<p>Run a cheaper, inference-oriented step (output unchanged).</p> <p>This variant avoids computing messages that travel \u201cto the right\u201d (exact meaning is schedule-specific; commonly skip edges with <code>j &gt; i</code>). Useful for partial refreshes of upstream buffers.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>StateT</code> <p>Current global state.</p> required <code>rng</code> <code>Array</code> <p>PRNG key to split.</p> required <p>Returns:</p> Type Description <code>(new_state, new_rng) : tuple[StateT, jax.Array]</code> <p>The updated state and a fresh RNG key.</p> Notes <p>Must not modify the output buffer <code>state[-1]</code>.</p>"},{"location":"reference/api/darnax/orchestrators/interface/#darnax.orchestrators.interface.AbstractOrchestrator.predict","title":"<code>predict(state, *, rng)</code>  <code>abstractmethod</code>","text":"<p>Compute/refresh the output buffer from current internal buffers.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>StateT</code> <p>Current global state (uses existing hidden buffers).</p> required <code>rng</code> <code>Array</code> <p>PRNG key to split, if output computation is stochastic.</p> required <p>Returns:</p> Type Description <code>(new_state, new_rng) : tuple[StateT, jax.Array]</code> <p>A state where <code>state[-1]</code> has been updated, and a fresh RNG key.</p> Notes <p>This method performs no parameter updates; it\u2019s a pure readout pass.</p>"},{"location":"reference/api/darnax/orchestrators/interface/#darnax.orchestrators.interface.AbstractOrchestrator.backward","title":"<code>backward(state, rng)</code>  <code>abstractmethod</code>","text":"<p>Compute parameter updates aligned with the layermap structure.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>StateT</code> <p>Current global state providing the activations/messages required by local learning rules (e.g., perceptron updates).</p> required <code>rng</code> <code>Array</code> <p>PRNG key to split for stochastic update rules.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>A PyTree with the same structure as <code>self</code> where the <code>lmap</code> subtree mirrors the original layermap but each module is replaced by its parameter update (zeros for non-trainable fields).</p> Notes <ul> <li>The returned object is intended for use with Optax:   <code>updates = orchestrator.backward(state, rng)</code>,   then <code>new_params = optax.apply_updates(params, updates)</code>.</li> <li>Implementations should avoid side effects and return only updates,   not updated parameters.</li> </ul>"},{"location":"reference/api/darnax/orchestrators/sequential/","title":"darnax.orchestrators.sequential","text":"<p>Sequential orchestrator for layered networks.</p> <p>This module provides :class:<code>SequentialOrchestrator</code>, a concrete implementation of the orchestrator contract that traverses a :class:<code>~darnax.layer_maps.sparse.LayerMap</code> row-by-row (deterministic order), computes messages along edges, reduces them at each receiver, and updates a :class:<code>~darnax.states.sequential.SequentialState</code>.</p> Overview <ul> <li>:meth:<code>step</code> updates all receivers except the output row (keeps <code>state[-1]</code> unchanged).</li> <li>:meth:<code>step_inference</code> is like :meth:<code>step</code> but skips right-going edges   (uses a forward-only schedule; cheaper partial refresh).</li> <li>:meth:<code>predict</code> computes the output buffer from current internal buffers.</li> <li>:meth:<code>backward</code> builds a LayerMap-shaped PyTree of parameter updates by   calling each module\u2019s local rule. The method returns a new orchestrator whose   <code>lmap</code> contains updates (not new parameters), so it can be fed directly   to Optax\u2019s <code>apply_updates</code>.</li> </ul> RNG handling <p>All public methods accept an RNG key and return a new key. Internally, keys are split per receiver and per sender to keep randomness reproducible and side-effect free.</p> See Also <p>tutorials/05_orchestrators.md tutorials/06_simple_net_on_artificial_data.md</p>"},{"location":"reference/api/darnax/orchestrators/sequential/#darnax.orchestrators.sequential.KeyArray","title":"<code>KeyArray = Array</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/orchestrators/sequential/#darnax.orchestrators.sequential.SequentialOrchestrator","title":"<code>SequentialOrchestrator(layers)</code>","text":"<p>               Bases: <code>AbstractOrchestrator[SequentialState]</code></p> <p>Sequential message-passing orchestrator.</p> Assumptions <ul> <li><code>lmap</code> has static structure (sorted integer keys); values are Equinox   modules that are PyTrees (parameters visible to JAX/Optax).</li> <li>For each receiver <code>i</code>, the diagonal module <code>lmap[i, i]</code> implements   <code>reduce(pytree_of_messages)</code> and <code>activation(Array) -&gt; Array</code>.</li> <li>Each edge module <code>lmap[i, j]</code> is callable as <code>module(x, rng=...) -&gt; Array</code>   and exposes <code>backward(x, y, y_hat) -&gt; AbstractModule</code> (same PyTree type).</li> </ul> Notes <p>This orchestrator is pure: given a state and RNG, it returns a new state and a new RNG key. All scheduling choices (row order, forward-only mode) are explicit and deterministic.</p> <p>Initialize the orchestrator from a layermap.</p> <p>Parameters:</p> Name Type Description Default <code>layers</code> <code>LayerMap</code> <p>Static adjacency (rows/cols) with Equinox modules as values.</p> required"},{"location":"reference/api/darnax/orchestrators/sequential/#darnax.orchestrators.sequential.SequentialOrchestrator.lmap","title":"<code>lmap = layers</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/orchestrators/sequential/#darnax.orchestrators.sequential.SequentialOrchestrator.step","title":"<code>step(state, *, rng)</code>","text":"<p>Run one forward/update sweep for all receivers except output.</p> <p>For each receiver row <code>i</code> (excluding the last), the orchestrator: (1) computes messages from all neighbors <code>j</code> in <code>lmap[i]</code>, (2) reduces them via <code>lmap[i, i].reduce(messages)</code>, (3) applies <code>lmap[i, i].activation(...)</code>, (4) writes the result into <code>state[i]</code>.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>SequentialState</code> <p>Current global state.</p> required <code>rng</code> <code>KeyArray</code> <p>PRNG key (split per receiver/sender).</p> required <p>Returns:</p> Type Description <code>(new_state, new_rng) : tuple[SequentialState, KeyArray]</code> <p>Updated state (output unchanged) and an advanced RNG key.</p>"},{"location":"reference/api/darnax/orchestrators/sequential/#darnax.orchestrators.sequential.SequentialOrchestrator.step_inference","title":"<code>step_inference(state, *, rng)</code>","text":"<p>Run a forward-only sweep (skip output and right-going edges).</p> <p>Like :meth:<code>step</code>, but calls <code>row_items(skip_last=True, forward_only=True)</code>, which filters neighbor edges to a forward schedule (commonly <code>j &lt;= i</code>).</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>SequentialState</code> <p>Current global state.</p> required <code>rng</code> <code>KeyArray</code> <p>PRNG key (split per receiver/sender).</p> required <p>Returns:</p> Type Description <code>(new_state, new_rng) : tuple[SequentialState, KeyArray]</code> <p>Updated state (output unchanged) and an advanced RNG key.</p>"},{"location":"reference/api/darnax/orchestrators/sequential/#darnax.orchestrators.sequential.SequentialOrchestrator.predict","title":"<code>predict(state, rng)</code>","text":"<p>Compute/refresh the output buffer <code>state[-1]</code> from current buffers.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>SequentialState</code> <p>Current global state (uses existing upstream buffers).</p> required <code>rng</code> <code>KeyArray</code> <p>PRNG key (split internally).</p> required <p>Returns:</p> Type Description <code>(new_state, new_rng) : tuple[SequentialState, KeyArray]</code> <p>A state where the output buffer has been updated, and a fresh RNG key.</p>"},{"location":"reference/api/darnax/orchestrators/sequential/#darnax.orchestrators.sequential.SequentialOrchestrator.backward","title":"<code>backward(state, rng)</code>","text":"<p>Compute per-edge parameter updates and return them as an orchestrator.</p> <p>Two-phase algorithm:</p> <p>1) Activation pass. For each receiver <code>i</code> (including the output),    compute messages <code>msg[j] = lmap[i, j](state[j], rng=...)</code> from    all neighbors, then compute the receiver\u2019s aggregate using forward    messages only (e.g., <code>j &lt;= i</code>) and store it under <code>msg[i]</code>. 2) Local rules. For every edge <code>(i, j)</code>, call    <code>lmap[i, j].backward(x=state[j], y=state[i], y_hat=msg[j])</code> and    assemble the results into a new LayerMap with the same key layout.</p> <p>The method returns <code>type(self)(layers=updates)</code> where <code>updates</code> is the LayerMap of per-edge updates. This allows <code>optax.apply_updates</code> to be used directly with the returned PyTree.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>SequentialState</code> <p>Current global state (provides inputs and targets for local rules).</p> required <code>rng</code> <code>KeyArray</code> <p>PRNG key (split per receiver/sender).</p> required <p>Returns:</p> Type Description <code>Self</code> <p>An orchestrator whose <code>lmap</code> contains updates (not new params), mirroring the original layermap structure.</p> Notes <p>The receiver aggregate used for learning is computed with forward-only messages (no right-going edges), matching the schedule used in inference.</p>"},{"location":"reference/api/darnax/states/interface/","title":"darnax.states.interface","text":"<p>State interfaces.</p> <p>This module defines the abstract :class:<code>State</code> contract used by orchestrators and layer maps. A concrete state holds per-layer tensors plus two special slots:</p> <ul> <li>index <code>0</code>: input (provided by the caller),</li> <li>last index: output (produced by the network).</li> </ul> <p>Implementations must be Equinox <code>Module</code>s (i.e., PyTrees) and support functional updates (returning new state instances rather than mutating in place).</p>"},{"location":"reference/api/darnax/states/interface/#darnax.states.interface.State","title":"<code>State</code>","text":"<p>               Bases: <code>Module</code></p> <p>Global state for a network (LayerMap).</p> <p>A <code>State</code> stores the tensors associated with each layer plus the designated input and output slots. The total number of elements must be at least <code>#layers + 1</code> (input + one per layer + output).</p> <p>Concretely, orchestrators assume: - <code>state[0]</code> holds the input, - <code>state[-1]</code> holds the output, - the remaining indices map to intermediate layer states according to the   LayerMap\u2019s ordering.</p> Notes <ul> <li><code>State</code> is an Equinox <code>Module</code> and thus a PyTree; it should be safe   to pass through JAX transformations (<code>jit</code>, <code>vmap</code>) as long as   implementations avoid in-place mutation.</li> <li>All mutating operations must be exposed as functional methods that   return a new instance (see :meth:<code>replace</code> and :meth:<code>replace_val</code>).</li> </ul> See Also <p>darnax.states.sequential.SequentialState     A concrete reference implementation using array-backed storage.</p>"},{"location":"reference/api/darnax/states/interface/#darnax.states.interface.State.init","title":"<code>init(x, y=None)</code>  <code>abstractmethod</code>","text":"<p>Initialize input/output slots and return a fresh state.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input tensor to place at index <code>0</code>.</p> required <code>y</code> <code>Array or None</code> <p>Optional target/output tensor to place at the last index. If <code>None</code>, the output slot should be initialized according to the implementation\u2019s policy (e.g., zeros or a placeholder).</p> <code>None</code> <p>Returns:</p> Type Description <code>Self</code> <p>A new state instance with input/output slots set.</p> Notes <p>This method must not mutate the receiver; it returns a new state object with the appropriate slots populated.</p>"},{"location":"reference/api/darnax/states/interface/#darnax.states.interface.State.replace","title":"<code>replace(value)</code>  <code>abstractmethod</code>","text":"<p>Return a new state with all underlying values replaced.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>PyTree</code> <p>A PyTree matching the internal storage structure.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>A new instance containing <code>value</code> as its storage.</p> Notes <p>This is the coarse-grained replacement primitive used by orchestrators when recomputing the entire state in one shot.</p>"},{"location":"reference/api/darnax/states/interface/#darnax.states.interface.State.replace_val","title":"<code>replace_val(idx, value)</code>  <code>abstractmethod</code>","text":"<p>Return a new state with a single slot replaced.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>Any</code> <p>Index/key indicating which slot to replace (commonly an <code>int</code>).</p> required <code>value</code> <code>Array</code> <p>New tensor to store at the selected position.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>A new instance where only the selected slot differs from <code>self</code>.</p> Notes <p>This is the fine-grained primitive used during step-wise updates (e.g., updating a single layer\u2019s activation while keeping the rest).</p>"},{"location":"reference/api/darnax/states/sequential/","title":"darnax.states.sequential","text":"<p>Sequential, array-backed global state.</p> <p><code>SequentialState</code> stores per-layer activations for layered networks as a list of JAX arrays and follows a strict indexing convention:</p> <ul> <li>index <code>0</code>: input buffer</li> <li>index <code>-1</code>: output buffer</li> <li>indices <code>1..-2</code>: intermediate layer buffers (left \u2192 right)</li> </ul> <p>Construction allocates placeholder buffers with batch size 1. Call :meth:<code>SequentialState.init</code> to resize all buffers to the true batch size and populate the endpoints. The object is an Equinox <code>Module</code> (a PyTree), so it plays nicely with JAX transforms; all \u201cupdates\u201d are functional (return a new instance) via :meth:<code>replace</code> and :meth:<code>replace_val</code>.</p>"},{"location":"reference/api/darnax/states/sequential/#darnax.states.sequential.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/states/sequential/#darnax.states.sequential.SequentialState","title":"<code>SequentialState(sizes, dtype=jnp.float32)</code>","text":"<p>               Bases: <code>State</code></p> <p>Sequential activations buffer for layered networks.</p> <p>Layers are indexed from left to right: <code>state[0]</code> is input and <code>state[-1]</code> is output. The internal storage is a list of arrays with shapes <code>(B, *size_l)</code> where <code>B</code> is the batch size and <code>*size_l</code> is the per-layer trailing shape.</p> <p>On construction, buffers are initialized as zeros with shape <code>(1, *size_l)</code> (a placeholder batch). Call :meth:<code>init</code> to resize all buffers to a real batch size and optionally set the output.</p> <p>Attributes:</p> Name Type Description <code>states</code> <code>list[Array]</code> <p>Per-layer buffers. Shapes are <code>(B, *size_l)</code> after :meth:<code>init</code>, or <code>(1, *size_l)</code> immediately after construction.</p> <code>dtype</code> <code>dtype</code> <p>Dtype for all buffers (marked static in the PyTree).</p> <code>data_min_ndim</code> <code>int</code> <p>Minimal rank for data buffers (default <code>2</code> \u2192 enforces a batch dim).</p> Notes <ul> <li>This class is functional: methods like :meth:<code>replace</code> and   :meth:<code>replace_val</code> return a new instance; the original is unchanged.</li> <li>Shape assertions in :meth:<code>init</code> are executed at trace time, so they are   checked once per JIT compilation.</li> <li>Nothing prevents you from storing non-floating dtypes, as long as they   are consistent with <code>dtype</code> at construction.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; st = SequentialState([4, 8, 2])        # input(4), hidden(8), output(2)\n&gt;&gt;&gt; st = st.init(jnp.zeros((32, 4)))       # B=32; output left as zeros\n&gt;&gt;&gt; x = st[0]                               # (32, 4)\n&gt;&gt;&gt; st2 = st.replace_val(1, jnp.ones((32, 8)))  # update hidden layer\n</code></pre> <p>Create a sequential state with one buffer per layer.</p> <p>Parameters:</p> Name Type Description Default <code>sizes</code> <code>Iterable[tuple[int, ...] | int]</code> <p>Iterable of positive sizes for each layer, including input and output. Each entry can be a positive <code>int</code> (for 1D layers) or a tuple of positive <code>int</code> (for multi-axis layers). Examples: <code>[D, N, C]</code> or <code>[(H, W, C_in), N, (C_out,)]</code>.</p> required <code>dtype</code> <code>dtype</code> <p>Dtype for all buffers. Default is <code>jnp.float32</code>.</p> <code>float32</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>sizes</code> is empty, or if any entry is not a positive integer or a tuple of positive integers.</p>"},{"location":"reference/api/darnax/states/sequential/#darnax.states.sequential.SequentialState.data_min_ndim","title":"<code>data_min_ndim = eqx.field(default=2, static=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/states/sequential/#darnax.states.sequential.SequentialState.dtype","title":"<code>dtype = dtype</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/states/sequential/#darnax.states.sequential.SequentialState.states","title":"<code>states = [(jnp.zeros((1, *size), dtype=dtype)) for size in shape_tuples]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/states/sequential/#darnax.states.sequential.SequentialState.replace","title":"<code>replace(value)</code>","text":"<p>Return a new instance with <code>states</code> replaced by <code>value</code>.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Sequence[Array] | PyTree</code> <p>A sequence (or PyTree) that will replace the internal list of buffers. In typical use, a list of arrays of length <code>len(self)</code>, each shaped <code>(B, *size_l)</code>.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>A new <code>SequentialState</code> carrying <code>value</code> as its storage.</p> Notes <p>This is a functional update; the original object is not mutated.</p>"},{"location":"reference/api/darnax/states/sequential/#darnax.states.sequential.SequentialState.replace_val","title":"<code>replace_val(idx, value)</code>","text":"<p>Return a new instance with layer <code>idx</code> replaced by <code>value</code>.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Layer index to modify.</p> required <code>value</code> <code>Array</code> <p>New buffer for that layer, typically with shape <code>(B, *size_idx)</code>.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>A new instance where only the selected layer differs from <code>self</code>.</p>"},{"location":"reference/api/darnax/states/sequential/#darnax.states.sequential.SequentialState.init","title":"<code>init(x, y=None)</code>","text":"<p>Resize buffers to batch <code>B</code> and set input (and optionally output).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input batch with shape <code>(B, *D)</code>; the trailing shape <code>*D</code> must equal the configured input layer shape.</p> required <code>y</code> <code>Array or None</code> <p>Optional output batch with shape <code>(B, *C)</code>; the trailing shape <code>*C</code> must equal the configured output layer shape. If omitted, the output buffer is zero-initialized.</p> <code>None</code> <p>Returns:</p> Type Description <code>Self</code> <p>A new instance whose buffers all have shape <code>(B, *size_l)</code>, with layer <code>0</code> set to <code>x</code> and (if provided) layer <code>-1</code> set to <code>y</code>.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If shapes are inconsistent (checked at trace time under JIT).</p> Notes <p>This method does not mutate the receiver; it constructs new buffers with batch size <code>B</code> and returns a fresh state.</p>"},{"location":"reference/api/darnax/utils/default_list/","title":"darnax.utils.default_list","text":"<p>Default-filling list registered as a JAX PyTree.</p> <p><code>DefaultList</code> behaves like a Python <code>list</code> but supports logical defaults: gaps created by assigning/inserting past the current end are represented by a sentinel and materialized only on read. This keeps storage compact while preserving where defaults were implied.</p> Features <ul> <li>Works like <code>collections.abc.MutableSequence</code> (indexing, slicing, insert, del).</li> <li>Extending past the end fills with a sentinel; the actual default value is   produced only when accessed (<code>__getitem__</code>/<code>__iter__</code>/<code>to_list</code>).</li> <li>Slicing returns another <code>DefaultList</code> that preserves sentinel slots.</li> <li>Registered as a JAX PyTree (<code>tree_flatten</code>/<code>tree_unflatten</code>).</li> </ul> Notes <ul> <li>If <code>default_factory</code> is provided, every read of a default slot creates   a fresh value (no per-slot caching).</li> <li>Leaves in the PyTree include the sentinel; JAX transforms that map over   leaves should tolerate non-numeric entries when defaults are present.</li> </ul>"},{"location":"reference/api/darnax/utils/default_list/#darnax.utils.default_list.T","title":"<code>T = TypeVar('T')</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/utils/default_list/#darnax.utils.default_list.DefaultList","title":"<code>DefaultList(initial=None, *, default=None, default_factory=None)</code>","text":"<p>               Bases: <code>MutableSequence[T | None]</code>, <code>Generic[T]</code></p> <p>Default-filling mutable list, registered as a JAX PyTree.</p> <p>Assigning or inserting beyond the current length fills gaps with a sentinel; the concrete default value is produced only when read. Slicing returns another :class:<code>DefaultList</code> that preserves sentinel slots.</p> <p>Parameters:</p> Name Type Description Default <code>initial</code> <code>Iterable[T | None]</code> <p>Concrete values to seed the list. Defaults are not inserted unless indices are explicitly extended by assignment/insert.</p> <code>None</code> <code>default</code> <code>T | None</code> <p>Value returned for a default slot if <code>default_factory</code> is not set.</p> <code>None</code> <code>default_factory</code> <code>Callable[[], T | None]</code> <p>Zero-arg callable producing the value for a default slot on read. Takes precedence over <code>default</code>. Each read yields a fresh value.</p> <code>None</code> Notes <p>The public element type is <code>T | None</code> because materialized defaults may legitimately be <code>None</code>.</p> <p>Create a :class:<code>DefaultList</code>.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>default_factory</code> is not callable.</p>"},{"location":"reference/api/darnax/utils/default_list/#darnax.utils.default_list.DefaultList.tree_flatten","title":"<code>tree_flatten()</code>","text":"<p>Return children and aux data for the JAX PyTree protocol.</p> <p>Returns:</p> Type Description <code>tuple[tuple[Any, ...], tuple[Any, ...]]</code> <p>Children are the raw underlying items (sentinels preserved). Aux data contains <code>(default, default_factory)</code>.</p>"},{"location":"reference/api/darnax/utils/default_list/#darnax.utils.default_list.DefaultList.tree_unflatten","title":"<code>tree_unflatten(aux, children)</code>  <code>classmethod</code>","text":"<p>Rebuild from aux and children (JAX PyTree protocol).</p> <p>Parameters:</p> Name Type Description Default <code>aux</code> <code>tuple[Any, ...]</code> <p>The auxiliary data returned by :meth:<code>tree_flatten</code>.</p> required <code>children</code> <code>tuple[Any, ...]</code> <p>The raw items (may include sentinels).</p> required <p>Returns:</p> Type Description <code>DefaultList[T]</code> <p>A reconstructed list with identical contents and defaults.</p>"},{"location":"reference/api/darnax/utils/default_list/#darnax.utils.default_list.DefaultList.insert","title":"<code>insert(index, value)</code>","text":"<p>Insert at <code>index</code>; if beyond end, fill gap with defaults then append.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Insertion position. Negative indices are clamped like <code>list.insert</code>.</p> required <code>value</code> <code>T | None</code> <p>Value to insert.</p> required"},{"location":"reference/api/darnax/utils/default_list/#darnax.utils.default_list.DefaultList.append","title":"<code>append(value)</code>","text":"<p>Append a value.</p>"},{"location":"reference/api/darnax/utils/default_list/#darnax.utils.default_list.DefaultList.extend","title":"<code>extend(values)</code>","text":"<p>Extend from an iterable.</p>"},{"location":"reference/api/darnax/utils/default_list/#darnax.utils.default_list.DefaultList.to_list","title":"<code>to_list(*, filter_defaults=False)</code>","text":"<p>Materialize to a plain Python list.</p> <p>Parameters:</p> Name Type Description Default <code>filter_defaults</code> <code>bool</code> <p>If <code>True</code>, drop default slots entirely (positions are lost). Otherwise, materialize defaults into concrete values.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[T | None]</code> <p>Either a list containing concrete values and materialized defaults, or (if <code>filter_defaults=True</code>) only the explicitly set values.</p> Notes <p>With <code>default_factory</code>, materialized defaults are freshly created and are not cached per-slot.</p>"},{"location":"reference/api/darnax/utils/perceptron_rule/","title":"darnax.utils.perceptron_rule","text":"<p>Perceptron (OVA) backward update in JAX.</p> <p>This module implements a one-vs-all perceptron-style local update that returns a weight increment <code>\u0394W</code> to be added to a dense weight matrix.</p> <ul> <li>Supports batched inputs.</li> <li>Labels are in <code>{-1, +1}</code> per class (one-vs-all coding).</li> <li><code>y_hat</code> are raw scores (pre-activation).</li> <li>No learning rate is applied here.</li> </ul> Shapes <p>x       : (d,) or (n, d) y       : (n, K) in {-1, +1} y_hat   : (n, K) raw scores margin  : broadcastable to (n, K) \u2014 e.g. scalar, (K,), or (n, K) returns : (d, K)  (\u0394W to add to weights)</p>"},{"location":"reference/api/darnax/utils/perceptron_rule/#darnax.utils.perceptron_rule.perceptron_rule_backward","title":"<code>perceptron_rule_backward(x, y, y_hat, margin)</code>","text":"<p>Multiclass (OVA) perceptron update (no learning rate).</p> <p>Applies an update when the margin condition is violated, i.e. <code>y * y_hat &lt;= margin</code> (ties count as mistakes). The result is a weight increment <code>\u0394W</code> aligned with a column-per-class layout.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input vector(s), shape <code>(d,)</code> or <code>(n, d)</code>.</p> required <code>y</code> <code>Array</code> <p>One-vs-all targets in <code>{-1, +1}</code>, shape <code>(n, K)</code>.</p> required <code>y_hat</code> <code>Array</code> <p>Raw scores (pre-activation), shape <code>(n, K)</code>.</p> required <code>margin</code> <code>Array</code> <p>Margin threshold, broadcastable to <code>(n, K)</code>; e.g. a scalar, a per-class vector <code>(K,)</code>, or an array <code>(n, K)</code>.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Weight update <code>\u0394W</code> of shape <code>(d, K)</code> to add to the weights.</p> Notes <ul> <li>Batch-size normalization: the update is divided by <code>n**0.5</code> so that its   magnitude is invariant to the batch size.</li> <li>Fan-in/width normalization (<code>1/sqrt(d)</code>) is applied here.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt; x = jnp.array([[1., 0.], [0., 1.]])      # (n=2, d=2)\n&gt;&gt;&gt; y = jnp.array([[+1, -1], [-1, +1]])      # (2, K=2)\n&gt;&gt;&gt; y_hat = jnp.array([[0.2, -0.3], [0.1, -0.4]])\n&gt;&gt;&gt; margin = 0.0\n&gt;&gt;&gt; dW = perceptron_rule_backward(x, y, y_hat, margin)\n&gt;&gt;&gt; dW.shape\n(2, 2)\n</code></pre>"},{"location":"reference/api/darnax/utils/typing/","title":"darnax.utils.typing","text":""},{"location":"reference/api/darnax/utils/typing/#darnax.utils.typing.PyTree","title":"<code>PyTree = Any</code>  <code>module-attribute</code>","text":""},{"location":"tutorials/","title":"Tutorials","text":"<p>This section provides useful tutorial to get started using this library, from JAX and Equinox basics to a first functioning example of a classifier on Entangled-MNIST.</p> <p>Models and concepts from this library are based on the paper by Badalotti, Baldassi, M\u00e9zard, Scardecchia and Zecchina. On the final tutorial, we will be training a 1-layer model on a simple benchmark dataset.</p> <p>The purpose of this library, however, is more general, as it allows for the quick and intuitive implementation of diverse architectures and learning schemes, loosely based on the principles of the original model.</p> <p>See the first tutorial to get started.</p> <p>For issues or questions, reach out to one of the Contributors.</p>"},{"location":"tutorials/01_jax_and_equinox/","title":"01 \u2014 JAX, Equinox, and Pytrees","text":"<p>This library is based on two foundational tools: JAX and Equinox. Understanding their philosophy is essential for working with this codebase. Both libraries share a unifying principle: everything is a pytree.</p> <p>This tutorial provides a concise introduction to these ideas. Readers are encouraged to consult the official documentation of JAX and Equinox for further technical details.</p>"},{"location":"tutorials/01_jax_and_equinox/#jax-transformations-of-numerical-functions","title":"JAX: Transformations of Numerical Functions","text":"<p>JAX extends NumPy with support for automatic differentiation and compilation. Its design emphasizes functional programming and composable transformations.</p> <p>Key concepts include:</p> <ul> <li>Array programming: JAX arrays follow NumPy semantics while executing efficiently on CPU, GPU, or TPU.</li> <li> <p>Transformations: JAX operates by transforming functions:</p> </li> <li> <p><code>jax.jit</code> compiles Python functions to optimized machine code.</p> </li> <li><code>jax.grad</code> computes derivatives automatically.</li> <li><code>jax.vmap</code> vectorizes functions across batch dimensions.</li> <li><code>jax.pmap</code> parallelizes computations across multiple devices.</li> <li>Composability: Transformations may be freely combined. For example, one may compute gradients of a JIT-compiled function or vectorize a function that already involves differentiation.</li> </ul> <p>The emphasis is not on predefined models or layers, but on transformations of user-defined functions.</p> <p>JAX gives the user a lot of powers, but this comes at a cost. If you're new to jax, be sure to read the Sharp bits.</p>"},{"location":"tutorials/01_jax_and_equinox/#equinox-pytrees-as-models","title":"Equinox: Pytrees as Models","text":"<p>Equinox is a lightweight neural network library for JAX. Its primary contribution is a consistent interface for defining models as plain Python classes.</p> <p>The design principles of Equinox are:</p> <ul> <li>Simplicity: Models are standard Python objects.</li> <li>Transparency: Parameters are stored directly as object attributes.</li> <li>Compatibility: Models are implemented as pytrees, allowing them to participate seamlessly in JAX transformations.</li> <li>Functional style: Updates to parameters or state return new objects, rather than mutating existing ones.</li> </ul> <p>This perspective aligns with the philosophy of this library: abstractions remain minimal, while full compatibility with JAX is preserved.</p>"},{"location":"tutorials/01_jax_and_equinox/#pytrees-a-unifying-abstraction","title":"Pytrees: A Unifying Abstraction","text":"<p>A pytree is a nested structure composed of Python containers (lists, tuples, dictionaries, dataclasses, and similar types) whose leaves are JAX arrays or compatible objects.</p> <p>Examples of pytrees:</p> <pre><code>import jax.numpy as jnp\n\n# Dictionary with arrays\nx = {\"a\": jnp.ones((2, 2)), \"b\": jnp.zeros((3,))}\n\n# Tuple of arrays\ny = (jnp.arange(3), jnp.ones((2,)))\n\n# Nested structures\nz = [x, y]\n</code></pre> <p>Pytrees are central to JAX for two reasons:</p> <ol> <li>They generalize beyond single arrays to arbitrary nested data structures.</li> <li>They allow JAX transformations to operate uniformly over these structures.</li> </ol> <p>In practice:</p> <ul> <li>A model is a pytree.</li> <li>Parameters and optimizer states are pytrees.</li> <li>Data batches may also be represented as pytrees.</li> </ul> <p>This single abstraction ensures that JAX transformations (such as <code>grad</code> or <code>jit</code>) can be applied consistently, regardless of structural complexity.</p>"},{"location":"tutorials/01_jax_and_equinox/#example-a-linear-module","title":"Example: A Linear Module","text":"<p>Equinox makes use of the pytree abstraction by treating models as pytrees.</p> <pre><code>import jax\nimport jax.numpy as jnp\nimport equinox as eqx\n\nclass Linear(eqx.Module):\n    weight: jax.Array\n    bias: jax.Array\n\n    def __init__(self, in_dim, out_dim, key):\n        wkey, bkey = jax.random.split(key)\n        self.weight = jax.random.normal(wkey, (in_dim, out_dim))\n        self.bias = jax.random.normal(bkey, (out_dim,))\n\n    def __call__(self, x):\n        return x @ self.weight + self.bias\n\n# Instantiate a model\nmodel = Linear(2, 3, jax.random.PRNGKey(0))\n\n# Forward evaluation\nx = jnp.ones((5, 2))\ny = model(x)\n\n# Differentiation\nloss_fn = lambda m, x: jnp.mean(m(x))\ngrads = jax.grad(loss_fn)(model, x)\n</code></pre> <p>Here, the model is both:</p> <ul> <li>a Python object with fields (<code>weight</code>, <code>bias</code>), and</li> <li>a pytree, enabling JAX to compute gradients and apply transformations directly.</li> </ul>"},{"location":"tutorials/01_jax_and_equinox/#philosophy-of-this-library","title":"Philosophy of This Library","text":"<p>The present library is guided by the following principles:</p> <ol> <li>Universality of pytrees: all major components (modules, layermaps, states) are structured as pytrees.</li> <li>Functional style: computations are expressed as pure functions, and updates return new objects.</li> <li>Composability: any component should be compatible with JAX transformations such as <code>jit</code>, <code>grad</code>, or <code>vmap</code>.</li> <li>Minimal abstraction: the library extends JAX and Equinox without concealing them. Users are encouraged to understand and directly employ these underlying tools.</li> </ol>"},{"location":"tutorials/01_jax_and_equinox/#next-tutorial","title":"Next Tutorial","text":"<p>The next tutorial will discuss the first component of this library: states and modules.</p>"},{"location":"tutorials/02_states/","title":"02 \u2014 States","text":"<p>This tutorial introduces States: the data structures that hold the current condition of the network. Unlike the transient activations of standard feedforward networks, states here are persistent, dynamical variables. They evolve iteratively under the network\u2019s dynamics and represent the system\u2019s position in configuration space, not merely intermediate results of a forward pass.</p>"},{"location":"tutorials/02_states/#1-conceptual-overview","title":"1. Conceptual overview","text":"<p>A state records the evolving configuration of the network at each layer. In the theoretical formulation, states are denoted</p> \\[ s^{(l)} \\in \\{\\pm 1\\}^N, \\] <p>but in this library they are general JAX arrays of arbitrary shape and dtype.</p> <p>Key points:</p> <ul> <li>Layer\u2013state relation. Every layer of the network is associated with a state buffer. This includes the input and output layers.</li> <li> <p>Initialization. When a batch <code>(x, y)</code> is presented:</p> </li> <li> <p>the input buffer is set to <code>x</code>,</p> </li> <li>the output buffer may be set to <code>y</code>,</li> <li>all intermediate buffers are initialized to zeros.</li> <li>Dynamics. States evolve by iterative updates until convergence (a fixed point or a steady regime). Later tutorials will explain these dynamics in detail.</li> <li>Shape generality. State buffers are not restricted to vectors. They may be multi-dimensional, e.g. <code>(H, W, C)</code> for convolutional or image-like architectures. The abstraction supports arbitrary shapes per layer.</li> </ul>"},{"location":"tutorials/02_states/#2-api-responsibilities","title":"2. API responsibilities","text":"<p>A state is deliberately minimal in API design but aligned with JAX\u2019s functional style. Each buffer is a JAX array, and the object provides simple functional accessors:</p> <pre><code>class State(eqx.Module):\n    def __getitem__(self, key: Any) -&gt; Array: ...\n    def init(self, x: Array, y: Array | None = None) -&gt; Self: ...\n    def replace(self, value: PyTree) -&gt; Self: ...\n    def replace_val(self, idx: Any, value: Array) -&gt; Self: ...\n</code></pre> <ul> <li> <p><code>__getitem__(key)</code>   Retrieves the buffer associated with <code>key</code> (e.g., layer index).</p> </li> <li> <p><code>init(x, y=None)</code>   Functionally initializes the state for a batch. Resizes all buffers to the batch dimension of <code>x</code>, sets the input to <code>x</code>, and optionally sets the output to <code>y</code>.</p> </li> <li> <p><code>replace(value)</code>   Returns a new state with the entire collection of buffers replaced by <code>value</code>.</p> </li> <li> <p><code>replace_val(idx, value)</code>   Returns a new state with only the buffer at position <code>idx</code> replaced by <code>value</code>.</p> </li> </ul> <p>All updates are functional and produce new objects. This immutability is essential for compatibility with JAX transformations.</p>"},{"location":"tutorials/02_states/#3-a-concrete-implementation-sequentialstate","title":"3. A concrete implementation: <code>SequentialState</code>","text":"<p><code>SequentialState</code> implements a left-to-right sequence of buffers, indexed by integers:</p> <ul> <li><code>0</code> = input buffer</li> <li><code>1, 2, \u2026, L-2</code> = intermediate buffers</li> <li><code>L-1</code> (or <code>-1</code>) = output buffer</li> </ul> <p>Internally, it stores a list of arrays with shapes <code>(B, *size_l)</code>, where <code>B</code> is the batch size.</p> <pre><code>state = SequentialState(sizes=[(4,), (8,), (3,)])\n</code></pre> <p>At construction time, buffers are initialized with dummy batch size <code>B=1</code>. The <code>init</code> method resizes them to the real batch size provided by <code>x</code>.</p>"},{"location":"tutorials/02_states/#example-initialization","title":"Example: initialization","text":"<pre><code>import jax\nimport jax.numpy as jnp\nfrom darnax.states.sequential import SequentialState\n\nstate = SequentialState(sizes=[(4,), (8,), (3,)])\n\nx = jax.random.normal(jax.random.PRNGKey(0), (32, 4))\ny = jax.random.normal(jax.random.PRNGKey(1), (32, 3))\n\ns0 = state.init(x, y)\nassert s0[0].shape == (32, 4)  # input\nassert s0[1].shape == (32, 8)  # hidden\nassert s0[-1].shape == (32, 3) # output\n</code></pre>"},{"location":"tutorials/02_states/#4-why-a-global-state","title":"4. Why a global state?","text":"<p>A deliberate design decision is to represent the state of the entire network globally, rather than letting each layer enclose its own state.</p> <p>The reasons are:</p> <ol> <li>Shared storage. Different layers may share access to portions of the same underlying state (e.g., in convolutional networks, multiple filters may act on overlapping regions of a global image-like buffer).</li> <li>Topological flexibility. A global state can be organized as a single multidimensional array or structured container, with different layers responsible for reading and writing specific slices.</li> <li>Consistency. This design allows heterogeneous architectures (dense, convolutional, graph-like) to operate on the same formal object without modifying the layer abstraction.</li> </ol> <p>This choice reflects the dynamical perspective: the network evolves as a whole, not as a collection of isolated layer-local states.</p>"},{"location":"tutorials/02_states/#5-states-as-pytrees","title":"5. States as pytrees","text":"<p>Because states are Equinox modules, they are pytrees. This has important consequences:</p> <ul> <li>JAX transformations (<code>jit</code>, <code>grad</code>, <code>vmap</code>, <code>pmap</code>) work seamlessly over state objects.</li> <li>Updates remain functional: replacing or modifying buffers yields new pytree instances.</li> <li>Static fields (e.g., <code>dtype</code>) are excluded from the dynamic leaves, reducing recompilation overhead.</li> </ul> <p>Thus, states are simultaneously containers of arrays and first-class JAX objects.</p>"},{"location":"tutorials/02_states/#6-summary","title":"6. Summary","text":"<ul> <li>States represent persistent network conditions, not just transient activations.</li> <li>Every layer is associated with part of the state, including input and output.</li> <li>States can store arbitrary shapes, enabling general architectures (vector-based, convolutional, etc.).</li> <li>The API is simple: indexing, functional initialization, and functional replacement.</li> <li>The design emphasizes a global state abstraction, allowing layers to share and reuse portions of it.</li> <li>Being pytrees, states integrate naturally with JAX\u2019s functional transformations.</li> </ul>"},{"location":"tutorials/03_modules/","title":"03 \u2014 Modules (Layers and Adapters)","text":"<p>This tutorial introduces Modules, the core building blocks of the library. Modules are divided into two categories:</p> <ul> <li>Layers: stateful modules that read and update a slice of the global network state.</li> <li>Adapters: stateless modules that transform one layer\u2019s state into a message for another.</li> </ul> <p>Only layers own a state. Both families may carry parameters; some are trainable (producing non-zero updates), others fixed (producing zero updates).</p>"},{"location":"tutorials/03_modules/#1-conceptual-background","title":"1. Conceptual background","text":"<p>The library is inspired by the recurrent dynamical model described in Dynamical Learning in Deep Asymmetric Recurrent Neural Networks. In that setting, each neuron (or unit) maintains a binary state \\(s \\in \\{-1, 1\\}\\), updated iteratively according to its local field:</p> \\[ s_i \\;\\leftarrow\\; \\operatorname{sign}\\!\\Big(\\sum_{j \\neq i} J_{ji}\\, s_j + J_D s_i \\Big). \\] <p>Extending to a multilayer chain, each layer \\(l\\) with state \\(s^{(l)}\\) also receives excitatory inputs from its neighbors with coupling \\(\\lambda\\):</p> \\[ s^{(l)}_i \\;\\leftarrow\\; \\operatorname{sign}\\!\\Big(\\sum_{j \\neq i} J^{(l)}_{ji}\\, s^{(l)}_j \\;+\\; J_D s^{(l)}_i \\;+\\; \\lambda (s^{(l-1)}_i + s^{(l+1)}_i) \\Big). \\] <p>Our modules provide a software abstraction of this process.</p> <ul> <li>Layers compute the recurrent/self contributions (\\(J\\), \\(J_D\\)) and handle aggregation + activation.</li> <li>Adapters contribute the cross-layer terms (e.g., \\(\\lambda s^{(l\\pm 1)}\\)).</li> </ul> <p>In other words, for a layer \\(l\\) with current state \\(s^{(l)}\\), a typical pre-activation is</p> \\[ h_i \\;=\\; \\underbrace{\\sum_{j \\neq i} J_{ji}\\, s_j + J_D s_i}_{\\text{layer self-message}} \\;+\\; \\underbrace{\\lambda\\, s_i^{(l-1)} + \\lambda\\, s_i^{(l+1)}}_{\\text{adapter messages}} \\] <p>and the new state is \\(s^{(l)} \\leftarrow \\text{activation}(h)\\).</p> <ul> <li>The self part is computed by the layer\u2019s <code>__call__</code>.</li> <li>The cross-layer parts are produced by adapters\u2019 <code>__call__</code> on neighboring (or otherwise connected) states.</li> <li>The layer\u2019s <code>reduce</code> performs the final aggregation into \\(h\\).</li> </ul>"},{"location":"tutorials/03_modules/#2-interfaces","title":"2. Interfaces","text":"<p>All modules extend a common base:</p> <pre><code>class AbstractModule(eqx.Module, ABC):\n\n    @property\n    @abstractmethod\n    def has_state(self) -&gt; bool: ...\n\n    @abstractmethod\n    def __call__(self, x: Array, rng: Array | None = None) -&gt; Array: ...\n\n    @abstractmethod\n    def backward(self, x: Array, y: Array, y_hat: Array) -&gt; Self: ...\n</code></pre> <ul> <li><code>__call__</code>: forward computation (a message).</li> <li><code>backward</code>: returns a module-shaped update. These are not gradients; they are local plasticity rules.</li> </ul>"},{"location":"tutorials/03_modules/#layers","title":"Layers","text":"<pre><code>class Layer(AbstractModule, ABC):\n\n    @property\n    def has_state(self) -&gt; bool: return True\n\n    @abstractmethod\n    def activation(self, x: Array) -&gt; Array: ...\n\n    @abstractmethod\n    def reduce(self, h: PyTree) -&gt; Array: ...\n</code></pre> <ul> <li><code>activation</code>: nonlinearity applied to the aggregated field.</li> <li><code>reduce</code>: combines incoming messages into a single tensor.</li> </ul>"},{"location":"tutorials/03_modules/#adapters","title":"Adapters","text":"<pre><code>class Adapter(AbstractModule, ABC):\n\n    @property\n    def has_state(self) -&gt; bool: return False\n</code></pre> <p>Adapters transform a source state into a message for another layer.</p>"},{"location":"tutorials/03_modules/#3-why-layer-states-are-messages","title":"3. Why layer states are \u201cmessages\u201d","text":"<p>In this architecture, a layer\u2019s current state is not just a transient activation, but the signal it emits to the rest of the network. Every update step consists of:</p> <ol> <li>Each layer publishing its state as a message.</li> <li>Adapters converting these messages into forms suitable for their targets.</li> <li>Layers aggregating self-messages and incoming adapter messages into \\(h\\).</li> <li>Layers applying their activation to obtain the new state.</li> </ol> <p>Thus, the global state itself is the medium of communication: states are messages.</p>"},{"location":"tutorials/03_modules/#4-example-modules","title":"4. Example modules","text":""},{"location":"tutorials/03_modules/#recurrentdiscrete","title":"RecurrentDiscrete","text":"<pre><code>class RecurrentDiscrete(Layer):\n    J: Array\n    J_D: Array\n    threshold: Array\n\n    def activation(self, x: Array) -&gt; Array:\n        return jnp.where(x &gt;= 0, 1, -1).astype(x.dtype)\n\n    def __call__(self, x: Array, rng=None) -&gt; Array:\n        return x @ self.J\n\n    def reduce(self, h: PyTree) -&gt; Array:\n        return jnp.asarray(tree_reduce(operator.add, h))\n\n    def backward(self, x: Array, y: Array, y_hat: Array) -&gt; Self:\n        dJ = perceptron_rule_backward(x, y, y_hat, self.threshold)\n        zero_update = jax.tree.map(jnp.zeros_like, self)\n        return eqx.tree_at(lambda m: m.J, zero_update, dJ)\n</code></pre> <ul> <li><code>__call__</code>: computes the recurrent/self-message.</li> <li><code>reduce</code>: aggregates all incoming messages.</li> <li><code>activation</code>: enforces \u00b11 states.</li> <li><code>backward</code>: computes a local perceptron-like rule to update \\(J\\).</li> </ul>"},{"location":"tutorials/03_modules/#ferromagnetic-adapter","title":"Ferromagnetic adapter","text":"<pre><code>class Ferromagnetic(Adapter):\n    strength: Array\n\n    def __call__(self, x: Array, rng=None) -&gt; Array:\n        return x * self.strength\n\n    def backward(self, x, y, y_hat) -&gt; Self:\n        return tree_map(jnp.zeros_like, self)\n</code></pre> <p>A fixed adapter implementing terms like \\(\\lambda s^{(l-1)}\\). Since it has no trainable parameters, the <code>backward</code> methods returns an array of zeros, meaning that the strength lambda remains unchanged.</p>"},{"location":"tutorials/03_modules/#5-one-update-step","title":"5. One update step","text":"<pre><code># state is a sequential state as shown in the first tutorial.\n# layer is our recurrent (or any other) implementation of the Layer\n# left and right are two adapters\n\ndef one_step(state, layer, left, right, l=1):\n    # we select the state of `layer`\n    s_l = state[l]\n    # we compute the self/recurrent update (message)\n    msg_self = layer(s_l)\n    # we compute the message from the left (s[l-1]) through the adapter\n    msg_l = left(state[l-1])\n    # we compute the message from the right (s[l+1]) through the adapter\n    msg_r = right(state[l+1])\n    # we aggregate the computed messages directed to the layer\n    h = layer.reduce([msg_self, msg_l, msg_r])\n    # we apply the non linearity to the result\n    s_next_l = layer.activation(h)\n    # we update the state at position l with the new value\n    return state.replace_val(l, s_next_l), h\n</code></pre>"},{"location":"tutorials/03_modules/#6-training-with-optax","title":"6. Training with Optax","text":"<p>Unlike gradient-based deep learning, learning here uses local updates. Each module\u2019s <code>backward</code> returns a module-shaped update, which we feed to Optax as if it were a gradient. This lets us retain momentum, schedules, clipping, etc., while remaining gradient-free.</p> <p>Be sure to check Optax documentation.</p> <pre><code>import optax\n\n# we define an optimizer such as adam\noptim = optax.adam(1e-2)\n\n# we use adam across our layer and adapters\nparams = (layer, left, right)\nopt_state = optim.init(eqx.filter(params, eqx.is_inexact_array))\n\ndef train_step(params, opt_state, state):\n    layer, left, right = params\n\n    # we compute the new state and preactivation as above\n    s1, h1 = one_step(state, layer, left, right)\n\n    # the backward functions compute updates of each layer\n    upd_layer = layer.backward(s1[1], s1[1], h1)\n    upd_left  = left.backward (s1[0], s1[0], h1)\n    upd_right = right.backward(s1[2], s1[2], h1)\n\n    # we use equinox to insert updates in the correct shapes\n    # this is a typical equinox/optax pattern\n    pseudo_grads = (upd_layer, upd_left, upd_right)\n    grads_f = eqx.filter(pseudo_grads, eqx.is_inexact_array)\n    params_f = eqx.filter(params, eqx.is_inexact_array)\n\n    # optax computes the updated parameters\n    updates, opt_state = optim.update(grads_f, opt_state, params=params_f)\n\n    # we apply them to our modules\n    params = eqx.apply_updates(params, updates)\n    return params, opt_state, s1\n</code></pre> <p>A training loop repeatedly calls <code>train_step</code>. Non-trainable adapters contribute zero updates, so they remain fixed. As said, we won't have to deal with the wiring of each layer directly. This will be handled by the orchestrator. Specifically, we will just need to call backward from the orchestrator to have a single update pytree of all modules in our network. Jax is totally transparent to the structure of our network, as long as it is a pytree.</p> <p>We provide a useful abstraction for this, see <code>LayerMap</code>.</p>"},{"location":"tutorials/03_modules/#7-multi-dimensional-states","title":"7. Multi-dimensional states","text":"<p>States can have arbitrary shapes: vectors <code>(B, F)</code> or image-like <code>(B, H, W, C)</code>. Layers and adapters generalize naturally as long as their operations are defined on those shapes. The global-state design also supports shared buffers, where multiple layers operate on slices of a single array.</p>"},{"location":"tutorials/03_modules/#8-summary","title":"8. Summary","text":"<ul> <li>Layers are stateful modules: self-message \u2192 reduce \u2192 activation.</li> <li>Adapters are stateless, transforming states into messages.</li> <li>States are messages: each layer\u2019s current state is the signal it emits.</li> <li>Backward rules: modules return structured updates, not gradients.</li> <li>Optax integration: these updates are fed as pseudo-gradients to Optax, enabling optimizer dynamics without autodiff.</li> <li>Global state: supports vector and multi-dimensional architectures with shared memory.</li> </ul> <p>This architecture mirrors the philosophy of distributed, local, gradient-free learning in asymmetric recurrent networks.</p>"},{"location":"tutorials/04_layermaps/","title":"04 \u2014 LayerMaps","text":"<p>A LayerMap is a PyTree wrapper that organizes the full set of layers and adapters in your network. It provides a consistent way to index them, guarantees immutability of the structure, and integrates seamlessly with JAX/Equinox/Optax.</p>"},{"location":"tutorials/04_layermaps/#1-matrix-view","title":"1. Matrix view","text":"<p>A LayerMap is conceptually a square matrix indexed by layer IDs:</p> <ul> <li>Diagonal <code>(i, i)</code>: the i-th layer. Each layer is stateful and sends a message to itself (its recurrent/self term).</li> <li>Off-diagonal <code>(i, j)</code> with <code>i \u2260 j</code>: an adapter. It converts the j-th state into a message for the i-th layer.</li> </ul> <p>This gives the following interpretation:</p> <ul> <li>Lower triangle (<code>i &gt; j</code>): forward adapters, messages flowing left \u2192 right.</li> <li>Upper triangle (<code>i &lt; j</code>): backward adapters, messages flowing right \u2192 left.</li> <li>Row <code>j</code>: everything that contributes into layer <code>j</code>.</li> <li>Column <code>i</code>: everything that originates from the state of layer <code>i</code>.</li> <li>Row 0: all connections going from layers to the input (unused for now).</li> <li>Row L: all connections going from layers to the output, meaning every layer whose state is used for prediction.</li> <li>Column 0: Forward skip connections from the input to each layer.</li> <li>Column L: Backward skip connections from the output to each layer.</li> </ul> <p>This matches the convention in <code>SequentialState</code>: <code>s[0]</code> is the input state, and <code>s[L]</code> is the output state.</p> <p>This structure, being in the end a dictionary of dictionaries is well suited for sparsity. For example, if layer <code>j</code> is not connected to layer <code>i</code>, there is no adapter in the <code>(i, j)</code> position. The element is simply not present in the structure, only the relevant modules are present.</p>"},{"location":"tutorials/04_layermaps/#2-api-overview","title":"2. API overview","text":"<pre><code>lm = LayerMap.from_dict({...})\n\nlm[i]: dict                        # read-only mapping of neighbors for row i\nlm[i, j]: Module                   # single module at (i, j)\n(i, j) in lm: bool                 # check if edge exists\n\nlm.rows(): tuple[int, ...]         # all row indices\nlm.cols_of(i): tuple[int, ...]     # all column indices in row i\nlm.neighbors(i): dict[int, Module] # read-only mapping {j: module} for row i\n\nlm.row_items(): Iterator[int, Module]                # iterate (row, neighbors)\nlm.edge_items(): Iterator[tuple[int , int], Module]  # iterate ((i, j), module)\nlm.to_dict(): dict[int, dict[int, Module]]           # copy as a dict-of-dicts\n</code></pre> <p>The API is intentionally dict-like but read-only: once built, the structure cannot be mutated. You cannot add layers or edges later.</p>"},{"location":"tutorials/04_layermaps/#3-immutability-of-structure","title":"3. Immutability of structure","text":"<p>A LayerMap is frozen once created:</p> <ul> <li>Row/column indices (the \u201cshape\u201d of the map) are part of the static treedef.</li> <li>Modules on each edge can change their parameters (via updates), but the adjacency cannot change.</li> </ul> <p>This immutability is not arbitrary. It is a basic requirement in JAX:</p> <ul> <li>The shape and structure of PyTrees must be static across JIT-compiled functions.</li> <li>If you were allowed to add or remove layers after creation, JIT cache keys would break and the compiled computation graph would need to be rebuilt every time.</li> <li>By freezing the structure, we ensure stability of compiled functions and allow the optimizer (Optax) to work on the entire network consistently.</li> </ul> <p>Thus, in JAX, data changes are dynamic, but structure is static.</p>"},{"location":"tutorials/04_layermaps/#4-example-building-a-simple-layermap","title":"4. Example: building a simple LayerMap","text":"<pre><code>import jax\nfrom darnax.modules.recurrent import RecurrentDiscrete\nfrom darnax.modules.adapters import Ferromagnetic\nfrom darnax.layer_maps.sparse import LayerMap\n\nkey0, key1 = jax.random.split(jax.random.PRNGKey(0))\nF = 8\n\n# Define two layers\nlayer0 = RecurrentDiscrete(features=F, j_d=0.0, threshold=0.0, key=key0)\nlayer1 = RecurrentDiscrete(features=F, j_d=0.0, threshold=0.0, key=key1)\n\n# Define adapters\nfwd_10 = Ferromagnetic(features=F, strength=0.5)  # forward (0 -&gt; 1)\nbwd_01 = Ferromagnetic(features=F, strength=0.2)  # backward (1 -&gt; 0)\n\n# Build dict-of-dicts\nraw = {\n    0: {0: layer0, 1: bwd_01},\n    1: {0: fwd_10, 1: layer1},\n}\n\nlm = LayerMap.from_dict(raw, require_diagonal=True)\n\n# Access\nprint(lm[1, 1])   # layer1\nprint(lm[1, 0])   # forward adapter\nprint(lm[1].keys())  # neighbors of row 1: {0, 1}\n</code></pre>"},{"location":"tutorials/04_layermaps/#5-a-layermap-as-a-pytree","title":"5. A LayerMap as a PyTree","text":"<p>Because <code>LayerMap</code> is registered as a PyTree:</p> <ul> <li>The keys (rows, columns) are static.</li> <li>The modules (layers/adapters) are leaves.</li> <li>Arrays inside those modules are visible to JAX and Optax.</li> </ul> <p>This means you can treat the entire network as a single object:</p> <pre><code>import equinox as eqx, optax\n\nopt = optax.adam(1e-2)\nopt_state = opt.init(eqx.filter(lm, eqx.is_inexact_array))\n\n# Later in training\nupdates, opt_state = opt.update(grads, opt_state, params=lm)\nlm = eqx.apply_updates(lm, updates)\n</code></pre> <p>All parameters inside all layers/adapters are updated in one go.</p>"},{"location":"tutorials/04_layermaps/#6-summary","title":"6. Summary","text":"<ul> <li>LayerMap = a collection of layers (diagonal) and adapters (off-diagonal) with integer keys.</li> <li>Matrix view: rows = inputs to a layer, columns = outputs from a layer.</li> <li>Input/output rows and columns handle special roles.</li> <li>Immutable structure: you cannot add or remove layers once built. This ensures JAX stability (PyTree structure must be static under JIT).</li> <li>PyTree integration: treat the whole network as one object, pass it to Equinox/Optax, and every parameter is handled correctly.</li> </ul> <p>This design makes LayerMap a central abstraction: a static graph of modules whose parameters evolve dynamically during training, while its topology remains fixed.</p>"},{"location":"tutorials/04_layermaps/#7-an-ascii-art","title":"7. An ascii art","text":"<pre><code>LayerMap (rows = receivers, columns = senders)\n\n            columns (senders: states/messages from j) \u2192\n            0          1          2        ...        L-1         L\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\nr    0   \u2502[L00]     [A01]     [A02\u2191]   \u2026            [A0,L-1]   [A0L\u2191]  \u2502\no        \u2502layer0    back      back                  back       back    \u2502\nw        \u2502(input)   adapters  adapters              adapters   adapters\u2502\ns        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n(    1   \u2502[A10\u2193]    [L11]     [A12\u2191]   \u2026            [A1,L-1]   [A1L]   \u2502\nr        \u2502fwd\u2192      layer1    \u2191back                 back       back    \u2502\ne        \u2502adapters            adapters              adapters   adapters\u2502\nc        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\ne    2   \u2502[A20\u2193]    [A21\u2193]    [L22]    \u2026            [A2,L-1\u2191]  [A2L\u2191]  \u2502\ni        \u2502fwd\u2192      fwd\u2192      layer2                \u2191 back     \u2191 back  \u2502\nv        \u2502adapters  adapters                        adapters   adapters\u2502\ne        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\nr    \u2026   \u2502\u2026          \u2026        \u2026        \u2026            \u2026          \u2026       \u2502\ns        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n     L   \u2502[AL0\u2193]    [AL1\u2193]    [AL2\u2193]   \u2026            [AL,L-1\u2193]  [LL]    \u2502\n         \u2502fwd\u2192      fwd\u2192      fwd\u2192                  fwd\u2192       \u2191layerL \u2502\n         \u2502adapters  adapters  adapters              adapters   (output)\u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2191\n           rows (receivers: layer i to be updated)\n</code></pre> <p>Legend:</p> <ul> <li>Lii   : layer on the diagonal (stateful). L00 is the input-layer slot; LL is the output-layer slot.</li> <li>Aij\u2193  : adapter at (i,j) with i &gt; j (lower triangle) \u2014 forward message (from j \u2192 i).</li> <li>Aij\u2191  : adapter at (i,j) with i &lt; j (upper triangle) \u2014 backward message (from j \u2192 i).</li> </ul> <p>Row/Column intuition:</p> <ul> <li>Row i collects everything needed to update layer i: the diagonal Lii (self-message) plus all Aij that transform state j into a message for i.</li> <li>Column j lists everything that uses state j as a source: the diagonal Ljj plus all Aij that send j\u2019s state to other layers.</li> </ul> <p>Input/Output:</p> <ul> <li>First column (\u00b7,0): forward skip connections from the input state to every layer.</li> <li>Last column (\u00b7,L): backward skip connections from the output state to earlier layers.</li> <li>Last row (L,\u00b7): all contributors that feed directly into the output layer (final prediction).</li> </ul> <p>Structure:</p> <ul> <li>Diagonal = layers; off-diagonal = adapters.</li> <li>The LayerMap\u2019s structure (rows/cols and which edges exist) is immutable after creation.</li> </ul>"},{"location":"tutorials/05_orchestrators/","title":"05 \u2014 Orchestrators","text":"<p>Having introduced states, modules, and layermaps, we now come to the final structural element: the orchestrator. Where the previous abstractions describe what the network is, the orchestrator specifies how the network evolves in time. It is the execution engine of the architecture: at each iteration it routes messages, applies updates, and advances the global state.</p>"},{"location":"tutorials/05_orchestrators/#1-conceptual-role","title":"1. Conceptual role","text":"<p>The orchestrator governs the dynamical process that characterizes this family of recurrent networks.</p> <p>As everything in the library, we propose a simple interface for the object, described below, and a first implementation. In this case, we focus on the <code>SequentialOrchestrator</code>, that plays nicely with sequential states and the sequential nature of the layer map. In detail, at each step, this specific orchestrator does the following:</p> <ol> <li>Message collection \u2014 for each receiving layer \\(i\\), all modules \\((i,j)\\) in the LayerMap are evaluated on the current sender states \\(s^{(j)}\\).</li> <li>Aggregation and activation \u2014 the diagonal module \\((i,i)\\) aggregates the incoming messages into a pre-activation \\(h^{(i)}\\) via its <code>reduce</code> method, then applies its nonlinearity with <code>activation</code>, yielding the new state slice \\(s^{(i)}\\).</li> <li>State update \u2014 the new slice replaces the old one in the global state, producing the next global configuration.</li> <li>Learning updates \u2014 in the training regime, each module also provides a local parameter update through its <code>backward(x, y, y_hat)</code> method. These are gathered into a LayerMap-structured PyTree of updates.</li> </ol> <p>In this way, the orchestrator realizes the iterative dynamics described in the paper: a distributed, gradient-free learning mechanism in which information is exchanged locally and parameters are updated via local rules.</p>"},{"location":"tutorials/05_orchestrators/#2-two-phases-of-dynamics","title":"2. Two phases of dynamics","text":"<p>In line with the two-phase protocol introduced in the theoretical model, the orchestrator provides two distinct update functions:</p> <ul> <li> <p>Training phase (<code>step</code>)   All available messages are considered, both forward (lower triangle) and backward (upper triangle). This corresponds to the supervised or clamped regime where input and output information are both present.</p> </li> <li> <p>Inference phase (<code>step_inference</code>)   Only causal messages are retained: for receiver \\(i\\), senders \\(j &lt; i\\) are discarded. Thus, information from the output or \u201cfuture\u201d layers does not leak backward. This corresponds to the free relaxation regime in which the system stabilizes autonomously.</p> </li> </ul> <p>This explicit separation ensures that training and inference dynamics are clearly distinguished in the implementation.</p>"},{"location":"tutorials/05_orchestrators/#3-public-api","title":"3. Public API","text":"<p>All orchestrators subclass the following abstract interface:</p> <pre><code>class AbstractOrchestrator(eqx.Module):\n    lmap: LayerMap  # fixed topology (rows, columns, edges)\n\n    def step(self, state: StateT, *, rng: KeyArray) -&gt; tuple[StateT, KeyArray]:\n        \"\"\"Run one full update step (training phase).\"\"\"\n\n    def step_inference(self, state: StateT, *, rng: KeyArray) -&gt; tuple[StateT, KeyArray]:\n        \"\"\"Run one update step (inference phase, discarding rightward messages).\"\"\"\n\n    def predict(self, state: SequentialState, rng: KeyArray) -&gt; tuple[SequentialState, KeyArray]:\n        \"\"\"Update the output state s[-1].\"\"\"\n\n    def backward(self, state: StateT, rng: KeyArray) -&gt; LayerMap:\n        \"\"\"Compute module-local updates in a LayerMap-structured PyTree.\"\"\"\n</code></pre>"},{"location":"tutorials/05_orchestrators/#stepstate-rng","title":"<code>step(state, rng)</code>","text":"<ul> <li>Executes one synchronous update of the network using all messages, both backward and forward.</li> <li>Returns the updated state and an advanced random key.</li> <li>It does not compute messages for the last row (messages going towards the output). So the last component of the state is never updated when applying <code>step</code>. There is a specific method called <code>predict</code> that needs to be run after the end of the dynamics if you want the actual prediction of the model.</li> </ul>"},{"location":"tutorials/05_orchestrators/#step_inferencestate-rng","title":"<code>step_inference(state, rng)</code>","text":"<ul> <li>Executes one update considering only messages from \\(j \\geq i\\).</li> <li>Used for prediction after training, ensuring purely causal message passing.</li> <li>As step, it does not compute messages for the last row (messages going towards the output). So the last component of the state is never updated when applying <code>step</code>. There is a specific method called <code>predict</code> that needs to be run after the end of the dynamics if you want the actual prediction of the model.</li> </ul>"},{"location":"tutorials/05_orchestrators/#predictstate-rng","title":"<code>predict(state, rng)</code>","text":"<ul> <li>Executes the update of the output state via usual forward + aggregation + activation. It is useful to check the implementation of <code>OutputLayer</code> to actually understand what is happening, as it is simply a sink that aggregates prediction from its neighboring adapters.</li> <li>Separating internal state update and final prediction allows to save some operations (computing the update of the output state at every step is useless), and also simplifies the overall API, see Tutorial 6.</li> </ul>"},{"location":"tutorials/05_orchestrators/#backwardstate-rng","title":"<code>backward(state, rng)</code>","text":"<ul> <li>For each edge \\((i,j)\\), invokes <code>lmap[i,j].backward(x=state[j], y=state[i], y_hat=local_field)</code> to obtain a module-shaped update.</li> <li>Returns a LayerMap with the same static structure as the original, but whose leaves are parameter updates.</li> <li>This PyTree can be passed directly to Optax as if it were a gradient structure.</li> </ul>"},{"location":"tutorials/05_orchestrators/#4-structural-properties","title":"4. Structural properties","text":"<ul> <li>Static topology: The orchestrator\u2019s LayerMap has a fixed set of rows, columns, and edges. This immutability is necessary for JAX compatibility, as PyTree structures must remain constant across compiled functions.</li> <li>Dynamic values: Within this static skeleton, the array values of module parameters evolve freely during training.</li> <li>PyTree compliance: Because the orchestrator itself is an Equinox module, it is also a PyTree. Its parameters can be filtered, updated, and optimized exactly like any other object in the system.</li> <li>Transformation compatibility: The orchestrator is fully compatible with <code>jax.jit</code>, <code>jax.vmap</code>, and all other JAX transformations. Since the topology is static, compilation is stable; only array values trigger recompilation when their shapes change.</li> </ul>"},{"location":"tutorials/05_orchestrators/#5-typical-usage","title":"5. Typical usage","text":"<p>A typical training loop involving an orchestrator proceeds as follows:</p> <pre><code># Forward update (training regime)\nstate, rng = orchestrator.step(state, rng=rng)\n\n# Forward update (inference regime)\nstate, rng = orchestrator.step_inference(state, rng=rng)\n\n# Compute module-local updates\nupd_lmap = orchestrator.backward(state, rng=rng)\n\n# Apply updates with Optax\ngrads  = eqx.filter(upd_lmap, eqx.is_inexact_array)\nparams = eqx.filter(orchestrator.lmap, eqx.is_inexact_array)\ndeltas, opt_state = opt.update(grads, opt_state, params=params)\nnew_lmap = eqx.apply_updates(orchestrator.lmap, deltas)\n\n# Replace the LayerMap inside the orchestrator\norchestrator = eqx.tree_at(lambda o: o.lmap, orchestrator, new_lmap)\n</code></pre> <p>Here the updates are not gradients: they are the outcome of local learning rules defined at the module level. Optax is used purely as a robust update engine.</p>"},{"location":"tutorials/05_orchestrators/#6-more-complex-logic","title":"6. More complex logic","text":"<p>Right until now we described a specific instance of orchestrator, i.e. the <code>SequentialOrchestrator</code>. It plays nicely with both the sequantial state and the sequential implementation of the layer map.</p> <p>The only assumption about this structures is a notion of order in the network, meaning that the first layer comes first, the second comes second, etc...</p> <p>This is totally arbitrary, this library allows for any structure and logic in the network functioning, a first example might be a totally synchronous network, where there is no notion of order and all layers are treated in sync. It is also possible to define a group structure, where different layers belong to different groups, each handled concurrently. This might involve an extension of the <code>LayerMap</code> to allow for string keys to identify groups...</p> <p>Another option is to specialize architectures for speed and efficiency. Instead of working with dict of dicts and simple loops, we might want to decide to pad and stack layer states together to be handled in parallel. This is also an easy extension of <code>LayerMap</code> and <code>Orchestrator</code>.</p> <p>We might even put orchestrators inside single modules, to encapsulate a into a single object complex logic.</p>"},{"location":"tutorials/05_orchestrators/#7-summary","title":"7. Summary","text":"<ul> <li>The orchestrator advances the network\u2019s dynamics by routing messages, aggregating them, and updating the global state.</li> <li><code>step</code> executes the full supervised/clamped update; <code>step_inference</code> executes the free, causal update.</li> <li><code>backward</code> collects local updates into a LayerMap-shaped PyTree, aligned with the parameter structure, enabling seamless integration with Optax.</li> <li>The orchestrator is a PyTree with static structure: immutable topology, mutable parameter values. This guarantees full compatibility with JAX transformations and ensures efficient compilation.</li> </ul> <p>Through the orchestrator, the network acquires its temporal dimension: states evolve, messages flow, and local rules drive learning, exactly as described in the underlying theoretical framework.</p>"},{"location":"tutorials/06_simple_net_on_artificial_data/","title":"06 \u2014 Simple Training Tutorial","text":"<p>This mini-notebook shows an end\u2011to\u2011end training loop. Notice that it resembles plain pytorch in the sense that you need to write your own train_Step and eval_step. We will see:</p> <ul> <li>How to define a <code>SequentialOrchestrator</code> over a sparse <code>LayerMap</code>.</li> <li>How to update the model via <code>.backward(...)</code> used as pseudo\u2011gradients for Optax.</li> <li>How to define train_steps, eval_steps and update_steps.</li> </ul> <p>The goal here is clarity: well\u2011ordered cells, consistent dtypes/PRNG usage, and inline comments explaining each step. We are not taking full advantage of jax for now. For example here we never call Jit on any function. Also, the dynamics <code>orchestrator.step/step_inference</code> is well suited for <code>jax.lax.scan</code>, but here we will just use a regular python-side for loop.</p> <pre><code># --- Imports ---------------------------------------------------------------\nimport time\nfrom typing import Any\n\nimport jax\nimport jax.numpy as jnp\nimport equinox as eqx\nimport optax\n\nfrom darnax.orchestrators.sequential import SequentialOrchestrator\nfrom darnax.modules.fully_connected import FrozenFullyConnected, FullyConnected\nfrom darnax.modules.recurrent import RecurrentDiscrete\nfrom darnax.modules.input_output import OutputLayer\nfrom darnax.layer_maps.sparse import LayerMap\nfrom darnax.states.sequential import SequentialState\n</code></pre>"},{"location":"tutorials/06_simple_net_on_artificial_data/#utilities-metrics-summaries","title":"Utilities (metrics &amp; summaries)","text":"<p>Small helpers used for monitoring. Labels are OVA \u00b11 and predictions are decoded via <code>argmax</code> over the output scores.</p> <pre><code>def batch_accuracy(y_true: jnp.ndarray, y_pred: jnp.ndarray) -&gt; float:\n    \"\"\"Accuracy with \u00b11 OVA labels (class = argmax along last dim).\"\"\"\n    y_true_idx = jnp.argmax(y_true, axis=-1)\n    y_pred_idx = jnp.argmax(y_pred, axis=-1)\n    return float(jnp.mean(y_true_idx == y_pred_idx))\n\n\ndef print_state_summary(s_out: jnp.ndarray, header: str = \"Output state\") -&gt; None:\n    \"\"\"Quick shape/range summary for an output buffer.\"\"\"\n    smin = float(jnp.min(s_out))\n    smax = float(jnp.max(s_out))\n    smu = float(jnp.mean(s_out))\n    sstd = float(jnp.std(s_out))\n    print(\n        f\"{header}: shape={tuple(s_out.shape)}, range=({smin:.3f},{smax:.3f}), mean={smu:.3f}, std={sstd:.3f}\"\n    )\n</code></pre>"},{"location":"tutorials/06_simple_net_on_artificial_data/#dataset-binary-prototypes","title":"Dataset: Binary Prototypes.","text":"<p>This object defines the data that we will be training our model on. It is a very simple task. The object is a compact generator that creates: - <code>K</code> prototype vectors in <code>{\u22121,+1}^D</code>. - A dataset of size <code>N</code> by corrupting prototypes with random bit-flips with probability <code>p_noise</code>. - \u00b11 labels (<code>+1</code> on the correct class, <code>\u22121</code> elsewhere), that indicate which prototype the data is coming from.</p> <pre><code>class PrototypeData:\n    \"\"\"Binary prototype dataset with OVA \u00b11 labels.\"\"\"\n\n    RAND_THRESHOLD = 0.5\n\n    def __init__(\n        self,\n        key: jax.Array,\n        batch_size: int = 16,\n        num_prototypes: int = 10,\n        dim_prototypes: int = 100,\n        num_data: int = 1000,\n        p_noise: float = 0.3,\n    ):\n        assert 0 &lt;= p_noise &lt; self.RAND_THRESHOLD, f\"Invalid {p_noise=}\"\n        assert num_data &gt;= num_prototypes, f\"Invalid {num_data=}, {num_prototypes=}\"\n        assert batch_size &gt; 1, f\"Invalid {batch_size=}\"\n        self.num_prototypes = int(num_prototypes)\n        self.dim_prototypes = int(dim_prototypes)\n        self.num_data = int(num_data)\n        self.p_noise = float(p_noise)\n        self.batch_size = int(batch_size)\n        self.num_batches = -(-self.num_data // self.batch_size)  # ceil division\n\n        key_prototypes, key_data = jax.random.split(key)\n        self._create_prototypes(key_prototypes)\n        self._create_data(key_data)\n\n    def __iter__(self):\n        \"\"\"Yield batches `(x, y)` as in the original implementation.\"\"\"\n        return zip(\n            jnp.array_split(self.x, self.num_batches),\n            jnp.array_split(self.y, self.num_batches),\n            strict=True,\n        )\n\n    def _create_prototypes(self, key: jax.Array) -&gt; None:\n        \"\"\"Generate \u00b11 prototypes (float32).\"\"\"\n        # Use rademacher \u2192 {\u22121,+1} with explicit float32 dtype.\n        self.prototypes = jax.random.rademacher(\n            key, shape=(self.num_prototypes, self.dim_prototypes), dtype=jnp.float32\n        )\n\n    def _create_data(self, key: jax.Array) -&gt; None:\n        \"\"\"Generate dataset by repeating prototypes and flipping signs with prob. `p_noise`.\"\"\"\n        # Build OVA labels: +1 on diag, \u22121 elsewhere, then repeat to length N.\n        self.y = jnp.full(\n            shape=(self.num_prototypes, self.num_prototypes), fill_value=-1.0, dtype=jnp.float32\n        )\n        self.y = self.y.at[jnp.diag_indices_from(self.y)].set(1.0)\n        self.y = jnp.repeat(\n            self.y,\n            self.num_data // self.num_prototypes + 1,\n            axis=0,\n            total_repeat_length=self.num_data,\n        )\n\n        # Repeat prototypes to length N, then flip signs with probability p_noise.\n        key, carry = jax.random.split(key)\n        self.x = jnp.repeat(\n            self.prototypes,\n            self.num_data // self.num_prototypes + 1,\n            axis=0,\n            total_repeat_length=self.num_data,\n        )\n        flip_mask = jax.random.bernoulli(carry, p=1 - self.p_noise, shape=self.x.shape) * 2.0 - 1.0\n        self.x = self.x * flip_mask\n        # Shuffle x and y in sync.\n        shuffle = jax.random.permutation(key, self.num_data)\n        self.x = self.x[shuffle].astype(jnp.float32)\n        self.y = self.y[shuffle].astype(jnp.float32)\n</code></pre>"},{"location":"tutorials/06_simple_net_on_artificial_data/#build-model-state","title":"Build Model &amp; State","text":"<p>Here we define a simple model with one hidden layer and fully-connected adapters, similar to a perceptron with one hidden layer, except that the hidden layer has internal recurrency.</p> <p>The topology of the layer map is the following: - Layer 1 (hidden) receives from input (0, forth), itself (1, recurrent), and labels (2, back). - Layer 2 (output) receives from hidden (1) and itself (2).</p> <p>Some first comments:</p> <ul> <li>Notice that we do not define a input layer, that would correspond to layer 0 in the receivers. This is totally fine and intended. The input in this case is simply a sent message and never updated. If we dont define layer 0 in the receivers, the first component of the state is fixed.</li> <li>You should inspect the implementation of the OutputLayer. It does not have an internal state, parameters, and the <code>__call__</code> function returns an array of zeros. It is basically a sink that aggregates messages from all layers that contribute to the output and sums them. This behaviour can change in the future with the definition of new OutputLayers with a more complex logic, but for now it is basically an aggregator.</li> </ul> <pre><code>DIM_DATA = 100\nNUM_DATA = 1000\nNUM_LABELS = 10\nDIM_HIDDEN = 256\nTHRESHOLD_OUT = 3.5\nTHRESHOLD_IN = 3.5\nTHRESHOLD_J = 0.5\nSTRENGTH_BACK = 0.3\nSTRENGTH_FORTH = 1.0\nJ_D = 0.5\n\n# Global state with three buffers: input (0), hidden (1), output/labels (2)\nstate = SequentialState((DIM_DATA, DIM_HIDDEN, NUM_LABELS))\n\n# Distinct keys per module to avoid accidental correlations.\nmaster_key = jax.random.key(seed=44)\nkeys = jax.random.split(master_key, num=5)\n\nlayer_map = {\n    # Hidden row (1): from input (0), self (1), and labels (2)\n    1: {\n        0: FullyConnected(\n            in_features=DIM_DATA,\n            out_features=DIM_HIDDEN,\n            strength=STRENGTH_FORTH,\n            threshold=THRESHOLD_IN,\n            key=keys[0],\n        ),\n        1: RecurrentDiscrete(features=DIM_HIDDEN, j_d=J_D, threshold=THRESHOLD_J, key=keys[1]),\n        2: FrozenFullyConnected(\n            in_features=NUM_LABELS,\n            out_features=DIM_HIDDEN,\n            strength=STRENGTH_BACK,\n            threshold=0.0,\n            key=keys[2],\n        ),\n    },\n    # Output row (2): from hidden (1), and itself (2)\n    2: {\n        1: FullyConnected(\n            in_features=DIM_HIDDEN,\n            out_features=NUM_LABELS,\n            strength=1.0,\n            threshold=THRESHOLD_OUT,\n            key=keys[3],\n        ),\n        2: OutputLayer(),  # the 2-2 __call__ is a vector of zeros, it does not contribute\n    },\n}\nlayer_map = LayerMap.from_dict(layer_map)\n\n# Trainable orchestrator built from the fixed topology.\norchestrator = SequentialOrchestrator(layers=layer_map)\n</code></pre>"},{"location":"tutorials/06_simple_net_on_artificial_data/#optimizer","title":"Optimizer","text":"<p>We choose Adam with <code>lr=5e-3</code>. As you can see we can call <code>eqx.filter</code> directly on the orchestrator, since it is a pytree. We also show how to update the model with the function <code>update(orchestrator, state, optimizer)</code>.</p> <pre><code>optimizer = optax.adam(5e-3)\nopt_state = optimizer.init(eqx.filter(orchestrator, eqx.is_inexact_array))\n\n\ndef update(\n    orchestrator: SequentialOrchestrator, state: SequentialState, optimizer, optimizer_state, rng\n) -&gt; tuple[SequentialOrchestrator, Any]:\n    \"\"\"Compute and applies the updates and returns the updated model.\n\n    It also returns the optimizer state, typed as Any for now.\n    \"\"\"\n    # 1) Local deltas (orchestrator-shaped deltas).\n    grads = orchestrator.backward(state, rng=rng)\n\n    # 2) Optax over the orchestrator (tree structures match by construction).\n    # This is common equinox + optax pattern, used in the same way when training\n    # \"regular\" deep learning models.\n    # First we filter fields with equinox and then we give them to the optimizer.\n    # This allows us to handle complex pytrees with static fields seamlessly during\n    # training.\n    params = eqx.filter(orchestrator, eqx.is_inexact_array)\n    grads = eqx.filter(grads, eqx.is_inexact_array)\n\n    # 3) We compute the updates and apply them to our model\n    updates, opt_state = optimizer.update(grads, optimizer_state, params=params)\n    orchestrator = eqx.apply_updates(orchestrator, updates)\n\n    # 4) We return the updated model and the optimizer state\n    return orchestrator, opt_state\n</code></pre>"},{"location":"tutorials/06_simple_net_on_artificial_data/#dynamics-helpers","title":"Dynamics helpers","text":"<p>We now define two simple functions that run during training, they are extremely simple. During run_dynamics_training we have two phases: a first one where we compute all messages and run the dynamics with both forward and backward messages. We run this phase for a fixed number of steps. Then, we do a second phase where we suppress all messages \"going backward\", we run this dynamics for a fixed number of steps and we obtain a second fixed point s^*.</p> <p>During inference, we only run the dynamics with suppressed messages from the right.</p> <pre><code>def run_dynamics_training(\n    orch: SequentialOrchestrator,\n    s,\n    rng: jax.Array,\n    steps: int,\n):\n    \"\"\"Run `steps` iterations using ALL messages (clamped phase).\n\n    Note: Kept identical to your working version for consistency.\n    \"\"\"\n    for _ in range(steps):\n        s, rng = orch.step(s, rng=rng)\n    for _ in range(steps):\n        s, rng = orch.step_inference(s, rng=rng)\n    return s, rng\n\n\ndef run_dynamics_inference(\n    orch: SequentialOrchestrator,\n    s,\n    rng: jax.Array,\n    steps: int,\n):\n    \"\"\"Run `steps` iterations discarding rightward/backward messages (free phase).\"\"\"\n    for _ in range(steps):\n        s, rng = orch.step_inference(s, rng=rng)\n    return s, rng\n</code></pre>"},{"location":"tutorials/06_simple_net_on_artificial_data/#train-step","title":"Train step","text":"<p>This function summarizes the whole training protocol, for a single batch.</p> <p>Protocol per batch: 1. Initialize/clamp the global state with <code>(x, y)</code>. 2. Run training dynamics for <code>2 * T_train</code> steps. 3. Update the model with <code>update</code>, as seen before.</p> <pre><code>def train_step(\n    orch: SequentialOrchestrator,\n    s: SequentialState,\n    x: jnp.ndarray,\n    y: jnp.ndarray,\n    rng: jax.Array,\n    *,\n    opt_state,\n    optimizer,\n    t_train: int = 3,\n):\n    # 1) Clamp current batch (inputs &amp; labels).\n    s = s.init(x, y)\n\n    # 2) Training dynamics (kept as-is).\n    s, rng = run_dynamics_training(orch, s, rng, steps=t_train)\n\n    # 3) Update the model\n    rng, update_key = jax.random.split(rng)\n    orch, opt_state = update(orch, s, optimizer, opt_state, update_key)\n    return orch, rng, opt_state\n</code></pre>"},{"location":"tutorials/06_simple_net_on_artificial_data/#eval-step","title":"Eval step","text":"<p>Initializes with inputs only (labels are just for metrics), then runs the inference dynamics and computes metrics.</p> <pre><code>def eval_step(\n    orch: SequentialOrchestrator,\n    s: SequentialState,\n    x: jnp.ndarray,\n    y: jnp.ndarray,\n    rng: jax.Array,\n    *,\n    t_eval: int = 5,\n) -&gt; tuple[SequentialOrchestrator, SequentialState, dict, jax.Array]:\n    s = s.init(x, None)\n    s, rng = run_dynamics_inference(orch, s, rng, steps=t_eval)\n    s, rng = orchestrator.predict(s, rng)\n    y_pred = s[-1]\n    metrics = {\"acc\": batch_accuracy(y, y_pred)}\n    return metrics, rng\n</code></pre>"},{"location":"tutorials/06_simple_net_on_artificial_data/#run-the-training","title":"Run the training","text":"<p>Finally, we build a small <code>PrototypeData</code> stream, train for a few epochs using <code>train_step</code> per batch, and evaluate on the same stream.</p> <pre><code># Constants\nP_NOISE = 0.3\nBATCH_SIZE = 16\nEPOCHS = 3\nT_TRAIN = 10  # training dynamics steps per batch\nT_EVAL = 10  # short inference steps for monitoring\n\n# RNGs\nmaster_key = jax.random.key(59)\nmaster_key, data_key = jax.random.split(master_key)\n\n# Data\ndata = PrototypeData(\n    key=data_key,\n    batch_size=BATCH_SIZE,\n    num_prototypes=NUM_LABELS,\n    dim_prototypes=DIM_DATA,\n    num_data=NUM_DATA,\n    p_noise=P_NOISE,\n)\nprint(f\"Dataset: x.shape={tuple(data.x.shape)}  y.shape={tuple(data.y.shape)}\")\n\n# Training config\n\nhistory = {\"acc\": []}\n\nfor epoch in range(1, EPOCHS + 1):\n    t0 = time.time()\n    print(f\"\\n=== Epoch {epoch}/{EPOCHS} ===\")\n    for x_batch, y_batch in data:\n        # Keep batch arrays float32 (consistency)\n        master_key, train_key, eval_key = jax.random.split(master_key, num=3)\n        orchestrator, master_key, opt_state = train_step(\n            orchestrator,\n            state,\n            x_batch,\n            y_batch,\n            rng=train_key,\n            opt_state=opt_state,\n            optimizer=optimizer,\n            t_train=T_TRAIN,\n        )\n        metrics, rng = eval_step(orchestrator, state, x_batch, y_batch, eval_key)\n    history[\"acc\"].append(metrics[\"acc\"])\n    print(f\"Epoch {epoch} done in {time.time()-t0:.2f}s\")\n    print(f\"Mean accuracy={float(jnp.mean(jnp.array(history['acc']))):.3f}\")\n</code></pre>"},{"location":"tutorials/06_simple_net_on_artificial_data/#final-evaluation-demo","title":"Final evaluation (demo)","text":"<p>Single pass over the same iterator; replace with a held\u2011out set in practice.</p> <pre><code>eval_acc = []\nfor x_b, y_b in data:\n    x_batch = x_b.astype(jnp.float32)\n    y_batch = y_b.astype(jnp.float32)\n    master_key, step_key = jax.random.split(master_key)\n    metrics, master_key = eval_step(\n        orchestrator,\n        state,\n        x_batch,\n        y_batch,\n        rng=step_key,\n        t_eval=T_EVAL,\n    )\n    eval_acc.append(metrics[\"acc\"])\n\nprint(\"\\n=== Final evaluation summary ===\")\nprint(f\"Accuracy={float(jnp.mean(jnp.array(eval_acc))):.3f}\")\n</code></pre>"},{"location":"tutorials/07_optimized_training_on_mnist/","title":"07 \u2014 Training on MNIST with darnax (JAX-first, CPU)","text":"<p>This tutorial is for readers who want to understand how darnax is used in practice: how a network is assembled from modules, how the orchestrator runs recurrent dynamics, and how local plasticity integrates with Equinox/Optax to update parameters without backpropagation.</p> <p>We\u2019ll implement a compact, JAX-friendly training loop:</p> <ul> <li>Only the outer steps are <code>jit</code>-compiled: <code>train_step</code>, <code>eval_step</code>.</li> <li>Recurrent dynamics use <code>jax.lax.scan</code> (no Python loops inside compiled code).</li> <li>The dataset iterator is slice-based (avoid <code>array_split</code>), better for accelerators and CPUs.</li> <li>We stay on CPU to keep the focus on design; the code is accelerator-ready.</li> </ul> <pre><code># --- Imports ---------------------------------------------------------------\nfrom __future__ import annotations\n\nimport logging\nimport time\nfrom typing import TYPE_CHECKING\n\nimport equinox as eqx\nimport jax\nimport jax.numpy as jnp\nimport optax\nfrom datasets import load_dataset\n\nfrom darnax.layer_maps.sparse import LayerMap\nfrom darnax.modules.fully_connected import FrozenFullyConnected, FullyConnected\nfrom darnax.modules.input_output import OutputLayer\nfrom darnax.modules.recurrent import RecurrentDiscrete\nfrom darnax.orchestrators.sequential import SequentialOrchestrator\nfrom darnax.states.sequential import SequentialState\n\nif TYPE_CHECKING:\n    from collections.abc import Iterator\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\n</code></pre>"},{"location":"tutorials/07_optimized_training_on_mnist/#1-what-darnax-gives-you","title":"1) What darnax gives you","text":"<p>darnax decomposes a model into modules connected by a LayerMap. At runtime, an orchestrator applies a message-passing schedule over a state vector.</p> <ul> <li>Modules (edges/diagonals) consume a sender buffer and emit a message to a receiver.   Some modules are trainable; some are frozen; some are \u201cdiagonal\u201d (operate on a buffer itself).</li> <li>The LayerMap defines the fixed topology: for each receiver row <code>i</code>, which senders <code>j</code>   contribute, and with which module.</li> <li>The SequentialOrchestrator drives the update order (left\u2192right, recurrent self, right\u2192left   as needed) and exposes:</li> <li><code>step</code>: full dynamics (all messages allowed).</li> <li><code>step_inference</code>: inference dynamics (typically suppress \u201cbackward\u201d messages).</li> <li><code>backward</code>: compute local parameter deltas from the current state (no backprop).</li> <li><code>predict</code>: produce output scores in the final buffer.</li> <li>The State is a fixed-shape tuple of buffers <code>(input, hidden, output)</code>. You clamp   inputs (and possibly labels) by writing them into the state, then run dynamics to a fixed point.</li> </ul>"},{"location":"tutorials/07_optimized_training_on_mnist/#2-a-tiny-metric-helper","title":"2) A tiny metric helper","text":"<p>Labels are One-Vs-All (OVA) in \u00b11. We decode predictions via <code>argmax</code>.</p> <pre><code>def batch_accuracy(y_true: jnp.ndarray, y_pred: jnp.ndarray) -&gt; float:\n    \"\"\"Accuracy with \u00b11 OVA labels (class = argmax along last dim).\"\"\"\n    y_true_idx = jnp.argmax(y_true, axis=-1)\n    y_pred_idx = jnp.argmax(y_pred, axis=-1)\n    return jnp.mean((y_true_idx == y_pred_idx).astype(jnp.float32))\n</code></pre>"},{"location":"tutorials/07_optimized_training_on_mnist/#3-dataset-object-designed-for-jax","title":"3) Dataset object designed for JAX","text":"<p>We keep the data pipeline deliberately simple to highlight the model mechanics:</p> <ul> <li>Materialize once: training subset and full test split live as device arrays.</li> <li>Shared projection: an optional linear projection (same matrix for train/test) reduces   dimensionality and can be followed by a <code>sign</code> transform (Entangled-MNIST style).</li> <li>Slice-based batches: iterators yield contiguous chunks; no list materialization or   <code>array_split</code>.</li> <li>Label scaling (your original choice):</li> <li>true class \u2192 <code>+\u221aC / 2</code></li> <li>others \u2192 <code>\u22120.5</code></li> </ul> <p>This scaling biases the output field to favor the target class during clamped dynamics.</p> <pre><code>class MNISTData:\n    \"\"\"MNIST dataset with optional linear projection and sign; slice-based iterators.\n\n    Design:\n      - Single in-memory materialization (train subset, full test).\n      - Shared projection across splits.\n      - Deterministic batch slicing (precomputed ranges).\n    \"\"\"\n\n    TOTAL_SIZE_PER_CLASS = 5900  # train split\n    TEST_SIZE_PER_CLASS = 1000  # test split\n    NUM_CLASSES = 10\n    FLAT_DIM = 28 * 28\n\n    def __init__(\n        self,\n        key: jax.Array,\n        batch_size: int = 64,\n        linear_projection: int | None = 100,\n        apply_sign_transform: bool = True,\n        num_images_per_class: int = TOTAL_SIZE_PER_CLASS,\n    ):\n        \"\"\"Initialize the dataset object.\"\"\"\n        # Lightweight validation; fail fast on easy mistakes.\n        if not (linear_projection is None or isinstance(linear_projection, int)):\n            raise TypeError(\"`linear_projection` must be `None` or `int`.\")\n        if batch_size &lt;= 1:\n            raise ValueError(f\"Invalid batch_size={batch_size!r}; must be &gt; 1.\")\n        if not (0 &lt; num_images_per_class &lt;= self.TOTAL_SIZE_PER_CLASS):\n            raise ValueError(f\"`num_images_per_class` must be in [1, {self.TOTAL_SIZE_PER_CLASS}]\")\n\n        self.linear_projection = linear_projection\n        self.apply_sign_transform = bool(apply_sign_transform)\n\n        self.num_data = int(num_images_per_class) * self.NUM_CLASSES\n        self.batch_size = int(batch_size)\n        self.num_batches = -(-self.num_data // self.batch_size)  # ceil div\n\n        # Build arrays once.\n        self._create_dataset(key)\n\n        # Precompute slicing ranges for train/eval.\n        self._train_bounds = [\n            (i * self.batch_size, min((i + 1) * self.batch_size, self.num_data))\n            for i in range(self.num_batches)\n        ]\n        self.num_eval_data = int(self.x_eval.shape[0])\n        self.num_eval_batches = -(-self.num_eval_data // self.batch_size)\n        self._eval_bounds = [\n            (i * self.batch_size, min((i + 1) * self.batch_size, self.num_eval_data))\n            for i in range(self.num_eval_batches)\n        ]\n\n    # ------------------------------- Public API ------------------------------- #\n    def __iter__(self) -&gt; Iterator[tuple[jax.Array, jax.Array]]:\n        \"\"\"Yield `(x, y)` training batches by contiguous slicing.\"\"\"\n        for lo, hi in self._train_bounds:\n            yield self.x[lo:hi], self.y[lo:hi]\n\n    def iter_eval(self) -&gt; Iterator[tuple[jax.Array, jax.Array]]:\n        \"\"\"Yield `(x_eval, y_eval)` validation batches (full test split).\"\"\"\n        for lo, hi in self._eval_bounds:\n            yield self.x_eval[lo:hi], self.y_eval[lo:hi]\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the number of batches.\"\"\"\n        return self.num_batches\n\n    # ------------------------------ Internals ------------------------------ #\n    @staticmethod\n    def _load_mnist_split(split: str) -&gt; tuple[jax.Array, jax.Array]:\n        \"\"\"Load MNIST split and flatten to (N, 784).\"\"\"\n        assert split in [\"train\", \"test\"]\n        ds = load_dataset(\"mnist\")\n        x = jnp.asarray([jnp.array(im) for im in ds[split][\"image\"]], dtype=jnp.float32)\n        y = jnp.asarray(ds[split][\"label\"], dtype=jnp.int32)\n        x = x.reshape(x.shape[0], -1) / 255.0\n        return x, y\n\n    @staticmethod\n    def _labels_to_pm1_scaled(y_scalar: jax.Array, num_classes: int) -&gt; jax.Array:\n        \"\"\"Original scaling: +\u221aC/2 at the true class, \u22120.5 elsewhere.\"\"\"\n        one_hot = jax.nn.one_hot(y_scalar, num_classes, dtype=jnp.float32)\n        return one_hot * (num_classes**0.5 / 2.0) - 0.5\n\n    @staticmethod\n    def _random_projection_matrix(key: jax.Array, out_dim: int, in_dim: int) -&gt; jax.Array:\n        \"\"\"Gaussian projection with variance 1/in_dim to keep outputs ~unit variance.\"\"\"\n        return jax.random.normal(key, (out_dim, in_dim), dtype=jnp.float32) / jnp.sqrt(in_dim)\n\n    @staticmethod\n    def _take_per_class(\n        key: jax.Array, x: jax.Array, y: jax.Array, k_per_class: int\n    ) -&gt; tuple[jax.Array, jax.Array]:\n        \"\"\"Uniformly sample `k_per_class` examples for each class 0..9.\"\"\"\n        xs, ys = [], []\n        for cls in range(MNISTData.NUM_CLASSES):\n            key, sub = jax.random.split(key)\n            idx = jnp.where(y == cls)[0]\n            if k_per_class &gt; idx.shape[0]:\n                raise ValueError(\n                    f\"Requested {k_per_class} for class {cls}, but only {idx.shape[0]} available.\"\n                )\n            perm = jax.random.permutation(sub, idx.shape[0])\n            take = idx[perm[:k_per_class]]\n            xs.append(x[take])\n            ys.append(y[take])\n        return jnp.concatenate(xs, axis=0), jnp.concatenate(ys, axis=0)\n\n    def _maybe_project_and_sign(self, w: jax.Array | None, x: jax.Array) -&gt; jax.Array:\n        \"\"\"Apply optional linear projection + optional sign nonlinearity (Entangled-MNIST).\"\"\"\n        if w is not None:\n            x = (x @ w.T).astype(jnp.float32)\n        if self.apply_sign_transform:\n            sgn = jnp.sign(x)\n            x = jnp.where(sgn == 0, jnp.array(-1.0, dtype=sgn.dtype), sgn)\n        return x\n\n    def _create_dataset(self, key: jax.Array) -&gt; None:\n        \"\"\"Materialize train subset and full test split with consistent preprocessing.\"\"\"\n        key_sample, key_proj, key_shuf_tr = jax.random.split(key, 3)\n\n        # Load raw splits.\n        x_tr_all, y_tr_all = self._load_mnist_split(\"train\")\n        x_ev_all, y_ev_scalar = self._load_mnist_split(\"test\")\n\n        # Uniform per-class sampling.\n        k_train = self.num_data // self.NUM_CLASSES\n        x_tr, y_tr_scalar = self._take_per_class(key_sample, x_tr_all, y_tr_all, k_train)\n\n        # Shared projection/sign across splits.\n        w = (\n            self._random_projection_matrix(key_proj, int(self.linear_projection), x_tr.shape[-1])\n            if self.linear_projection is not None\n            else None\n        )\n        x_tr = self._maybe_project_and_sign(w, x_tr)\n        x_ev = self._maybe_project_and_sign(w, x_ev_all)\n\n        # Labels with original scaling; shuffle train only.\n        y_tr = self._labels_to_pm1_scaled(y_tr_scalar, self.NUM_CLASSES)\n        perm_tr = jax.random.permutation(key_shuf_tr, x_tr.shape[0])\n        self.x, self.y = x_tr[perm_tr], y_tr[perm_tr]\n\n        self.x_eval = x_ev\n        self.y_eval = self._labels_to_pm1_scaled(y_ev_scalar, self.NUM_CLASSES)\n\n        # Convenience metadata.\n        self.input_dim = int(self.x.shape[1])\n</code></pre>"},{"location":"tutorials/07_optimized_training_on_mnist/#4-model-topology-as-a-layermap","title":"4) Model topology as a LayerMap","text":"<p>We build a minimal network with one hidden layer and an output sink:</p> <ul> <li>Receiver row 1 (hidden) gets messages from:</li> <li>0 (input) via <code>FullyConnected</code> (forward path)</li> <li>1 (itself) via <code>RecurrentDiscrete</code> (internal recurrency)</li> <li>2 (labels) via <code>FrozenFullyConnected</code> (backward/clamping path)</li> <li>Receiver row 2 (output) gets:</li> <li>1 (hidden) via <code>FullyConnected</code> (readout)</li> <li>2 (itself) via <code>OutputLayer</code> (diagonal sink/aggregator, returns zeros)</li> </ul> <p>The SequentialState is <code>(input, hidden, output)</code> with fixed sizes. The SequentialOrchestrator knows how to: - aggregate edge messages for each receiver, - apply diagonal modules, - and run the chosen schedule (<code>step</code>, <code>step_inference</code>, <code>predict</code>, <code>backward</code>).</p> <pre><code>DIM_DATA = 100\nNUM_LABELS = MNISTData.NUM_CLASSES\nDIM_HIDDEN = 300\n\nTHRESHOLD_OUT = 1.0\nTHRESHOLD_IN = 1.0\nTHRESHOLD_J = 1.0\nSTRENGTH_BACK = 0.5\nSTRENGTH_FORTH = 5.0\nJ_D = 0.5\n\n# Global state with three buffers: input (0), hidden (1), output/labels (2)\nstate = SequentialState((DIM_DATA, DIM_HIDDEN, NUM_LABELS))\n\n# Independent keys for each module (avoid accidental correlations).\nmaster_key = jax.random.key(seed=44)\nkeys = jax.random.split(master_key, num=5)\n\nlayer_map = {\n    1: {  # Hidden row receives from input, itself, and labels\n        0: FullyConnected(\n            in_features=DIM_DATA,\n            out_features=DIM_HIDDEN,\n            strength=STRENGTH_FORTH,\n            threshold=THRESHOLD_IN,\n            key=keys[0],\n        ),\n        1: RecurrentDiscrete(\n            features=DIM_HIDDEN,\n            j_d=J_D,\n            threshold=THRESHOLD_J,\n            key=keys[1],\n        ),\n        2: FrozenFullyConnected(  # clamping/teaching signal, not trainable\n            in_features=NUM_LABELS,\n            out_features=DIM_HIDDEN,\n            strength=STRENGTH_BACK,\n            threshold=0.0,\n            key=keys[2],\n        ),\n    },\n    2: {  # Output row receives from hidden and aggregates\n        1: FullyConnected(\n            in_features=DIM_HIDDEN,\n            out_features=NUM_LABELS,\n            strength=1.0,\n            threshold=THRESHOLD_OUT,\n            key=keys[3],\n        ),\n        2: OutputLayer(),  # diagonal sink: produces zeros; acts as aggregator anchor\n    },\n}\nlayer_map = LayerMap.from_dict(layer_map)\n\n# Trainable orchestrator built from the fixed topology.\norchestrator = SequentialOrchestrator(layers=layer_map)\nlogger.info(\"Model initialized with SequentialOrchestrator.\")\n</code></pre>"},{"location":"tutorials/07_optimized_training_on_mnist/#5-optimizer-and-the-no-backprop-update","title":"5) Optimizer and the \u201cno-backprop\u201d update","text":"<p>darnax does not use backpropagation here. Instead:</p> <ol> <li>Run recurrent dynamics with the current batch clamped (inputs + labels in the state).</li> <li>Call <code>orchestrator.backward(state, rng)</code> to get local deltas for every trainable module.</li> <li>Apply those deltas using Optax\u2014this gives you the familiar optimizer ergonomics.</li> </ol> <p>Notes for JAX compilation: - We pass the optimizer object as an argument to the jitted functions.   Under <code>eqx.filter_jit</code>, non-array args are static. Reusing the same instance prevents retracing. - Only the optimizer state (arrays) flows through the jitted code.</p> <pre><code>optimizer = optax.adam(2e-3)\nopt_state = optimizer.init(eqx.filter(orchestrator, eqx.is_inexact_array))\n\n\ndef _apply_update(\n    orch: SequentialOrchestrator,\n    s: SequentialState,\n    opt_state,\n    rng: jax.Array,\n    optimizer,\n):\n    \"\"\"Compute local deltas via .backward, then apply Optax updates.\n\n    Why separate this helper?\n      - Clear separation of concerns (dynamics vs parameter updates).\n      - Easier to unit-test and profile independently.\n    \"\"\"\n    grads = orch.backward(s, rng=rng)  # local deltas, tree-shaped like `orch`\n    params = eqx.filter(orch, eqx.is_inexact_array)  # trainable leaves\n    grads = eqx.filter(grads, eqx.is_inexact_array)  # drop non-arrays from grads\n\n    updates, opt_state = optimizer.update(grads, opt_state, params=params)\n    orch = eqx.apply_updates(orch, updates)\n    return orch, opt_state\n</code></pre>"},{"location":"tutorials/07_optimized_training_on_mnist/#6-dynamics-with-laxscan-not-jitted-directly","title":"6) Dynamics with <code>lax.scan</code> (not jitted directly)","text":"<p>The orchestrator exposes one-step transitions: - <code>step(s, rng)</code> \u2192 (s\u2019, rng\u2019): full dynamics (includes backward/label messages). - <code>step_inference(s, rng)</code> \u2192 (s\u2019, rng\u2019): inference-only dynamics (suppress backward messages).</p> <p>We wrap those into scans. These helpers are not jitted on their own; they are traced as part of the outer jitted steps. That keeps the code modular and the compiled graph clean.</p> <pre><code>def _scan_steps(fn, s: SequentialState, rng: jax.Array, steps: int):\n    \"\"\"Scan `steps` times a (s, rng)-&gt;(s, rng) transition.\"\"\"\n\n    def body(carry, _):\n        s, rng = carry\n        s, rng = fn(s, rng=rng)\n        return (s, rng), None\n\n    (s, rng), _ = jax.lax.scan(body, (s, rng), xs=None, length=steps)\n    return s, rng\n\n\ndef run_dynamics_training(\n    orch: SequentialOrchestrator,\n    s: SequentialState,\n    rng: jax.Array,\n    steps: int,\n):\n    \"\"\"Clamped phase (full dynamics) followed by a short free relaxation (inference).\"\"\"\n    s, rng = _scan_steps(orch.step, s, rng, steps)  # clamped\n    s, rng = _scan_steps(orch.step_inference, s, rng, steps)  # free\n    return s, rng\n\n\ndef run_dynamics_inference(\n    orch: SequentialOrchestrator,\n    s: SequentialState,\n    rng: jax.Array,\n    steps: int,\n):\n    \"\"\"Inference-only relaxation to a fixed point.\"\"\"\n    s, rng = _scan_steps(orch.step_inference, s, rng, 2 * steps)\n    return s, rng\n</code></pre>"},{"location":"tutorials/07_optimized_training_on_mnist/#7-outer-steps-the-only-jit-compiled-functions","title":"7) Outer steps (the only <code>jit</code>-compiled functions)","text":"<p>We jit only the functions that are called many times and represent the outer boundary of our computation:</p> <ul> <li> <p><code>train_step</code> (per batch):   1) write <code>(x, y)</code> into the state (clamp),   2) run clamped + free dynamics,   3) compute local deltas and apply the Optax update.</p> </li> <li> <p><code>eval_step</code> (per batch):   1) write <code>x</code> only,   2) run free dynamics,   3) <code>predict</code> and compute accuracy.</p> </li> </ul> <p>JIT boundary discipline: - Static args (optimizer object, Python ints like <code>t_train</code>) trigger retraces only if they   change. Keep them fixed during a run.</p> <pre><code>@eqx.filter_jit\ndef train_step(\n    orch: SequentialOrchestrator,\n    s: SequentialState,\n    x: jnp.ndarray,\n    y: jnp.ndarray,\n    rng: jax.Array,\n    *,\n    opt_state,\n    optimizer,\n    t_train: int = 3,\n):\n    \"\"\"Perform a train step in a single batch.\"\"\"\n    # 1) Clamp inputs + labels into the global state.\n    s = s.init(x, y)\n\n    # 2) Recurrent dynamics: clamped phase then free relaxation.\n    s, rng = run_dynamics_training(orch, s, rng, steps=t_train)\n\n    # 3) Local deltas + Optax update.\n    rng, update_key = jax.random.split(rng)\n    orch, opt_state = _apply_update(orch, s, opt_state, update_key, optimizer)\n    return orch, rng, opt_state\n\n\n@eqx.filter_jit\ndef eval_step(\n    orch: SequentialOrchestrator,\n    s: SequentialState,\n    x: jnp.ndarray,\n    y: jnp.ndarray,\n    rng: jax.Array,\n    *,\n    t_eval: int = 5,\n) -&gt; tuple[float, jax.Array]:\n    \"\"\"Perform a validation step on a single batch.\"\"\"\n    # 1) Clamp inputs only (labels aren't used by dynamics here).\n    s = s.init(x, None)\n\n    # 2) Free relaxation to a fixed point.\n    s, rng = run_dynamics_inference(orch, s, rng, steps=t_eval)\n\n    # 3) Predict scores from the settled state and measure accuracy.\n    s, rng = orch.predict(s, rng)\n    y_pred = s[-1]\n    acc = batch_accuracy(y, y_pred)\n    return acc, rng\n</code></pre>"},{"location":"tutorials/07_optimized_training_on_mnist/#8-training-loop-cpu","title":"8) Training loop (CPU)","text":"<p>The Python epoch loop shepherds data and RNG. All heavy lifting happens inside the two jitted steps above. Practical guidance:</p> <ul> <li>Keep array shapes/dtypes and the pytrees\u2019 structures stable across calls.</li> <li>Reuse the same optimizer instance; pass its state through the jitted code.</li> <li>If you change <code>t_train</code>/<code>t_eval</code> between calls, expect a retrace (they are static).</li> </ul> <pre><code># Experiment knobs\nNUM_IMAGES_PER_CLASS = 5400\nAPPLY_SIGN_TRANSFORM = True\nBATCH_SIZE = 16\nEPOCHS = 5\nT_TRAIN = 10  # clamped + free steps per batch\nT_EVAL = 10  # inference steps multiplier (2*T_EVAL iterations)\n\n# RNGs\nmaster_key = jax.random.key(59)\nmaster_key, data_key = jax.random.split(master_key)\n\n# Data\ndata = MNISTData(\n    key=data_key,\n    batch_size=BATCH_SIZE,\n    linear_projection=DIM_DATA,\n    apply_sign_transform=APPLY_SIGN_TRANSFORM,\n    num_images_per_class=NUM_IMAGES_PER_CLASS,\n)\nprint(f\"Dataset ready \u2014 x.shape={tuple(data.x.shape)}, y.shape={tuple(data.y.shape)}\")\n\n# Train &amp; evaluate\nfor epoch in range(1, EPOCHS + 1):\n    t0 = time.time()\n    print(f\"\\n=== Epoch {epoch}/{EPOCHS} ===\")\n\n    # Training epoch\n    for x_batch, y_batch in data:\n        master_key, step_key = jax.random.split(master_key)\n        orchestrator, master_key, opt_state = train_step(\n            orchestrator,\n            state,\n            x_batch,\n            y_batch,\n            rng=step_key,\n            opt_state=opt_state,\n            optimizer=optimizer,  # static in the JIT sense; same instance every call\n            t_train=T_TRAIN,\n        )\n\n    # Evaluation epoch (full test split)\n    accs = []\n    for x_b, y_b in data.iter_eval():\n        master_key, step_key = jax.random.split(master_key)\n        acc, master_key = eval_step(\n            orchestrator,\n            state,\n            x_b.astype(jnp.float32),\n            y_b.astype(jnp.float32),\n            rng=step_key,\n            t_eval=T_EVAL,\n        )\n        accs.append(acc)\n\n    acc_epoch = float(jnp.mean(jnp.array(accs)))\n    print(f\"Eval Accuracy = {acc_epoch:.3f}  |  epoch time: {time.time() - t0:.2f}s\")\n</code></pre>"},{"location":"tutorials/07_optimized_training_on_mnist/#9-one-line-final-report","title":"9) One-line final report","text":"<p>This is just to have a single scalar you can grep from logs or compare across runs.</p> <pre><code>final_accs = []\nfor x_b, y_b in data.iter_eval():\n    master_key, step_key = jax.random.split(master_key)\n    acc, master_key = eval_step(\n        orchestrator,\n        state,\n        x_b.astype(jnp.float32),\n        y_b.astype(jnp.float32),\n        rng=step_key,\n        t_eval=T_EVAL,\n    )\n    final_accs.append(acc)\n\nprint(\"\\n=== Final evaluation summary ===\")\nprint(f\"Accuracy = {float(jnp.mean(jnp.array(final_accs))):.3f}\")\n</code></pre>"},{"location":"tutorials/07_optimized_training_on_mnist/#10-recap-next-steps","title":"10) Recap &amp; next steps","text":"<p>You just trained a recurrent, locally-plastic network on MNIST using darnax:</p> <ul> <li>You declared topology with a <code>LayerMap</code>, not a layer stack.</li> <li>A state of fixed buffers <code>(input, hidden, output)</code> was clamped and then   relaxed to a fixed point by the orchestrator.</li> <li>You updated parameters using local deltas (<code>orchestrator.backward</code>) funneled through Optax.</li> <li>You JIT-compiled the outer loop only, using <code>lax.scan</code> for inner dynamics.</li> </ul> <p>If you\u2019re serious about scaling this:</p> <ul> <li>Parallel orchestrators: swap <code>SequentialOrchestrator</code> for a parallel flavor when your   graphs grow (careful with data dependencies).</li> <li>Topology as data: generate <code>LayerMap</code> programmatically (e.g., blocks, conv-like bands).</li> <li>Per-block scalings: match initialization and LR magnitudes to each path\u2019s fan-in/out.</li> <li>Profiling: dump HLO for <code>train_step</code>/<code>eval_step</code>, sanity-check fusion and shape stability.</li> </ul> <p>Don\u2019t just accept the defaults\u2014pressure-test the schedule and the rules. If a path doesn\u2019t pull its weight (e.g., backward clamp too weak/strong), instrument it and fix it.</p>"}]}