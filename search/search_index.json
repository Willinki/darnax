{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"darnax","text":"<p>Deep Asymmetric Recurrent Networks in JAX.</p> <p>darnax is a research library for building and experimenting with asymmetric recurrent neural networks and their learning dynamics. It is inspired by recent work by Badalotti, Baldassi, M\u00e9zard, Scardecchia and Zecchina, showing that simple, distributed plasticity rules can give rise to powerful learning behaviors without relying on backpropagation.</p> <p>The library provides:</p> <ul> <li>Composable modules based on Equinox, with clean support for sparse and structured connectivity. They can be easily extended to accomodate for complex network structures (convolutional, transformers-like, etc...). They also naturally support layers of varying shapes and connectivity.</li> <li>Orchestrators for running recurrent dynamics, either sequentially or in parallel.</li> <li>Local update rules implementing gradient-free plasticity mechanisms.</li> <li>Jax speed and transparency. Everything is a pytree, whether you are building a simple 1 hidden-layer model or a complex interconnected structure, the training logic remains the same.</li> <li>Natural integration with optax. Despite not relying on explicit gradients, the models can be naturally optimized with Optax.</li> </ul> <p>darnax is not a framework chasing SOTA benchmarks. It is a sandbox for exploring recurrent dynamics as a computational primitive \u2014 bridging machine learning, theoretical neuroscience, and statistical physics. This is also a work-in-progress, and contributions are more than welcome!</p> <p>\ud83d\udc49 Check the Tutorials to get started, or browse the API Reference for details.</p>"},{"location":"reference/","title":"Reference","text":"<p>Auto-generated API lives here.</p> <ul> <li>\ud83d\udc49 API Index</li> </ul>"},{"location":"reference/api/","title":"API Index","text":"<p>Auto-generated. Top-level packages mirror the directory layout.</p>"},{"location":"reference/api/#packages","title":"Packages","text":"<ul> <li>darnax.layer_maps</li> <li>darnax.modules</li> <li>darnax.orchestrators</li> <li>darnax.states</li> <li>darnax.utils</li> </ul>"},{"location":"reference/api/darnax/","title":"darnax","text":""},{"location":"reference/api/darnax/#subpackages","title":"Subpackages","text":"<ul> <li>darnax.layer_maps</li> <li>darnax.modules</li> <li>darnax.orchestrators</li> <li>darnax.states</li> <li>darnax.utils</li> </ul>"},{"location":"reference/api/darnax/layer_maps/","title":"darnax.layer_maps","text":""},{"location":"reference/api/darnax/layer_maps/#modules","title":"Modules","text":"<ul> <li>darnax.layer_maps.sparse</li> </ul>"},{"location":"reference/api/darnax/modules/","title":"darnax.modules","text":""},{"location":"reference/api/darnax/modules/#modules","title":"Modules","text":"<ul> <li>darnax.modules.debug</li> <li>darnax.modules.ferromagnetic</li> <li>darnax.modules.fully_connected</li> <li>darnax.modules.input_output</li> <li>darnax.modules.interfaces</li> <li>darnax.modules.recurrent</li> </ul>"},{"location":"reference/api/darnax/orchestrators/","title":"darnax.orchestrators","text":""},{"location":"reference/api/darnax/orchestrators/#modules","title":"Modules","text":"<ul> <li>darnax.orchestrators.interface</li> <li>darnax.orchestrators.sequential</li> </ul>"},{"location":"reference/api/darnax/states/","title":"darnax.states","text":""},{"location":"reference/api/darnax/states/#modules","title":"Modules","text":"<ul> <li>darnax.states.interface</li> <li>darnax.states.sequential</li> </ul>"},{"location":"reference/api/darnax/utils/","title":"darnax.utils","text":""},{"location":"reference/api/darnax/utils/#modules","title":"Modules","text":"<ul> <li>darnax.utils.default_list</li> <li>darnax.utils.perceptron_rule</li> <li>darnax.utils.typing</li> </ul>"},{"location":"reference/api/darnax/layer_maps/sparse/","title":"darnax.layer_maps.sparse","text":""},{"location":"reference/api/darnax/layer_maps/sparse/#darnax.layer_maps.sparse.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/layer_maps/sparse/#darnax.layer_maps.sparse.LayerMap","title":"<code>LayerMap(_data, _rows, _ndim=2)</code>  <code>dataclass</code>","text":"<p>PyTree wrapper around a dict-of-dicts with static keys and non-static values.</p> Design goals <ul> <li>Maintain the clarity of a nested dict-of-dicts API.</li> <li>Keep the structure (row/column keys and their order) static for JIT stability.</li> <li>Flatten through modules so that arrays inside Equinox modules are visible to   JAX/Optax (parameters get gradients and updates).</li> <li>Prevent structural mutation after construction.</li> </ul> Notes <ul> <li>Row keys are sorted once at initialization.</li> <li>Column keys for each row are also sorted at initialization.</li> <li>Keys are part of the treedef (static). Module parameters are leaves.</li> <li>Public dict-like accessors return read-only mappings to avoid accidental   structural mutation. Use <code>to_dict()</code> if you need a deep copy for external use.</li> </ul> Public type <p>Values are typed as <code>AbstractModule</code> so that any subclass can be stored.</p>"},{"location":"reference/api/darnax/layer_maps/sparse/#darnax.layer_maps.sparse.LayerMap.from_dict","title":"<code>from_dict(mapping, *, require_diagonal=True)</code>  <code>staticmethod</code>","text":"<p>Construct a LayerMap from a nested mapping.</p> <p>Parameters:</p> Name Type Description Default <code>mapping</code> <code>Mapping[int, Mapping[int, AbstractModule]]</code> <p>Nested mapping from row -&gt; (col -&gt; module).</p> required <code>require_diagonal</code> <code>bool</code> <p>Enforce that (i, i) exists for any i present as either a row or a column.</p> <code>True</code>"},{"location":"reference/api/darnax/layer_maps/sparse/#darnax.layer_maps.sparse.LayerMap.tree_flatten","title":"<code>tree_flatten()</code>","text":"<p>Deconstruct the tree. Does NOT flatten through modules.</p>"},{"location":"reference/api/darnax/layer_maps/sparse/#darnax.layer_maps.sparse.LayerMap.tree_unflatten","title":"<code>tree_unflatten(aux, children)</code>  <code>classmethod</code>","text":"<p>Reconstruct the tree based on aux and children provided by tree_flatten.</p>"},{"location":"reference/api/darnax/layer_maps/sparse/#darnax.layer_maps.sparse.LayerMap.rows","title":"<code>rows()</code>","text":"<p>All row indices in sorted order (static).</p>"},{"location":"reference/api/darnax/layer_maps/sparse/#darnax.layer_maps.sparse.LayerMap.cols_of","title":"<code>cols_of(i)</code>","text":"<p>All column indices of row <code>i</code> in sorted order (static for a given map).</p>"},{"location":"reference/api/darnax/layer_maps/sparse/#darnax.layer_maps.sparse.LayerMap.neighbors","title":"<code>neighbors(i)</code>","text":"<p>Read-only mapping of neighbors (col \u2192 module) for row <code>i</code>.</p>"},{"location":"reference/api/darnax/layer_maps/sparse/#darnax.layer_maps.sparse.LayerMap.row_items","title":"<code>row_items(skip_last=False, forward_only=False)</code>","text":"<p>Iterate <code>(row, neighbors)</code> with deterministic ordering and read-only views.</p> <p>Parameters:</p> Name Type Description Default <code>skip_last</code> <code>bool</code> <p>If True, the last receiver row is omitted.</p> <code>False</code> <code>forward_only</code> <code>bool</code> <p>If True, only modules that send messages \"forward\" are computed.</p> <code>False</code>"},{"location":"reference/api/darnax/layer_maps/sparse/#darnax.layer_maps.sparse.LayerMap.edge_items","title":"<code>edge_items()</code>","text":"<p>Iterate over <code>((i, j), module)</code> in deterministic row-major order.</p>"},{"location":"reference/api/darnax/layer_maps/sparse/#darnax.layer_maps.sparse.LayerMap.to_dict","title":"<code>to_dict()</code>","text":"<p>Deep copy as a plain dict-of-dicts (mutable, not tied to this object).</p>"},{"location":"reference/api/darnax/modules/debug/","title":"darnax.modules.debug","text":""},{"location":"reference/api/darnax/modules/debug/#darnax.modules.debug.DebugLayer","title":"<code>DebugLayer()</code>","text":"<p>               Bases: <code>Layer</code></p> <p>A simple trainable layer for testing and debugging.</p> <p>Initialize the weight equal to 1.0.</p>"},{"location":"reference/api/darnax/modules/debug/#darnax.modules.debug.DebugLayer.w","title":"<code>w = jnp.ones((1,), dtype=(jnp.float32))</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/debug/#darnax.modules.debug.DebugLayer.a","title":"<code>a = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/debug/#darnax.modules.debug.DebugLayer.activation","title":"<code>activation(x)</code>","text":"<p>Apply the layer\u2019s activation function.</p>"},{"location":"reference/api/darnax/modules/debug/#darnax.modules.debug.DebugLayer.reduce","title":"<code>reduce(h)</code>","text":"<p>Aggregate incoming messages into a single tensor.</p>"},{"location":"reference/api/darnax/modules/debug/#darnax.modules.debug.DebugLayer.backward","title":"<code>backward(x, y, y_hat)</code>","text":"<p>Return the update is the itself.</p>"},{"location":"reference/api/darnax/modules/debug/#darnax.modules.debug.DebugAdapter","title":"<code>DebugAdapter()</code>","text":"<p>               Bases: <code>Adapter</code></p> <p>A stateless mapping between layers.</p> <p>Initialize the weight equal to 1.0.</p>"},{"location":"reference/api/darnax/modules/debug/#darnax.modules.debug.DebugAdapter.w","title":"<code>w = jnp.ones((1,), dtype=(jnp.float32))</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/debug/#darnax.modules.debug.DebugAdapter.a","title":"<code>a = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/debug/#darnax.modules.debug.DebugAdapter.backward","title":"<code>backward(x, y, y_hat)</code>","text":"<p>Return the update is the itself.</p>"},{"location":"reference/api/darnax/modules/ferromagnetic/","title":"darnax.modules.ferromagnetic","text":""},{"location":"reference/api/darnax/modules/ferromagnetic/#darnax.modules.ferromagnetic.KeyArray","title":"<code>KeyArray = Array</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/modules/ferromagnetic/#darnax.modules.ferromagnetic.Ferromagnetic","title":"<code>Ferromagnetic(features, strength, dtype=jnp.float32)</code>","text":"<p>               Bases: <code>Adapter</code></p> <p>Elementwise scaling adapter with fixed coupling strength.</p> <p>Initialize the adapter with scalar or per-feature strength.</p>"},{"location":"reference/api/darnax/modules/ferromagnetic/#darnax.modules.ferromagnetic.Ferromagnetic.strength","title":"<code>strength = self._set_shape(strength, features, dtype)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/ferromagnetic/#darnax.modules.ferromagnetic.Ferromagnetic.backward","title":"<code>backward(x, y, y_hat)</code>","text":"<p>Return a zero update to indicate non-trainability.</p>"},{"location":"reference/api/darnax/modules/fully_connected/","title":"darnax.modules.fully_connected","text":""},{"location":"reference/api/darnax/modules/fully_connected/#darnax.modules.fully_connected.KeyArray","title":"<code>KeyArray = Array</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/modules/fully_connected/#darnax.modules.fully_connected.FullyConnected","title":"<code>FullyConnected(in_features, out_features, strength, threshold, key, dtype=jnp.float32)</code>","text":"<p>               Bases: <code>Adapter</code></p> <p>Fully-connected trainable adapter (x @ W, per-output scaling).</p> <p>Initialize weights and per-output strength/threshold.</p>"},{"location":"reference/api/darnax/modules/fully_connected/#darnax.modules.fully_connected.FullyConnected.W","title":"<code>W = jax.random.normal(key, (in_features, out_features), dtype=dtype) / jnp.sqrt(in_features)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/fully_connected/#darnax.modules.fully_connected.FullyConnected.strength","title":"<code>strength = self._set_shape(strength, out_features, dtype)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/fully_connected/#darnax.modules.fully_connected.FullyConnected.threshold","title":"<code>threshold = self._set_shape(threshold, out_features, dtype)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/fully_connected/#darnax.modules.fully_connected.FullyConnected.backward","title":"<code>backward(x, y, y_hat)</code>","text":"<p>Return a module-shaped update: \u0394W set; strength/threshold zero.</p>"},{"location":"reference/api/darnax/modules/fully_connected/#darnax.modules.fully_connected.FrozenFullyConnected","title":"<code>FrozenFullyConnected(in_features, out_features, strength, threshold, key, dtype=jnp.float32)</code>","text":"<p>               Bases: <code>FullyConnected</code></p> <p>Implements a fully connected layer with no updates.</p> <p>QOL implementation for Wback.</p>"},{"location":"reference/api/darnax/modules/fully_connected/#darnax.modules.fully_connected.FrozenFullyConnected.backward","title":"<code>backward(x, y, y_hat)</code>","text":"<p>Return zero update for all parameters.</p>"},{"location":"reference/api/darnax/modules/input_output/","title":"darnax.modules.input_output","text":""},{"location":"reference/api/darnax/modules/input_output/#darnax.modules.input_output.KeyArray","title":"<code>KeyArray = Array</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/modules/input_output/#darnax.modules.input_output.OutputLayer","title":"<code>OutputLayer</code>","text":"<p>               Bases: <code>Layer</code></p> <p>Simple identity output layer that sums predictions.</p> <p>This layer leaves inputs unchanged on forward/activation, and provides a <code>reduce</code> method that elementwise-sums all array leaves in a PyTree of predictions. The backward pass is a no-op because this layer has no trainable parameters.</p>"},{"location":"reference/api/darnax/modules/input_output/#darnax.modules.input_output.OutputLayer.reduce","title":"<code>reduce(h)</code>","text":"<p>Elementwise-sum all array leaves in a PyTree of predictions.</p> <p>Parameters:</p> Name Type Description Default <code>h</code> <code>PyTree</code> <p>PyTree whose leaves are arrays with identical shapes/dtypes.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>The elementwise sum across all leaves.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>h</code> has no leaves.</p>"},{"location":"reference/api/darnax/modules/input_output/#darnax.modules.input_output.OutputLayer.activation","title":"<code>activation(x)</code>","text":"<p>Identity activation.</p>"},{"location":"reference/api/darnax/modules/input_output/#darnax.modules.input_output.OutputLayer.backward","title":"<code>backward(x, y, y_hat)</code>","text":"<p>No-op backward because the output layer has no parameters.</p>"},{"location":"reference/api/darnax/modules/interfaces/","title":"darnax.modules.interfaces","text":""},{"location":"reference/api/darnax/modules/interfaces/#darnax.modules.interfaces.KeyArray","title":"<code>KeyArray = Array</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/modules/interfaces/#darnax.modules.interfaces.AbstractModule","title":"<code>AbstractModule</code>","text":"<p>               Bases: <code>Module</code>, <code>ABC</code></p> <p>Base class for layers and adapters.</p>"},{"location":"reference/api/darnax/modules/interfaces/#darnax.modules.interfaces.AbstractModule.has_state","title":"<code>has_state</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Return whether the module carries persistent state.</p>"},{"location":"reference/api/darnax/modules/interfaces/#darnax.modules.interfaces.AbstractModule.backward","title":"<code>backward(x, y, y_hat)</code>  <code>abstractmethod</code>","text":"<p>Compute a parameter update with the same PyTree structure.</p>"},{"location":"reference/api/darnax/modules/interfaces/#darnax.modules.interfaces.Layer","title":"<code>Layer</code>","text":"<p>               Bases: <code>AbstractModule</code>, <code>ABC</code></p> <p>A trainable layer with an activation and reducer.</p>"},{"location":"reference/api/darnax/modules/interfaces/#darnax.modules.interfaces.Layer.has_state","title":"<code>has_state</code>  <code>property</code>","text":"<p>Return True; layers are stateful by design.</p>"},{"location":"reference/api/darnax/modules/interfaces/#darnax.modules.interfaces.Layer.activation","title":"<code>activation(x)</code>  <code>abstractmethod</code>","text":"<p>Apply the layer\u2019s activation function.</p>"},{"location":"reference/api/darnax/modules/interfaces/#darnax.modules.interfaces.Layer.reduce","title":"<code>reduce(h)</code>  <code>abstractmethod</code>","text":"<p>Aggregate incoming messages into a single tensor.</p>"},{"location":"reference/api/darnax/modules/interfaces/#darnax.modules.interfaces.Adapter","title":"<code>Adapter</code>","text":"<p>               Bases: <code>AbstractModule</code>, <code>ABC</code></p> <p>A stateless mapping between layers.</p>"},{"location":"reference/api/darnax/modules/interfaces/#darnax.modules.interfaces.Adapter.has_state","title":"<code>has_state</code>  <code>property</code>","text":"<p>Return False; adapters carry no persistent state.</p>"},{"location":"reference/api/darnax/modules/recurrent/","title":"darnax.modules.recurrent","text":""},{"location":"reference/api/darnax/modules/recurrent/#darnax.modules.recurrent.KeyArray","title":"<code>KeyArray = jax.Array</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/modules/recurrent/#darnax.modules.recurrent.RecurrentDiscrete","title":"<code>RecurrentDiscrete(features, j_d, threshold, key, dtype=jnp.float32)</code>","text":"<p>               Bases: <code>Layer</code></p> <p>Binary (\u00b11) recurrent layer with dense couplings.</p> <p>Initialize J with Gaussian entries, set diag to j_d, and store thresholds.</p>"},{"location":"reference/api/darnax/modules/recurrent/#darnax.modules.recurrent.RecurrentDiscrete.J","title":"<code>J = J</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/recurrent/#darnax.modules.recurrent.RecurrentDiscrete.J_D","title":"<code>J_D = j_d_vec</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/recurrent/#darnax.modules.recurrent.RecurrentDiscrete.threshold","title":"<code>threshold = thresh_vec</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/modules/recurrent/#darnax.modules.recurrent.RecurrentDiscrete.activation","title":"<code>activation(x)</code>","text":"<p>Return strict \u00b11 activation with ties mapped to +1.</p>"},{"location":"reference/api/darnax/modules/recurrent/#darnax.modules.recurrent.RecurrentDiscrete.reduce","title":"<code>reduce(h)</code>","text":"<p>Aggregate incoming messages by summation.</p>"},{"location":"reference/api/darnax/modules/recurrent/#darnax.modules.recurrent.RecurrentDiscrete.backward","title":"<code>backward(x, y, y_hat)</code>","text":"<p>Return a module-shaped update with \u0394J in J and zeros elsewhere.</p>"},{"location":"reference/api/darnax/orchestrators/interface/","title":"darnax.orchestrators.interface","text":""},{"location":"reference/api/darnax/orchestrators/interface/#darnax.orchestrators.interface.ModuleT","title":"<code>ModuleT = TypeVar('ModuleT', bound='AbstractModule')</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/orchestrators/interface/#darnax.orchestrators.interface.StateT","title":"<code>StateT = TypeVar('StateT', bound='State')</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/orchestrators/interface/#darnax.orchestrators.interface.KeyArray","title":"<code>KeyArray = jax.Array</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/orchestrators/interface/#darnax.orchestrators.interface.AbstractOrchestrator","title":"<code>AbstractOrchestrator</code>","text":"<p>               Bases: <code>Module</code>, <code>Generic[StateT]</code></p> <p>Handles communication and messages between modules in a layermap.</p>"},{"location":"reference/api/darnax/orchestrators/interface/#darnax.orchestrators.interface.AbstractOrchestrator.lmap","title":"<code>lmap</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/orchestrators/interface/#darnax.orchestrators.interface.AbstractOrchestrator.step","title":"<code>step(state, *, rng)</code>  <code>abstractmethod</code>","text":"<p>Given the current state, run one forward/update step and return the new state.</p> <p>It does not change/affect the value of the output state.</p>"},{"location":"reference/api/darnax/orchestrators/interface/#darnax.orchestrators.interface.AbstractOrchestrator.step_inference","title":"<code>step_inference(state, *, rng)</code>  <code>abstractmethod</code>","text":"<p>Given the current state, run one forward/update step and return the new state.</p> <p>Does not compute messages traveling \"to the right\". Does not change/affect the value of the output state.</p>"},{"location":"reference/api/darnax/orchestrators/interface/#darnax.orchestrators.interface.AbstractOrchestrator.predict","title":"<code>predict(state, *, rng)</code>  <code>abstractmethod</code>","text":"<p>Update the output state.</p>"},{"location":"reference/api/darnax/orchestrators/interface/#darnax.orchestrators.interface.AbstractOrchestrator.backward","title":"<code>backward(state, rng)</code>  <code>abstractmethod</code>","text":"<p>Given the current state, compute the updates for the parameters in <code>lmap</code>.</p> <p>The returned PyTree must have the same structure as <code>lmap</code> so that it can be used with Optax's <code>update</code> and <code>apply_updates</code>.</p>"},{"location":"reference/api/darnax/orchestrators/sequential/","title":"darnax.orchestrators.sequential","text":""},{"location":"reference/api/darnax/orchestrators/sequential/#darnax.orchestrators.sequential.KeyArray","title":"<code>KeyArray = Array</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/orchestrators/sequential/#darnax.orchestrators.sequential.SequentialOrchestrator","title":"<code>SequentialOrchestrator(layers)</code>","text":"<p>               Bases: <code>AbstractOrchestrator[SequentialState]</code></p> <p>Sequential message-passing orchestrator compatible with the new LayerMap.</p> Assumptions <ul> <li><code>lmap</code> is a dict-of-dicts PyTree with static structure (sorted keys) and   Equinox modules as values; the LayerMap flattens through modules so their   parameters are visible to JAX/Optax.</li> <li>For each receiver <code>i</code>, the diagonal module <code>lmap[i, i]</code> implements   <code>reduce(pytree_of_messages)</code> and <code>activation(Array) -&gt; Array</code>.</li> <li>Each edge module <code>lmap[i, j]</code> is callable as <code>module(x, rng=...) -&gt; Array</code>   and provides <code>backward(x, y, y_hat) -&gt; AbstractModule</code> (same PyTree type).</li> </ul> <p>Initialize the orchestrator from the layermap.</p>"},{"location":"reference/api/darnax/orchestrators/sequential/#darnax.orchestrators.sequential.SequentialOrchestrator.lmap","title":"<code>lmap = layers</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/orchestrators/sequential/#darnax.orchestrators.sequential.SequentialOrchestrator.step","title":"<code>step(state, *, rng)</code>","text":"<p>One forward update over all receivers, except the output layer (skip_last=True).</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>SequentialState</code> <p>The current network state.</p> required <code>rng</code> <code>KeyArray</code> <p>PRNG key; will be split per receiver/sender and advanced.</p> required"},{"location":"reference/api/darnax/orchestrators/sequential/#darnax.orchestrators.sequential.SequentialOrchestrator.step_inference","title":"<code>step_inference(state, *, rng)</code>","text":"<p>One forward update over all receivers, except the output layer (skip_last=True).</p> <p>Also skips messages coming from the right (forward_only=True)</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>SequentialState</code> <p>The current network state.</p> required <code>rng</code> <code>KeyArray</code> <p>PRNG key; will be split per receiver/sender and advanced.</p> required"},{"location":"reference/api/darnax/orchestrators/sequential/#darnax.orchestrators.sequential.SequentialOrchestrator.predict","title":"<code>predict(state, rng)</code>","text":"<p>Update the output state s[-1].</p>"},{"location":"reference/api/darnax/orchestrators/sequential/#darnax.orchestrators.sequential.SequentialOrchestrator.backward","title":"<code>backward(state, rng)</code>","text":"<p>Compute per-edge updates for all modules.</p> <p>Returns a LayerMap-structured pytree of updates (same PyTree type as modules).</p>"},{"location":"reference/api/darnax/states/interface/","title":"darnax.states.interface","text":""},{"location":"reference/api/darnax/states/interface/#darnax.states.interface.State","title":"<code>State</code>","text":"<p>               Bases: <code>Module</code></p> <p>Global state for a network (layermap).</p> <p>Each layer of a layermap has an associated state. The number of elements in the global state needs to be equal to the number of layers in the LayerMap + 1 at least.</p> <p>See \"inputs\" and \"output\" in docs.</p>"},{"location":"reference/api/darnax/states/interface/#darnax.states.interface.State.init","title":"<code>init(x, y=None)</code>  <code>abstractmethod</code>","text":"<p>Initialize the state with the input and output in the correct place.</p> <p>They are the first [0] and last element of the state.</p>"},{"location":"reference/api/darnax/states/interface/#darnax.states.interface.State.replace","title":"<code>replace(value)</code>  <code>abstractmethod</code>","text":"<p>Replace the whole state functionally with new values.</p>"},{"location":"reference/api/darnax/states/interface/#darnax.states.interface.State.replace_val","title":"<code>replace_val(idx, value)</code>  <code>abstractmethod</code>","text":"<p>Return a new instance with layer state at id idx replaced by value .</p>"},{"location":"reference/api/darnax/states/sequential/","title":"darnax.states.sequential","text":""},{"location":"reference/api/darnax/states/sequential/#darnax.states.sequential.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/states/sequential/#darnax.states.sequential.SequentialState","title":"<code>SequentialState(sizes, dtype=jnp.float32)</code>","text":"<p>               Bases: <code>State</code></p> <p>Sequential activations buffer for layered networks.</p> <p>Layers are indexed from left to right: layer <code>0</code> is the input buffer and layer <code>-1</code> is the output buffer.</p> <p>The internal storage is a list of JAX arrays with shapes <code>(B, *size_l)</code>. At construction time buffers are initialized with <code>(1, *size_l)</code> zeros. Use :meth:<code>init</code> to resize them to a real batch size <code>B</code> and optionally set the output.</p> <p>Create a sequential state with one buffer per layer.</p> <p>Parameters:</p> Name Type Description Default <code>sizes</code> <code>Iterable[tuple[int, ...] | int]</code> <p>Iterable of tuples of positive integers or positive integers for each layer width, including input and output. For example, a one-hidden-layer classifier could be <code>[D, N, C]</code> or mixed-rank like <code>[(H, W, C_in), N, (C_out,)]</code>.</p> required <code>dtype</code> <code>dtype</code> <p>JAX dtype for all buffers (static in the PyTree).</p> <code>float32</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If any size is not a positive <code>int</code>/tuple of positive <code>int</code> or if the iterable is empty.</p>"},{"location":"reference/api/darnax/states/sequential/#darnax.states.sequential.SequentialState.data_min_ndim","title":"<code>data_min_ndim = eqx.field(default=2, static=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/states/sequential/#darnax.states.sequential.SequentialState.dtype","title":"<code>dtype = dtype</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/states/sequential/#darnax.states.sequential.SequentialState.states","title":"<code>states = [(jnp.zeros((1, *size), dtype=dtype)) for size in shape_tuples]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/api/darnax/states/sequential/#darnax.states.sequential.SequentialState.replace","title":"<code>replace(value)</code>","text":"<p>Return a new instance with <code>states</code> replaced by <code>value</code>.</p> Notes <p>This is a functional update; the original object is not mutated.</p>"},{"location":"reference/api/darnax/states/sequential/#darnax.states.sequential.SequentialState.replace_val","title":"<code>replace_val(idx, value)</code>","text":"<p>Return a new instance with layer <code>idx</code> replaced by <code>value</code>.</p>"},{"location":"reference/api/darnax/states/sequential/#darnax.states.sequential.SequentialState.init","title":"<code>init(x, y=None)</code>","text":"<p>Resize buffers to batch <code>B</code> and set input (and optionally output).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input batch with shape <code>(B, *D)</code> where <code>*D</code> must match the input layer shape.</p> required <code>y</code> <code>Array | None</code> <p>Optional output batch with shape <code>(B, *C)</code> where <code>*C</code> must match the output layer shape.</p> <code>None</code> <p>Returns:</p> Type Description <code>SequentialState</code> <p>A new instance whose buffers all have shape <code>(B, *size_l)</code>, with layer 0 set to <code>x</code> and (if provided) layer -1 set to <code>y</code>.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If shapes are inconsistent. Checks run at trace time (once per compilation).</p>"},{"location":"reference/api/darnax/utils/default_list/","title":"darnax.utils.default_list","text":""},{"location":"reference/api/darnax/utils/default_list/#darnax.utils.default_list.T","title":"<code>T = TypeVar('T')</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/darnax/utils/default_list/#darnax.utils.default_list.DefaultList","title":"<code>DefaultList(initial=None, *, default=None, default_factory=None)</code>","text":"<p>               Bases: <code>MutableSequence[T | None]</code>, <code>Generic[T]</code></p> <p>Default-filling mutable list, registered as a JAX PyTree.</p> Key behavior <ul> <li>Assigning or inserting beyond the current length fills gaps with a   default sentinel (materialized only when read).</li> <li>Slicing returns another :class:<code>DefaultList</code> that preserves default slots.</li> <li>Behaves like a normal <code>MutableSequence</code> for typing and basic list ops.</li> <li>Participates in JAX PyTree flatten/unflatten.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>initial</code> <code>Iterable[T | None]</code> <p>Initial concrete values to store. Defaults are not inserted unless indices are explicitly extended by assignment/insert.</p> <code>None</code> <code>default</code> <code>T | None</code> <p>Value returned when reading a default slot if <code>default_factory</code> is not provided. May be <code>None</code>.</p> <code>None</code> <code>default_factory</code> <code>Callable[[], T | None]</code> <p>Zero-arg callable that produces the value for a default slot on read. Takes precedence over <code>default</code>. Note: each read materializes a fresh value; defaults are not cached per-slot.</p> <code>None</code> Notes <p>PyTree leaves are the underlying elements, including the sentinel; any JAX <code>tree_map</code> should account for non-numeric leaves if defaults are present.</p> Public type <p>The public element type is <code>T | None</code> because materialized defaults may legitimately be <code>None</code>.</p> <p>Create a :class:<code>DefaultList</code>.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>default_factory</code> is not callable.</p>"},{"location":"reference/api/darnax/utils/default_list/#darnax.utils.default_list.DefaultList.tree_flatten","title":"<code>tree_flatten()</code>","text":"<p>Return children and aux data for JAX PyTree protocol.</p> <p>Returns:</p> Type Description <code>tuple[tuple[Any, ...], tuple[Any, ...]]</code> <p>Children are the raw underlying items (sentinels preserved). Aux data contains <code>(default, default_factory)</code>.</p>"},{"location":"reference/api/darnax/utils/default_list/#darnax.utils.default_list.DefaultList.tree_unflatten","title":"<code>tree_unflatten(aux, children)</code>  <code>classmethod</code>","text":"<p>Rebuild from aux and children (JAX PyTree protocol).</p>"},{"location":"reference/api/darnax/utils/default_list/#darnax.utils.default_list.DefaultList.insert","title":"<code>insert(index, value)</code>","text":"<p>Insert at <code>index</code>; if beyond end, fill gap with defaults then append.</p> <p>Negative indices are clamped like Python's <code>list.insert</code>.</p>"},{"location":"reference/api/darnax/utils/default_list/#darnax.utils.default_list.DefaultList.append","title":"<code>append(value)</code>","text":"<p>Append a value.</p>"},{"location":"reference/api/darnax/utils/default_list/#darnax.utils.default_list.DefaultList.extend","title":"<code>extend(values)</code>","text":"<p>Extend from an iterable.</p>"},{"location":"reference/api/darnax/utils/default_list/#darnax.utils.default_list.DefaultList.to_list","title":"<code>to_list(*, filter_defaults=False)</code>","text":"<p>Materialize to a plain Python list.</p> <p>Parameters:</p> Name Type Description Default <code>filter_defaults</code> <code>bool</code> <p>If True, drop default slots entirely. Otherwise materialize them.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[T | None]</code> <p>A list containing either materialized defaults or only concrete values.</p>"},{"location":"reference/api/darnax/utils/perceptron_rule/","title":"darnax.utils.perceptron_rule","text":"<p>Perceptron (OVA) backward update in JAX.</p> <ul> <li>Supports batched inputs.</li> <li>Labels are {-1, +1} per class (one-vs-all).</li> <li><code>y_hat</code> are raw scores (pre-activation).</li> <li>Returns weight update <code>dW</code> without applying a learning rate.</li> </ul> Shapes <p>x      : (d,) or (n, d) y      : (n, K) in {-1, +1} y_hat  : (n, K) raw scores returns: (d, K)</p>"},{"location":"reference/api/darnax/utils/perceptron_rule/#darnax.utils.perceptron_rule.perceptron_rule_backward","title":"<code>perceptron_rule_backward(x, y, y_hat, margin)</code>","text":"<p>Compute multiclass (OVA) perceptron weight update with scaling (no learning rate).</p> <p>Updates are applied when <code>y * y_hat &lt;= margin</code> (ties counted as mistakes).</p> <p>Args:     x: Input vector(s), shape <code>(d,)</code> or <code>(n, d)</code>.     y: Targets in <code>{-1, +1}</code>, shape <code>(n, K)</code>.     y_hat: Raw scores, shape <code>(n, K)</code>.     margin: Scalar margin threshold; changing its value will not retrace.</p> <p>Returns:     Weight update <code>dW</code> of shape <code>(d, K)</code> to add to the weights.</p> <p>Scaling:     - Divides by batch size <code>n</code> so updates are batch-size invariant.     - Divides by <code>sqrt(d)</code> (fan-in) so updates remain stable as width grows.</p>"},{"location":"reference/api/darnax/utils/typing/","title":"darnax.utils.typing","text":""},{"location":"reference/api/darnax/utils/typing/#darnax.utils.typing.PyTree","title":"<code>PyTree = Any</code>  <code>module-attribute</code>","text":""},{"location":"tutorials/","title":"Tutorials","text":"<p>This section provides useful tutorial to get started using this library, from JAX and Equinox basics to a first functioning example of a classifier on Entangled-MNIST.</p> <p>Models and concepts from this library are based on the paper by Badalotti, Baldassi, M\u00e9zard, Scardecchia and Zecchina. On the final tutorial, we will be training a 1-layer model on a simple benchmark dataset.</p> <p>The purpose of this library, however, is more general, as it allows for the quick and intuitive implementation of diverse architectures and learning schemes, loosely based on the principles of the original model.</p> <p>See the first tutorial to get started.</p> <p>For issues or questions, reach out to one of the Contributors.</p>"},{"location":"tutorials/01_jax_and_equinox/","title":"01 \u2014 JAX, Equinox, and Pytrees","text":"<p>This library is based on two foundational tools: JAX and Equinox. Understanding their philosophy is essential for working with this codebase. Both libraries share a unifying principle: everything is a pytree.</p> <p>This tutorial provides a concise introduction to these ideas. Readers are encouraged to consult the official documentation of JAX and Equinox for further technical details.</p>"},{"location":"tutorials/01_jax_and_equinox/#jax-transformations-of-numerical-functions","title":"JAX: Transformations of Numerical Functions","text":"<p>JAX extends NumPy with support for automatic differentiation and compilation. Its design emphasizes functional programming and composable transformations.</p> <p>Key concepts include:</p> <ul> <li>Array programming: JAX arrays follow NumPy semantics while executing efficiently on CPU, GPU, or TPU.</li> <li> <p>Transformations: JAX operates by transforming functions:</p> </li> <li> <p><code>jax.jit</code> compiles Python functions to optimized machine code.</p> </li> <li><code>jax.grad</code> computes derivatives automatically.</li> <li><code>jax.vmap</code> vectorizes functions across batch dimensions.</li> <li><code>jax.pmap</code> parallelizes computations across multiple devices.</li> <li>Composability: Transformations may be freely combined. For example, one may compute gradients of a JIT-compiled function or vectorize a function that already involves differentiation.</li> </ul> <p>The emphasis is not on predefined models or layers, but on transformations of user-defined functions.</p> <p>JAX gives the user a lot of powers, but this comes at a cost. If you're new to jax, be sure to read the Sharp bits.</p>"},{"location":"tutorials/01_jax_and_equinox/#equinox-pytrees-as-models","title":"Equinox: Pytrees as Models","text":"<p>Equinox is a lightweight neural network library for JAX. Its primary contribution is a consistent interface for defining models as plain Python classes.</p> <p>The design principles of Equinox are:</p> <ul> <li>Simplicity: Models are standard Python objects.</li> <li>Transparency: Parameters are stored directly as object attributes.</li> <li>Compatibility: Models are implemented as pytrees, allowing them to participate seamlessly in JAX transformations.</li> <li>Functional style: Updates to parameters or state return new objects, rather than mutating existing ones.</li> </ul> <p>This perspective aligns with the philosophy of this library: abstractions remain minimal, while full compatibility with JAX is preserved.</p>"},{"location":"tutorials/01_jax_and_equinox/#pytrees-a-unifying-abstraction","title":"Pytrees: A Unifying Abstraction","text":"<p>A pytree is a nested structure composed of Python containers (lists, tuples, dictionaries, dataclasses, and similar types) whose leaves are JAX arrays or compatible objects.</p> <p>Examples of pytrees:</p> <pre><code>import jax.numpy as jnp\n\n# Dictionary with arrays\nx = {\"a\": jnp.ones((2, 2)), \"b\": jnp.zeros((3,))}\n\n# Tuple of arrays\ny = (jnp.arange(3), jnp.ones((2,)))\n\n# Nested structures\nz = [x, y]\n</code></pre> <p>Pytrees are central to JAX for two reasons:</p> <ol> <li>They generalize beyond single arrays to arbitrary nested data structures.</li> <li>They allow JAX transformations to operate uniformly over these structures.</li> </ol> <p>In practice:</p> <ul> <li>A model is a pytree.</li> <li>Parameters and optimizer states are pytrees.</li> <li>Data batches may also be represented as pytrees.</li> </ul> <p>This single abstraction ensures that JAX transformations (such as <code>grad</code> or <code>jit</code>) can be applied consistently, regardless of structural complexity.</p>"},{"location":"tutorials/01_jax_and_equinox/#example-a-linear-module","title":"Example: A Linear Module","text":"<p>Equinox makes use of the pytree abstraction by treating models as pytrees.</p> <pre><code>import jax\nimport jax.numpy as jnp\nimport equinox as eqx\n\nclass Linear(eqx.Module):\n    weight: jax.Array\n    bias: jax.Array\n\n    def __init__(self, in_dim, out_dim, key):\n        wkey, bkey = jax.random.split(key)\n        self.weight = jax.random.normal(wkey, (in_dim, out_dim))\n        self.bias = jax.random.normal(bkey, (out_dim,))\n\n    def __call__(self, x):\n        return x @ self.weight + self.bias\n\n# Instantiate a model\nmodel = Linear(2, 3, jax.random.PRNGKey(0))\n\n# Forward evaluation\nx = jnp.ones((5, 2))\ny = model(x)\n\n# Differentiation\nloss_fn = lambda m, x: jnp.mean(m(x))\ngrads = jax.grad(loss_fn)(model, x)\n</code></pre> <p>Here, the model is both:</p> <ul> <li>a Python object with fields (<code>weight</code>, <code>bias</code>), and</li> <li>a pytree, enabling JAX to compute gradients and apply transformations directly.</li> </ul>"},{"location":"tutorials/01_jax_and_equinox/#philosophy-of-this-library","title":"Philosophy of This Library","text":"<p>The present library is guided by the following principles:</p> <ol> <li>Universality of pytrees: all major components (modules, layermaps, states) are structured as pytrees.</li> <li>Functional style: computations are expressed as pure functions, and updates return new objects.</li> <li>Composability: any component should be compatible with JAX transformations such as <code>jit</code>, <code>grad</code>, or <code>vmap</code>.</li> <li>Minimal abstraction: the library extends JAX and Equinox without concealing them. Users are encouraged to understand and directly employ these underlying tools.</li> </ol>"},{"location":"tutorials/01_jax_and_equinox/#next-tutorial","title":"Next Tutorial","text":"<p>The next tutorial will discuss the first component of this library: states and modules.</p>"},{"location":"tutorials/02_states/","title":"02 \u2014 States","text":"<p>This tutorial introduces States: the data structures that hold the current condition of the network. Unlike the transient activations of standard feedforward networks, states here are persistent, dynamical variables. They evolve iteratively under the network\u2019s dynamics and represent the system\u2019s position in configuration space, not merely intermediate results of a forward pass.</p>"},{"location":"tutorials/02_states/#1-conceptual-overview","title":"1. Conceptual overview","text":"<p>A state records the evolving configuration of the network at each layer. In the theoretical formulation, states are denoted</p> \\[ s^{(l)} \\in \\{\\pm 1\\}^N, \\] <p>but in this library they are general JAX arrays of arbitrary shape and dtype.</p> <p>Key points:</p> <ul> <li>Layer\u2013state relation. Every layer of the network is associated with a state buffer. This includes the input and output layers.</li> <li> <p>Initialization. When a batch <code>(x, y)</code> is presented:</p> </li> <li> <p>the input buffer is set to <code>x</code>,</p> </li> <li>the output buffer may be set to <code>y</code>,</li> <li>all intermediate buffers are initialized to zeros.</li> <li>Dynamics. States evolve by iterative updates until convergence (a fixed point or a steady regime). Later tutorials will explain these dynamics in detail.</li> <li>Shape generality. State buffers are not restricted to vectors. They may be multi-dimensional, e.g. <code>(H, W, C)</code> for convolutional or image-like architectures. The abstraction supports arbitrary shapes per layer.</li> </ul>"},{"location":"tutorials/02_states/#2-api-responsibilities","title":"2. API responsibilities","text":"<p>A state is deliberately minimal in API design but aligned with JAX\u2019s functional style. Each buffer is a JAX array, and the object provides simple functional accessors:</p> <pre><code>class State(eqx.Module):\n    def __getitem__(self, key: Any) -&gt; Array: ...\n    def init(self, x: Array, y: Array | None = None) -&gt; Self: ...\n    def replace(self, value: PyTree) -&gt; Self: ...\n    def replace_val(self, idx: Any, value: Array) -&gt; Self: ...\n</code></pre> <ul> <li> <p><code>__getitem__(key)</code>   Retrieves the buffer associated with <code>key</code> (e.g., layer index).</p> </li> <li> <p><code>init(x, y=None)</code>   Functionally initializes the state for a batch. Resizes all buffers to the batch dimension of <code>x</code>, sets the input to <code>x</code>, and optionally sets the output to <code>y</code>.</p> </li> <li> <p><code>replace(value)</code>   Returns a new state with the entire collection of buffers replaced by <code>value</code>.</p> </li> <li> <p><code>replace_val(idx, value)</code>   Returns a new state with only the buffer at position <code>idx</code> replaced by <code>value</code>.</p> </li> </ul> <p>All updates are functional and produce new objects. This immutability is essential for compatibility with JAX transformations.</p>"},{"location":"tutorials/02_states/#3-a-concrete-implementation-sequentialstate","title":"3. A concrete implementation: <code>SequentialState</code>","text":"<p><code>SequentialState</code> implements a left-to-right sequence of buffers, indexed by integers:</p> <ul> <li><code>0</code> = input buffer</li> <li><code>1, 2, \u2026, L-2</code> = intermediate buffers</li> <li><code>L-1</code> (or <code>-1</code>) = output buffer</li> </ul> <p>Internally, it stores a list of arrays with shapes <code>(B, *size_l)</code>, where <code>B</code> is the batch size.</p> <pre><code>state = SequentialState(sizes=[(4,), (8,), (3,)])\n</code></pre> <p>At construction time, buffers are initialized with dummy batch size <code>B=1</code>. The <code>init</code> method resizes them to the real batch size provided by <code>x</code>.</p>"},{"location":"tutorials/02_states/#example-initialization","title":"Example: initialization","text":"<pre><code>import jax\nimport jax.numpy as jnp\nfrom darnax.states.sequential import SequentialState\n\nstate = SequentialState(sizes=[(4,), (8,), (3,)])\n\nx = jax.random.normal(jax.random.PRNGKey(0), (32, 4))\ny = jax.random.normal(jax.random.PRNGKey(1), (32, 3))\n\ns0 = state.init(x, y)\nassert s0[0].shape == (32, 4)  # input\nassert s0[1].shape == (32, 8)  # hidden\nassert s0[-1].shape == (32, 3) # output\n</code></pre>"},{"location":"tutorials/02_states/#4-why-a-global-state","title":"4. Why a global state?","text":"<p>A deliberate design decision is to represent the state of the entire network globally, rather than letting each layer enclose its own state.</p> <p>The reasons are:</p> <ol> <li>Shared storage. Different layers may share access to portions of the same underlying state (e.g., in convolutional networks, multiple filters may act on overlapping regions of a global image-like buffer).</li> <li>Topological flexibility. A global state can be organized as a single multidimensional array or structured container, with different layers responsible for reading and writing specific slices.</li> <li>Consistency. This design allows heterogeneous architectures (dense, convolutional, graph-like) to operate on the same formal object without modifying the layer abstraction.</li> </ol> <p>This choice reflects the dynamical perspective: the network evolves as a whole, not as a collection of isolated layer-local states.</p>"},{"location":"tutorials/02_states/#5-states-as-pytrees","title":"5. States as pytrees","text":"<p>Because states are Equinox modules, they are pytrees. This has important consequences:</p> <ul> <li>JAX transformations (<code>jit</code>, <code>grad</code>, <code>vmap</code>, <code>pmap</code>) work seamlessly over state objects.</li> <li>Updates remain functional: replacing or modifying buffers yields new pytree instances.</li> <li>Static fields (e.g., <code>dtype</code>) are excluded from the dynamic leaves, reducing recompilation overhead.</li> </ul> <p>Thus, states are simultaneously containers of arrays and first-class JAX objects.</p>"},{"location":"tutorials/02_states/#6-summary","title":"6. Summary","text":"<ul> <li>States represent persistent network conditions, not just transient activations.</li> <li>Every layer is associated with part of the state, including input and output.</li> <li>States can store arbitrary shapes, enabling general architectures (vector-based, convolutional, etc.).</li> <li>The API is simple: indexing, functional initialization, and functional replacement.</li> <li>The design emphasizes a global state abstraction, allowing layers to share and reuse portions of it.</li> <li>Being pytrees, states integrate naturally with JAX\u2019s functional transformations.</li> </ul>"},{"location":"tutorials/03_modules/","title":"03 \u2014 Modules (Layers and Adapters)","text":"<p>This tutorial introduces Modules, the core building blocks of the library. Modules are divided into two categories:</p> <ul> <li>Layers: stateful modules that read and update a slice of the global network state.</li> <li>Adapters: stateless modules that transform one layer\u2019s state into a message for another.</li> </ul> <p>Only layers own a state. Both families may carry parameters; some are trainable (producing non-zero updates), others fixed (producing zero updates).</p>"},{"location":"tutorials/03_modules/#1-conceptual-background","title":"1. Conceptual background","text":"<p>The library is inspired by the recurrent dynamical model described in Dynamical Learning in Deep Asymmetric Recurrent Neural Networks. In that setting, each neuron (or unit) maintains a binary state \\(s \\in \\{-1, 1\\}\\), updated iteratively according to its local field:</p> \\[ s_i \\;\\leftarrow\\; \\operatorname{sign}\\!\\Big(\\sum_{j \\neq i} J_{ji}\\, s_j + J_D s_i \\Big). \\] <p>Extending to a multilayer chain, each layer \\(l\\) with state \\(s^{(l)}\\) also receives excitatory inputs from its neighbors with coupling \\(\\lambda\\):</p> \\[ s^{(l)}_i \\;\\leftarrow\\; \\operatorname{sign}\\!\\Big(\\sum_{j \\neq i} J^{(l)}_{ji}\\, s^{(l)}_j \\;+\\; J_D s^{(l)}_i \\;+\\; \\lambda (s^{(l-1)}_i + s^{(l+1)}_i) \\Big). \\] <p>Our modules provide a software abstraction of this process.</p> <ul> <li>Layers compute the recurrent/self contributions (\\(J\\), \\(J_D\\)) and handle aggregation + activation.</li> <li>Adapters contribute the cross-layer terms (e.g., \\(\\lambda s^{(l\\pm 1)}\\)).</li> </ul> <p>In other words, for a layer \\(l\\) with current state \\(s^{(l)}\\), a typical pre-activation is</p> \\[ h_i \\;=\\; \\underbrace{\\sum_{j \\neq i} J_{ji}\\, s_j + J_D s_i}_{\\text{layer self-message}} \\;+\\; \\underbrace{\\lambda\\, s_i^{(l-1)} + \\lambda\\, s_i^{(l+1)}}_{\\text{adapter messages}} \\] <p>and the new state is \\(s^{(l)} \\leftarrow \\text{activation}(h)\\).</p> <ul> <li>The self part is computed by the layer\u2019s <code>__call__</code>.</li> <li>The cross-layer parts are produced by adapters\u2019 <code>__call__</code> on neighboring (or otherwise connected) states.</li> <li>The layer\u2019s <code>reduce</code> performs the final aggregation into \\(h\\).</li> </ul>"},{"location":"tutorials/03_modules/#2-interfaces","title":"2. Interfaces","text":"<p>All modules extend a common base:</p> <pre><code>class AbstractModule(eqx.Module, ABC):\n\n    @property\n    @abstractmethod\n    def has_state(self) -&gt; bool: ...\n\n    @abstractmethod\n    def __call__(self, x: Array, rng: Array | None = None) -&gt; Array: ...\n\n    @abstractmethod\n    def backward(self, x: Array, y: Array, y_hat: Array) -&gt; Self: ...\n</code></pre> <ul> <li><code>__call__</code>: forward computation (a message).</li> <li><code>backward</code>: returns a module-shaped update. These are not gradients; they are local plasticity rules.</li> </ul>"},{"location":"tutorials/03_modules/#layers","title":"Layers","text":"<pre><code>class Layer(AbstractModule, ABC):\n\n    @property\n    def has_state(self) -&gt; bool: return True\n\n    @abstractmethod\n    def activation(self, x: Array) -&gt; Array: ...\n\n    @abstractmethod\n    def reduce(self, h: PyTree) -&gt; Array: ...\n</code></pre> <ul> <li><code>activation</code>: nonlinearity applied to the aggregated field.</li> <li><code>reduce</code>: combines incoming messages into a single tensor.</li> </ul>"},{"location":"tutorials/03_modules/#adapters","title":"Adapters","text":"<pre><code>class Adapter(AbstractModule, ABC):\n\n    @property\n    def has_state(self) -&gt; bool: return False\n</code></pre> <p>Adapters transform a source state into a message for another layer.</p>"},{"location":"tutorials/03_modules/#3-why-layer-states-are-messages","title":"3. Why layer states are \u201cmessages\u201d","text":"<p>In this architecture, a layer\u2019s current state is not just a transient activation, but the signal it emits to the rest of the network. Every update step consists of:</p> <ol> <li>Each layer publishing its state as a message.</li> <li>Adapters converting these messages into forms suitable for their targets.</li> <li>Layers aggregating self-messages and incoming adapter messages into \\(h\\).</li> <li>Layers applying their activation to obtain the new state.</li> </ol> <p>Thus, the global state itself is the medium of communication: states are messages.</p>"},{"location":"tutorials/03_modules/#4-example-modules","title":"4. Example modules","text":""},{"location":"tutorials/03_modules/#recurrentdiscrete","title":"RecurrentDiscrete","text":"<pre><code>class RecurrentDiscrete(Layer):\n    J: Array\n    J_D: Array\n    threshold: Array\n\n    def activation(self, x: Array) -&gt; Array:\n        return jnp.where(x &gt;= 0, 1, -1).astype(x.dtype)\n\n    def __call__(self, x: Array, rng=None) -&gt; Array:\n        return x @ self.J\n\n    def reduce(self, h: PyTree) -&gt; Array:\n        return jnp.asarray(tree_reduce(operator.add, h))\n\n    def backward(self, x: Array, y: Array, y_hat: Array) -&gt; Self:\n        dJ = perceptron_rule_backward(x, y, y_hat, self.threshold)\n        zero_update = jax.tree.map(jnp.zeros_like, self)\n        return eqx.tree_at(lambda m: m.J, zero_update, dJ)\n</code></pre> <ul> <li><code>__call__</code>: computes the recurrent/self-message.</li> <li><code>reduce</code>: aggregates all incoming messages.</li> <li><code>activation</code>: enforces \u00b11 states.</li> <li><code>backward</code>: computes a local perceptron-like rule to update \\(J\\).</li> </ul>"},{"location":"tutorials/03_modules/#ferromagnetic-adapter","title":"Ferromagnetic adapter","text":"<pre><code>class Ferromagnetic(Adapter):\n    strength: Array\n\n    def __call__(self, x: Array, rng=None) -&gt; Array:\n        return x * self.strength\n\n    def backward(self, x, y, y_hat) -&gt; Self:\n        return tree_map(jnp.zeros_like, self)\n</code></pre> <p>A fixed adapter implementing terms like \\(\\lambda s^{(l-1)}\\). Since it has no trainable parameters, the <code>backward</code> methods returns an array of zeros, meaning that the strength lambda remains unchanged.</p>"},{"location":"tutorials/03_modules/#5-one-update-step","title":"5. One update step","text":"<pre><code># state is a sequential state as shown in the first tutorial.\n# layer is our recurrent (or any other) implementation of the Layer\n# left and right are two adapters\n\ndef one_step(state, layer, left, right, l=1):\n    # we select the state of `layer`\n    s_l = state[l]\n    # we compute the self/recurrent update (message)\n    msg_self = layer(s_l)\n    # we compute the message from the left (s[l-1]) through the adapter\n    msg_l = left(state[l-1])\n    # we compute the message from the right (s[l+1]) through the adapter\n    msg_r = right(state[l+1])\n    # we aggregate the computed messages directed to the layer\n    h = layer.reduce([msg_self, msg_l, msg_r])\n    # we apply the non linearity to the result\n    s_next_l = layer.activation(h)\n    # we update the state at position l with the new value\n    return state.replace_val(l, s_next_l), h\n</code></pre>"},{"location":"tutorials/03_modules/#6-training-with-optax","title":"6. Training with Optax","text":"<p>Unlike gradient-based deep learning, learning here uses local updates. Each module\u2019s <code>backward</code> returns a module-shaped update, which we feed to Optax as if it were a gradient. This lets us retain momentum, schedules, clipping, etc., while remaining gradient-free.</p> <p>Be sure to check Optax documentation.</p> <pre><code>import optax\n\n# we define an optimizer such as adam\noptim = optax.adam(1e-2)\n\n# we use adam across our layer and adapters\nparams = (layer, left, right)\nopt_state = optim.init(eqx.filter(params, eqx.is_inexact_array))\n\ndef train_step(params, opt_state, state):\n    layer, left, right = params\n\n    # we compute the new state and preactivation as above\n    s1, h1 = one_step(state, layer, left, right)\n\n    # the backward functions compute updates of each layer\n    upd_layer = layer.backward(s1[1], s1[1], h1)\n    upd_left  = left.backward (s1[0], s1[0], h1)\n    upd_right = right.backward(s1[2], s1[2], h1)\n\n    # we use equinox to insert updates in the correct shapes\n    # this is a typical equinox/optax pattern\n    pseudo_grads = (upd_layer, upd_left, upd_right)\n    grads_f = eqx.filter(pseudo_grads, eqx.is_inexact_array)\n    params_f = eqx.filter(params, eqx.is_inexact_array)\n\n    # optax computes the updated parameters\n    updates, opt_state = optim.update(grads_f, opt_state, params=params_f)\n\n    # we apply them to our modules\n    params = eqx.apply_updates(params, updates)\n    return params, opt_state, s1\n</code></pre> <p>A training loop repeatedly calls <code>train_step</code>. Non-trainable adapters contribute zero updates, so they remain fixed. As said, we won't have to deal with the wiring of each layer directly. This will be handled by the orchestrator. Specifically, we will just need to call backward from the orchestrator to have a single update pytree of all modules in our network. Jax is totally transparent to the structure of our network, as long as it is a pytree.</p> <p>We provide a useful abstraction for this, see <code>LayerMap</code>.</p>"},{"location":"tutorials/03_modules/#7-multi-dimensional-states","title":"7. Multi-dimensional states","text":"<p>States can have arbitrary shapes: vectors <code>(B, F)</code> or image-like <code>(B, H, W, C)</code>. Layers and adapters generalize naturally as long as their operations are defined on those shapes. The global-state design also supports shared buffers, where multiple layers operate on slices of a single array.</p>"},{"location":"tutorials/03_modules/#8-summary","title":"8. Summary","text":"<ul> <li>Layers are stateful modules: self-message \u2192 reduce \u2192 activation.</li> <li>Adapters are stateless, transforming states into messages.</li> <li>States are messages: each layer\u2019s current state is the signal it emits.</li> <li>Backward rules: modules return structured updates, not gradients.</li> <li>Optax integration: these updates are fed as pseudo-gradients to Optax, enabling optimizer dynamics without autodiff.</li> <li>Global state: supports vector and multi-dimensional architectures with shared memory.</li> </ul> <p>This architecture mirrors the philosophy of distributed, local, gradient-free learning in asymmetric recurrent networks.</p>"},{"location":"tutorials/04_layermaps/","title":"04 \u2014 LayerMaps","text":"<p>A LayerMap is a PyTree wrapper that organizes the full set of layers and adapters in your network. It provides a consistent way to index them, guarantees immutability of the structure, and integrates seamlessly with JAX/Equinox/Optax.</p>"},{"location":"tutorials/04_layermaps/#1-matrix-view","title":"1. Matrix view","text":"<p>A LayerMap is conceptually a square matrix indexed by layer IDs:</p> <ul> <li>Diagonal <code>(i, i)</code>: the i-th layer. Each layer is stateful and sends a message to itself (its recurrent/self term).</li> <li>Off-diagonal <code>(i, j)</code> with <code>i \u2260 j</code>: an adapter. It converts the j-th state into a message for the i-th layer.</li> </ul> <p>This gives the following interpretation:</p> <ul> <li>Lower triangle (<code>i &gt; j</code>): forward adapters, messages flowing left \u2192 right.</li> <li>Upper triangle (<code>i &lt; j</code>): backward adapters, messages flowing right \u2192 left.</li> <li>Row <code>j</code>: everything that contributes into layer <code>j</code>.</li> <li>Column <code>i</code>: everything that originates from the state of layer <code>i</code>.</li> <li>Row 0: all connections going from layers to the input (unused for now).</li> <li>Row L: all connections going from layers to the output, meaning every layer whose state is used for prediction.</li> <li>Column 0: Forward skip connections from the input to each layer.</li> <li>Column L: Backward skip connections from the output to each layer.</li> </ul> <p>This matches the convention in <code>SequentialState</code>: <code>s[0]</code> is the input state, and <code>s[L]</code> is the output state.</p> <p>This structure, being in the end a dictionary of dictionaries is well suited for sparsity. For example, if layer <code>j</code> is not connected to layer <code>i</code>, there is no adapter in the <code>(i, j)</code> position. The element is simply not present in the structure, only the relevant modules are present.</p>"},{"location":"tutorials/04_layermaps/#2-api-overview","title":"2. API overview","text":"<pre><code>lm = LayerMap.from_dict({...})\n\nlm[i]: dict                        # read-only mapping of neighbors for row i\nlm[i, j]: Module                   # single module at (i, j)\n(i, j) in lm: bool                 # check if edge exists\n\nlm.rows(): tuple[int, ...]         # all row indices\nlm.cols_of(i): tuple[int, ...]     # all column indices in row i\nlm.neighbors(i): dict[int, Module] # read-only mapping {j: module} for row i\n\nlm.row_items(): Iterator[int, Module]                # iterate (row, neighbors)\nlm.edge_items(): Iterator[tuple[int , int], Module]  # iterate ((i, j), module)\nlm.to_dict(): dict[int, dict[int, Module]]           # copy as a dict-of-dicts\n</code></pre> <p>The API is intentionally dict-like but read-only: once built, the structure cannot be mutated. You cannot add layers or edges later.</p>"},{"location":"tutorials/04_layermaps/#3-immutability-of-structure","title":"3. Immutability of structure","text":"<p>A LayerMap is frozen once created:</p> <ul> <li>Row/column indices (the \u201cshape\u201d of the map) are part of the static treedef.</li> <li>Modules on each edge can change their parameters (via updates), but the adjacency cannot change.</li> </ul> <p>This immutability is not arbitrary. It is a basic requirement in JAX:</p> <ul> <li>The shape and structure of PyTrees must be static across JIT-compiled functions.</li> <li>If you were allowed to add or remove layers after creation, JIT cache keys would break and the compiled computation graph would need to be rebuilt every time.</li> <li>By freezing the structure, we ensure stability of compiled functions and allow the optimizer (Optax) to work on the entire network consistently.</li> </ul> <p>Thus, in JAX, data changes are dynamic, but structure is static.</p>"},{"location":"tutorials/04_layermaps/#4-example-building-a-simple-layermap","title":"4. Example: building a simple LayerMap","text":"<pre><code>import jax\nfrom darnax.modules.recurrent import RecurrentDiscrete\nfrom darnax.modules.adapters import Ferromagnetic\nfrom darnax.layer_maps.sparse import LayerMap\n\nkey0, key1 = jax.random.split(jax.random.PRNGKey(0))\nF = 8\n\n# Define two layers\nlayer0 = RecurrentDiscrete(features=F, j_d=0.0, threshold=0.0, key=key0)\nlayer1 = RecurrentDiscrete(features=F, j_d=0.0, threshold=0.0, key=key1)\n\n# Define adapters\nfwd_10 = Ferromagnetic(features=F, strength=0.5)  # forward (0 -&gt; 1)\nbwd_01 = Ferromagnetic(features=F, strength=0.2)  # backward (1 -&gt; 0)\n\n# Build dict-of-dicts\nraw = {\n    0: {0: layer0, 1: bwd_01},\n    1: {0: fwd_10, 1: layer1},\n}\n\nlm = LayerMap.from_dict(raw, require_diagonal=True)\n\n# Access\nprint(lm[1, 1])   # layer1\nprint(lm[1, 0])   # forward adapter\nprint(lm[1].keys())  # neighbors of row 1: {0, 1}\n</code></pre>"},{"location":"tutorials/04_layermaps/#5-a-layermap-as-a-pytree","title":"5. A LayerMap as a PyTree","text":"<p>Because <code>LayerMap</code> is registered as a PyTree:</p> <ul> <li>The keys (rows, columns) are static.</li> <li>The modules (layers/adapters) are leaves.</li> <li>Arrays inside those modules are visible to JAX and Optax.</li> </ul> <p>This means you can treat the entire network as a single object:</p> <pre><code>import equinox as eqx, optax\n\nopt = optax.adam(1e-2)\nopt_state = opt.init(eqx.filter(lm, eqx.is_inexact_array))\n\n# Later in training\nupdates, opt_state = opt.update(grads, opt_state, params=lm)\nlm = eqx.apply_updates(lm, updates)\n</code></pre> <p>All parameters inside all layers/adapters are updated in one go.</p>"},{"location":"tutorials/04_layermaps/#6-summary","title":"6. Summary","text":"<ul> <li>LayerMap = a collection of layers (diagonal) and adapters (off-diagonal) with integer keys.</li> <li>Matrix view: rows = inputs to a layer, columns = outputs from a layer.</li> <li>Input/output rows and columns handle special roles.</li> <li>Immutable structure: you cannot add or remove layers once built. This ensures JAX stability (PyTree structure must be static under JIT).</li> <li>PyTree integration: treat the whole network as one object, pass it to Equinox/Optax, and every parameter is handled correctly.</li> </ul> <p>This design makes LayerMap a central abstraction: a static graph of modules whose parameters evolve dynamically during training, while its topology remains fixed.</p>"},{"location":"tutorials/04_layermaps/#7-an-ascii-art","title":"7. An ascii art","text":"<pre><code>LayerMap (rows = receivers, columns = senders)\n\n            columns (senders: states/messages from j) \u2192\n            0          1          2        ...        L-1         L\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\nr    0   \u2502[L00]     [A01]     [A02\u2191]   \u2026            [A0,L-1]   [A0L\u2191]  \u2502\no        \u2502layer0    back      back                  back       back    \u2502\nw        \u2502(input)   adapters  adapters              adapters   adapters\u2502\ns        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n(    1   \u2502[A10\u2193]    [L11]     [A12\u2191]   \u2026            [A1,L-1]   [A1L]   \u2502\nr        \u2502fwd\u2192      layer1    \u2191back                 back       back    \u2502\ne        \u2502adapters            adapters              adapters   adapters\u2502\nc        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\ne    2   \u2502[A20\u2193]    [A21\u2193]    [L22]    \u2026            [A2,L-1\u2191]  [A2L\u2191]  \u2502\ni        \u2502fwd\u2192      fwd\u2192      layer2                \u2191 back     \u2191 back  \u2502\nv        \u2502adapters  adapters                        adapters   adapters\u2502\ne        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\nr    \u2026   \u2502\u2026          \u2026        \u2026        \u2026            \u2026          \u2026       \u2502\ns        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n     L   \u2502[AL0\u2193]    [AL1\u2193]    [AL2\u2193]   \u2026            [AL,L-1\u2193]  [LL]    \u2502\n         \u2502fwd\u2192      fwd\u2192      fwd\u2192                  fwd\u2192       \u2191layerL \u2502\n         \u2502adapters  adapters  adapters              adapters   (output)\u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2191\n           rows (receivers: layer i to be updated)\n</code></pre> <p>Legend:</p> <ul> <li>Lii   : layer on the diagonal (stateful). L00 is the input-layer slot; LL is the output-layer slot.</li> <li>Aij\u2193  : adapter at (i,j) with i &gt; j (lower triangle) \u2014 forward message (from j \u2192 i).</li> <li>Aij\u2191  : adapter at (i,j) with i &lt; j (upper triangle) \u2014 backward message (from j \u2192 i).</li> </ul> <p>Row/Column intuition:</p> <ul> <li>Row i collects everything needed to update layer i: the diagonal Lii (self-message) plus all Aij that transform state j into a message for i.</li> <li>Column j lists everything that uses state j as a source: the diagonal Ljj plus all Aij that send j\u2019s state to other layers.</li> </ul> <p>Input/Output:</p> <ul> <li>First column (\u00b7,0): forward skip connections from the input state to every layer.</li> <li>Last column (\u00b7,L): backward skip connections from the output state to earlier layers.</li> <li>Last row (L,\u00b7): all contributors that feed directly into the output layer (final prediction).</li> </ul> <p>Structure:</p> <ul> <li>Diagonal = layers; off-diagonal = adapters.</li> <li>The LayerMap\u2019s structure (rows/cols and which edges exist) is immutable after creation.</li> </ul>"},{"location":"tutorials/05_orchestrators/","title":"05 \u2014 Orchestrators","text":"<p>Having introduced states, modules, and layermaps, we now come to the final structural element: the orchestrator. Where the previous abstractions describe what the network is, the orchestrator specifies how the network evolves in time. It is the execution engine of the architecture: at each iteration it routes messages, applies updates, and advances the global state.</p>"},{"location":"tutorials/05_orchestrators/#1-conceptual-role","title":"1. Conceptual role","text":"<p>The orchestrator governs the dynamical process that characterizes this family of recurrent networks.</p> <p>As everything in the library, we propose a simple interface for the object, described below, and a first implementation. In this case, we focus on the <code>SequentialOrchestrator</code>, that plays nicely with sequential states and the sequential nature of the layer map. In detail, at each step, this specific orchestrator does the following:</p> <ol> <li>Message collection \u2014 for each receiving layer \\(i\\), all modules \\((i,j)\\) in the LayerMap are evaluated on the current sender states \\(s^{(j)}\\).</li> <li>Aggregation and activation \u2014 the diagonal module \\((i,i)\\) aggregates the incoming messages into a pre-activation \\(h^{(i)}\\) via its <code>reduce</code> method, then applies its nonlinearity with <code>activation</code>, yielding the new state slice \\(s^{(i)}\\).</li> <li>State update \u2014 the new slice replaces the old one in the global state, producing the next global configuration.</li> <li>Learning updates \u2014 in the training regime, each module also provides a local parameter update through its <code>backward(x, y, y_hat)</code> method. These are gathered into a LayerMap-structured PyTree of updates.</li> </ol> <p>In this way, the orchestrator realizes the iterative dynamics described in the paper: a distributed, gradient-free learning mechanism in which information is exchanged locally and parameters are updated via local rules.</p>"},{"location":"tutorials/05_orchestrators/#2-two-phases-of-dynamics","title":"2. Two phases of dynamics","text":"<p>In line with the two-phase protocol introduced in the theoretical model, the orchestrator provides two distinct update functions:</p> <ul> <li> <p>Training phase (<code>step</code>)   All available messages are considered, both forward (lower triangle) and backward (upper triangle). This corresponds to the supervised or clamped regime where input and output information are both present.</p> </li> <li> <p>Inference phase (<code>step_inference</code>)   Only causal messages are retained: for receiver \\(i\\), senders \\(j &lt; i\\) are discarded. Thus, information from the output or \u201cfuture\u201d layers does not leak backward. This corresponds to the free relaxation regime in which the system stabilizes autonomously.</p> </li> </ul> <p>This explicit separation ensures that training and inference dynamics are clearly distinguished in the implementation.</p>"},{"location":"tutorials/05_orchestrators/#3-public-api","title":"3. Public API","text":"<p>All orchestrators subclass the following abstract interface:</p> <pre><code>class AbstractOrchestrator(eqx.Module):\n    lmap: LayerMap  # fixed topology (rows, columns, edges)\n\n    def step(self, state: StateT, *, rng: KeyArray) -&gt; tuple[StateT, KeyArray]:\n        \"\"\"Run one full update step (training phase).\"\"\"\n\n    def step_inference(self, state: StateT, *, rng: KeyArray) -&gt; tuple[StateT, KeyArray]:\n        \"\"\"Run one update step (inference phase, discarding rightward messages).\"\"\"\n\n    def predict(self, state: SequentialState, rng: KeyArray) -&gt; tuple[SequentialState, KeyArray]:\n        \"\"\"Update the output state s[-1].\"\"\"\n\n    def backward(self, state: StateT, rng: KeyArray) -&gt; LayerMap:\n        \"\"\"Compute module-local updates in a LayerMap-structured PyTree.\"\"\"\n</code></pre>"},{"location":"tutorials/05_orchestrators/#stepstate-rng","title":"<code>step(state, rng)</code>","text":"<ul> <li>Executes one synchronous update of the network using all messages, both backward and forward.</li> <li>Returns the updated state and an advanced random key.</li> <li>It does not compute messages for the last row (messages going towards the output). So the last component of the state is never updated when applying <code>step</code>. There is a specific method called <code>predict</code> that needs to be run after the end of the dynamics if you want the actual prediction of the model.</li> </ul>"},{"location":"tutorials/05_orchestrators/#step_inferencestate-rng","title":"<code>step_inference(state, rng)</code>","text":"<ul> <li>Executes one update considering only messages from \\(j \\geq i\\).</li> <li>Used for prediction after training, ensuring purely causal message passing.</li> <li>As step, it does not compute messages for the last row (messages going towards the output). So the last component of the state is never updated when applying <code>step</code>. There is a specific method called <code>predict</code> that needs to be run after the end of the dynamics if you want the actual prediction of the model.</li> </ul>"},{"location":"tutorials/05_orchestrators/#predictstate-rng","title":"<code>predict(state, rng)</code>","text":"<ul> <li>Executes the update of the output state via usual forward + aggregation + activation. It is useful to check the implementation of <code>OutputLayer</code> to actually understand what is happening, as it is simply a sink that aggregates prediction from its neighboring adapters.</li> <li>Separating internal state update and final prediction allows to save some operations (computing the update of the output state at every step is useless), and also simplifies the overall API, see Tutorial 6.</li> </ul>"},{"location":"tutorials/05_orchestrators/#backwardstate-rng","title":"<code>backward(state, rng)</code>","text":"<ul> <li>For each edge \\((i,j)\\), invokes <code>lmap[i,j].backward(x=state[j], y=state[i], y_hat=local_field)</code> to obtain a module-shaped update.</li> <li>Returns a LayerMap with the same static structure as the original, but whose leaves are parameter updates.</li> <li>This PyTree can be passed directly to Optax as if it were a gradient structure.</li> </ul>"},{"location":"tutorials/05_orchestrators/#4-structural-properties","title":"4. Structural properties","text":"<ul> <li>Static topology: The orchestrator\u2019s LayerMap has a fixed set of rows, columns, and edges. This immutability is necessary for JAX compatibility, as PyTree structures must remain constant across compiled functions.</li> <li>Dynamic values: Within this static skeleton, the array values of module parameters evolve freely during training.</li> <li>PyTree compliance: Because the orchestrator itself is an Equinox module, it is also a PyTree. Its parameters can be filtered, updated, and optimized exactly like any other object in the system.</li> <li>Transformation compatibility: The orchestrator is fully compatible with <code>jax.jit</code>, <code>jax.vmap</code>, and all other JAX transformations. Since the topology is static, compilation is stable; only array values trigger recompilation when their shapes change.</li> </ul>"},{"location":"tutorials/05_orchestrators/#5-typical-usage","title":"5. Typical usage","text":"<p>A typical training loop involving an orchestrator proceeds as follows:</p> <pre><code># Forward update (training regime)\nstate, rng = orchestrator.step(state, rng=rng)\n\n# Forward update (inference regime)\nstate, rng = orchestrator.step_inference(state, rng=rng)\n\n# Compute module-local updates\nupd_lmap = orchestrator.backward(state, rng=rng)\n\n# Apply updates with Optax\ngrads  = eqx.filter(upd_lmap, eqx.is_inexact_array)\nparams = eqx.filter(orchestrator.lmap, eqx.is_inexact_array)\ndeltas, opt_state = opt.update(grads, opt_state, params=params)\nnew_lmap = eqx.apply_updates(orchestrator.lmap, deltas)\n\n# Replace the LayerMap inside the orchestrator\norchestrator = eqx.tree_at(lambda o: o.lmap, orchestrator, new_lmap)\n</code></pre> <p>Here the updates are not gradients: they are the outcome of local learning rules defined at the module level. Optax is used purely as a robust update engine.</p>"},{"location":"tutorials/05_orchestrators/#6-more-complex-logic","title":"6. More complex logic","text":"<p>Right until now we described a specific instance of orchestrator, i.e. the <code>SequentialOrchestrator</code>. It plays nicely with both the sequantial state and the sequential implementation of the layer map.</p> <p>The only assumption about this structures is a notion of order in the network, meaning that the first layer comes first, the second comes second, etc...</p> <p>This is totally arbitrary, this library allows for any structure and logic in the network functioning, a first example might be a totally synchronous network, where there is no notion of order and all layers are treated in sync. It is also possible to define a group structure, where different layers belong to different groups, each handled concurrently. This might involve an extension of the <code>LayerMap</code> to allow for string keys to identify groups...</p> <p>Another option is to specialize architectures for speed and efficiency. Instead of working with dict of dicts and simple loops, we might want to decide to pad and stack layer states together to be handled in parallel. This is also an easy extension of <code>LayerMap</code> and <code>Orchestrator</code>.</p> <p>We might even put orchestrators inside single modules, to encapsulate a into a single object complex logic.</p>"},{"location":"tutorials/05_orchestrators/#7-summary","title":"7. Summary","text":"<ul> <li>The orchestrator advances the network\u2019s dynamics by routing messages, aggregating them, and updating the global state.</li> <li><code>step</code> executes the full supervised/clamped update; <code>step_inference</code> executes the free, causal update.</li> <li><code>backward</code> collects local updates into a LayerMap-shaped PyTree, aligned with the parameter structure, enabling seamless integration with Optax.</li> <li>The orchestrator is a PyTree with static structure: immutable topology, mutable parameter values. This guarantees full compatibility with JAX transformations and ensures efficient compilation.</li> </ul> <p>Through the orchestrator, the network acquires its temporal dimension: states evolve, messages flow, and local rules drive learning, exactly as described in the underlying theoretical framework.</p>"},{"location":"tutorials/06_simple_net_on_artificial_data/","title":"06 \u2014 Simple Training Tutorial","text":"<p>This mini-notebook shows an end\u2011to\u2011end training loop. Notice that it resembles plain pytorch in the sense that you need to write your own train_Step and eval_step. We will see:</p> <ul> <li>How to define a <code>SequentialOrchestrator</code> over a sparse <code>LayerMap</code>.</li> <li>How to update the model via <code>.backward(...)</code> used as pseudo\u2011gradients for Optax.</li> <li>How to define train_steps, eval_steps and update_steps.</li> </ul> <p>The goal here is clarity: well\u2011ordered cells, consistent dtypes/PRNG usage, and inline comments explaining each step. We are not taking full advantage of jax for now. For example here we never call Jit on any function. Also, the dynamics <code>orchestrator.step/step_inference</code> is well suited for <code>jax.lax.scan</code>, but here we will just use a regular python-side for loop.</p> <pre><code># --- Imports ---------------------------------------------------------------\nimport time\nfrom typing import Any\n\nimport jax\nimport jax.numpy as jnp\nimport equinox as eqx\nimport optax\n\nfrom darnax.orchestrators.sequential import SequentialOrchestrator\nfrom darnax.modules.fully_connected import FrozenFullyConnected, FullyConnected\nfrom darnax.modules.recurrent import RecurrentDiscrete\nfrom darnax.modules.input_output import OutputLayer\nfrom darnax.layer_maps.sparse import LayerMap\nfrom darnax.states.sequential import SequentialState\n</code></pre>"},{"location":"tutorials/06_simple_net_on_artificial_data/#utilities-metrics-summaries","title":"Utilities (metrics &amp; summaries)","text":"<p>Small helpers used for monitoring. Labels are OVA \u00b11 and predictions are decoded via <code>argmax</code> over the output scores.</p> <pre><code>def batch_accuracy(y_true: jnp.ndarray, y_pred: jnp.ndarray) -&gt; float:\n    \"\"\"Accuracy with \u00b11 OVA labels (class = argmax along last dim).\"\"\"\n    y_true_idx = jnp.argmax(y_true, axis=-1)\n    y_pred_idx = jnp.argmax(y_pred, axis=-1)\n    return float(jnp.mean(y_true_idx == y_pred_idx))\n\n\ndef print_state_summary(s_out: jnp.ndarray, header: str = \"Output state\") -&gt; None:\n    \"\"\"Quick shape/range summary for an output buffer.\"\"\"\n    smin = float(jnp.min(s_out))\n    smax = float(jnp.max(s_out))\n    smu = float(jnp.mean(s_out))\n    sstd = float(jnp.std(s_out))\n    print(\n        f\"{header}: shape={tuple(s_out.shape)}, range=({smin:.3f},{smax:.3f}), mean={smu:.3f}, std={sstd:.3f}\"\n    )\n</code></pre>"},{"location":"tutorials/06_simple_net_on_artificial_data/#dataset-binary-prototypes","title":"Dataset: Binary Prototypes.","text":"<p>This object defines the data that we will be training our model on. It is a very simple task. The object is a compact generator that creates: - <code>K</code> prototype vectors in <code>{\u22121,+1}^D</code>. - A dataset of size <code>N</code> by corrupting prototypes with random bit-flips with probability <code>p_noise</code>. - \u00b11 labels (<code>+1</code> on the correct class, <code>\u22121</code> elsewhere), that indicate which prototype the data is coming from.</p> <pre><code>class PrototypeData:\n    \"\"\"Binary prototype dataset with OVA \u00b11 labels.\"\"\"\n\n    RAND_THRESHOLD = 0.5\n\n    def __init__(\n        self,\n        key: jax.Array,\n        batch_size: int = 16,\n        num_prototypes: int = 10,\n        dim_prototypes: int = 100,\n        num_data: int = 1000,\n        p_noise: float = 0.3,\n    ):\n        assert 0 &lt;= p_noise &lt; self.RAND_THRESHOLD, f\"Invalid {p_noise=}\"\n        assert num_data &gt;= num_prototypes, f\"Invalid {num_data=}, {num_prototypes=}\"\n        assert batch_size &gt; 1, f\"Invalid {batch_size=}\"\n        self.num_prototypes = int(num_prototypes)\n        self.dim_prototypes = int(dim_prototypes)\n        self.num_data = int(num_data)\n        self.p_noise = float(p_noise)\n        self.batch_size = int(batch_size)\n        self.num_batches = -(-self.num_data // self.batch_size)  # ceil division\n\n        key_prototypes, key_data = jax.random.split(key)\n        self._create_prototypes(key_prototypes)\n        self._create_data(key_data)\n\n    def __iter__(self):\n        \"\"\"Yield batches `(x, y)` as in the original implementation.\"\"\"\n        return zip(\n            jnp.array_split(self.x, self.num_batches),\n            jnp.array_split(self.y, self.num_batches),\n            strict=True,\n        )\n\n    def _create_prototypes(self, key: jax.Array) -&gt; None:\n        \"\"\"Generate \u00b11 prototypes (float32).\"\"\"\n        # Use rademacher \u2192 {\u22121,+1} with explicit float32 dtype.\n        self.prototypes = jax.random.rademacher(\n            key, shape=(self.num_prototypes, self.dim_prototypes), dtype=jnp.float32\n        )\n\n    def _create_data(self, key: jax.Array) -&gt; None:\n        \"\"\"Generate dataset by repeating prototypes and flipping signs with prob. `p_noise`.\"\"\"\n        # Build OVA labels: +1 on diag, \u22121 elsewhere, then repeat to length N.\n        self.y = jnp.full(\n            shape=(self.num_prototypes, self.num_prototypes), fill_value=-1.0, dtype=jnp.float32\n        )\n        self.y = self.y.at[jnp.diag_indices_from(self.y)].set(1.0)\n        self.y = jnp.repeat(\n            self.y,\n            self.num_data // self.num_prototypes + 1,\n            axis=0,\n            total_repeat_length=self.num_data,\n        )\n\n        # Repeat prototypes to length N, then flip signs with probability p_noise.\n        key, carry = jax.random.split(key)\n        self.x = jnp.repeat(\n            self.prototypes,\n            self.num_data // self.num_prototypes + 1,\n            axis=0,\n            total_repeat_length=self.num_data,\n        )\n        flip_mask = jax.random.bernoulli(carry, p=1 - self.p_noise, shape=self.x.shape) * 2.0 - 1.0\n        self.x = self.x * flip_mask\n        # Shuffle x and y in sync.\n        shuffle = jax.random.permutation(key, self.num_data)\n        self.x = self.x[shuffle].astype(jnp.float32)\n        self.y = self.y[shuffle].astype(jnp.float32)\n</code></pre>"},{"location":"tutorials/06_simple_net_on_artificial_data/#build-model-state","title":"Build Model &amp; State","text":"<p>Here we define a simple model with one hidden layer and fully-connected adapters, similar to a perceptron with one hidden layer, except that the hidden layer has internal recurrency.</p> <p>The topology of the layer map is the following: - Layer 1 (hidden) receives from input (0, forth), itself (1, recurrent), and labels (2, back). - Layer 2 (output) receives from hidden (1) and itself (2).</p> <p>Some first comments:</p> <ul> <li>Notice that we do not define a input layer, that would correspond to layer 0 in the receivers. This is totally fine and intended. The input in this case is simply a sent message and never updated. If we dont define layer 0 in the receivers, the first component of the state is fixed.</li> <li>You should inspect the implementation of the OutputLayer. It does not have an internal state, parameters, and the <code>__call__</code> function returns an array of zeros. It is basically a sink that aggregates messages from all layers that contribute to the output and sums them. This behaviour can change in the future with the definition of new OutputLayers with a more complex logic, but for now it is basically an aggregator.</li> </ul> <pre><code>DIM_DATA = 100\nNUM_DATA = 1000\nNUM_LABELS = 10\nDIM_HIDDEN = 256\nTHRESHOLD_OUT = 3.5\nTHRESHOLD_IN = 3.5\nTHRESHOLD_J = 0.5\nSTRENGTH_BACK = 0.3\nSTRENGTH_FORTH = 1.0\nJ_D = 0.5\n\n# Global state with three buffers: input (0), hidden (1), output/labels (2)\nstate = SequentialState((DIM_DATA, DIM_HIDDEN, NUM_LABELS))\n\n# Distinct keys per module to avoid accidental correlations.\nmaster_key = jax.random.key(seed=44)\nkeys = jax.random.split(master_key, num=5)\n\nlayer_map = {\n    # Hidden row (1): from input (0), self (1), and labels (2)\n    1: {\n        0: FullyConnected(\n            in_features=DIM_DATA,\n            out_features=DIM_HIDDEN,\n            strength=STRENGTH_FORTH,\n            threshold=THRESHOLD_IN,\n            key=keys[0],\n        ),\n        1: RecurrentDiscrete(features=DIM_HIDDEN, j_d=J_D, threshold=THRESHOLD_J, key=keys[1]),\n        2: FrozenFullyConnected(\n            in_features=NUM_LABELS,\n            out_features=DIM_HIDDEN,\n            strength=STRENGTH_BACK,\n            threshold=0.0,\n            key=keys[2],\n        ),\n    },\n    # Output row (2): from hidden (1), and itself (2)\n    2: {\n        1: FullyConnected(\n            in_features=DIM_HIDDEN,\n            out_features=NUM_LABELS,\n            strength=1.0,\n            threshold=THRESHOLD_OUT,\n            key=keys[3],\n        ),\n        2: OutputLayer(),  # the 2-2 __call__ is a vector of zeros, it does not contribute\n    },\n}\nlayer_map = LayerMap.from_dict(layer_map)\n\n# Trainable orchestrator built from the fixed topology.\norchestrator = SequentialOrchestrator(layers=layer_map)\n</code></pre>"},{"location":"tutorials/06_simple_net_on_artificial_data/#optimizer","title":"Optimizer","text":"<p>We choose Adam with <code>lr=5e-3</code>. As you can see we can call <code>eqx.filter</code> directly on the orchestrator, since it is a pytree. We also show how to update the model with the function <code>update(orchestrator, state, optimizer)</code>.</p> <pre><code>optimizer = optax.adam(5e-3)\nopt_state = optimizer.init(eqx.filter(orchestrator, eqx.is_inexact_array))\n\n\ndef update(\n    orchestrator: SequentialOrchestrator, state: SequentialState, optimizer, optimizer_state, rng\n) -&gt; tuple[SequentialOrchestrator, Any]:\n    \"\"\"Compute and applies the updates and returns the updated model.\n\n    It also returns the optimizer state, typed as Any for now.\n    \"\"\"\n    # 1) Local deltas (orchestrator-shaped deltas).\n    grads = orchestrator.backward(state, rng=rng)\n\n    # 2) Optax over the orchestrator (tree structures match by construction).\n    # This is common equinox + optax pattern, used in the same way when training\n    # \"regular\" deep learning models.\n    # First we filter fields with equinox and then we give them to the optimizer.\n    # This allows us to handle complex pytrees with static fields seamlessly during\n    # training.\n    params = eqx.filter(orchestrator, eqx.is_inexact_array)\n    grads = eqx.filter(grads, eqx.is_inexact_array)\n\n    # 3) We compute the updates and apply them to our model\n    updates, opt_state = optimizer.update(grads, optimizer_state, params=params)\n    orchestrator = eqx.apply_updates(orchestrator, updates)\n\n    # 4) We return the updated model and the optimizer state\n    return orchestrator, opt_state\n</code></pre>"},{"location":"tutorials/06_simple_net_on_artificial_data/#dynamics-helpers","title":"Dynamics helpers","text":"<p>We now define two simple functions that run during training, they are extremely simple. During run_dynamics_training we have two phases: a first one where we compute all messages and run the dynamics with both forward and backward messages. We run this phase for a fixed number of steps. Then, we do a second phase where we suppress all messages \"going backward\", we run this dynamics for a fixed number of steps and we obtain a second fixed point s^*.</p> <p>During inference, we only run the dynamics with suppressed messages from the right.</p> <pre><code>def run_dynamics_training(\n    orch: SequentialOrchestrator,\n    s,\n    rng: jax.Array,\n    steps: int,\n):\n    \"\"\"Run `steps` iterations using ALL messages (clamped phase).\n\n    Note: Kept identical to your working version for consistency.\n    \"\"\"\n    for _ in range(steps):\n        s, rng = orch.step(s, rng=rng)\n    for _ in range(steps):\n        s, rng = orch.step_inference(s, rng=rng)\n    return s, rng\n\n\ndef run_dynamics_inference(\n    orch: SequentialOrchestrator,\n    s,\n    rng: jax.Array,\n    steps: int,\n):\n    \"\"\"Run `steps` iterations discarding rightward/backward messages (free phase).\"\"\"\n    for _ in range(steps):\n        s, rng = orch.step_inference(s, rng=rng)\n    return s, rng\n</code></pre>"},{"location":"tutorials/06_simple_net_on_artificial_data/#train-step","title":"Train step","text":"<p>This function summarizes the whole training protocol, for a single batch.</p> <p>Protocol per batch: 1. Initialize/clamp the global state with <code>(x, y)</code>. 2. Run training dynamics for <code>2 * T_train</code> steps. 3. Update the model with <code>update</code>, as seen before.</p> <pre><code>def train_step(\n    orch: SequentialOrchestrator,\n    s: SequentialState,\n    x: jnp.ndarray,\n    y: jnp.ndarray,\n    rng: jax.Array,\n    *,\n    opt_state,\n    optimizer,\n    t_train: int = 3,\n):\n    # 1) Clamp current batch (inputs &amp; labels).\n    s = s.init(x, y)\n\n    # 2) Training dynamics (kept as-is).\n    s, rng = run_dynamics_training(orch, s, rng, steps=t_train)\n\n    # 3) Update the model\n    rng, update_key = jax.random.split(rng)\n    orch, opt_state = update(orch, s, optimizer, opt_state, update_key)\n    return orch, rng, opt_state\n</code></pre>"},{"location":"tutorials/06_simple_net_on_artificial_data/#eval-step","title":"Eval step","text":"<p>Initializes with inputs only (labels are just for metrics), then runs the inference dynamics and computes metrics.</p> <pre><code>def eval_step(\n    orch: SequentialOrchestrator,\n    s: SequentialState,\n    x: jnp.ndarray,\n    y: jnp.ndarray,\n    rng: jax.Array,\n    *,\n    t_eval: int = 5,\n) -&gt; tuple[SequentialOrchestrator, SequentialState, dict, jax.Array]:\n    s = s.init(x, None)\n    s, rng = run_dynamics_inference(orch, s, rng, steps=t_eval)\n    s, rng = orchestrator.predict(s, rng)\n    y_pred = s[-1]\n    metrics = {\"acc\": batch_accuracy(y, y_pred)}\n    return metrics, rng\n</code></pre>"},{"location":"tutorials/06_simple_net_on_artificial_data/#run-the-training","title":"Run the training","text":"<p>Finally, we build a small <code>PrototypeData</code> stream, train for a few epochs using <code>train_step</code> per batch, and evaluate on the same stream.</p> <pre><code># Constants\nP_NOISE = 0.3\nBATCH_SIZE = 16\nEPOCHS = 3\nT_TRAIN = 10  # training dynamics steps per batch\nT_EVAL = 10  # short inference steps for monitoring\n\n# RNGs\nmaster_key = jax.random.key(59)\nmaster_key, data_key = jax.random.split(master_key)\n\n# Data\ndata = PrototypeData(\n    key=data_key,\n    batch_size=BATCH_SIZE,\n    num_prototypes=NUM_LABELS,\n    dim_prototypes=DIM_DATA,\n    num_data=NUM_DATA,\n    p_noise=P_NOISE,\n)\nprint(f\"Dataset: x.shape={tuple(data.x.shape)}  y.shape={tuple(data.y.shape)}\")\n\n# Training config\n\nhistory = {\"acc\": []}\n\nfor epoch in range(1, EPOCHS + 1):\n    t0 = time.time()\n    print(f\"\\n=== Epoch {epoch}/{EPOCHS} ===\")\n    for x_batch, y_batch in data:\n        # Keep batch arrays float32 (consistency)\n        master_key, train_key, eval_key = jax.random.split(master_key, num=3)\n        orchestrator, master_key, opt_state = train_step(\n            orchestrator,\n            state,\n            x_batch,\n            y_batch,\n            rng=train_key,\n            opt_state=opt_state,\n            optimizer=optimizer,\n            t_train=T_TRAIN,\n        )\n        metrics, rng = eval_step(orchestrator, state, x_batch, y_batch, eval_key)\n    history[\"acc\"].append(metrics[\"acc\"])\n    print(f\"Epoch {epoch} done in {time.time()-t0:.2f}s\")\n    print(f\"Mean accuracy={float(jnp.mean(jnp.array(history['acc']))):.3f}\")\n</code></pre>"},{"location":"tutorials/06_simple_net_on_artificial_data/#final-evaluation-demo","title":"Final evaluation (demo)","text":"<p>Single pass over the same iterator; replace with a held\u2011out set in practice.</p> <pre><code>eval_acc = []\nfor x_b, y_b in data:\n    x_batch = x_b.astype(jnp.float32)\n    y_batch = y_b.astype(jnp.float32)\n    master_key, step_key = jax.random.split(master_key)\n    metrics, master_key = eval_step(\n        orchestrator,\n        state,\n        x_batch,\n        y_batch,\n        rng=step_key,\n        t_eval=T_EVAL,\n    )\n    eval_acc.append(metrics[\"acc\"])\n\nprint(\"\\n=== Final evaluation summary ===\")\nprint(f\"Accuracy={float(jnp.mean(jnp.array(eval_acc))):.3f}\")\n</code></pre>"}]}